Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs

----- Training -----
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name              | Type                               | Params | Mode
---------------------------------------------------------------------------------
0 | mel               | AugmentMelSTFT                     | 0      | train
1 | imitation_encoder | MobileNetV3                        | 2.7 M  | train
2 | reference_encoder | MobileNetV3                        | 2.7 M  | train
3 | clap_model        | CLAPAudioEmbeddingClassifierFreev2 | 158 M  | eval
  | other params      | n/a                                | 2      | n/a
---------------------------------------------------------------------------------
5.4 M     Trainable params
158 M     Non-trainable params
163 M     Total params
655.085   Total estimated model params size (MB)
419       Modules in train mode
465       Modules in eval mode
Sanity Checking DataLoader 0:   0%|                                                                                                                       | 0/2 [00:00<?, ?it/s]
[VALIDATION] Batch 0 - Processing validation data
[VALIDATION] Imitation shape: torch.Size([1, 320000]), Reference shape: torch.Size([1, 320000])
[VALIDATION] Imitation: min=-0.8460, max=1.0018
[VALIDATION] Reference: min=-0.9751, max=0.9976
[CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9751, max=0.9976, mean=-0.0001, std=0.2598
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Sample 0: 25/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[VALIDATION] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[VALIDATION] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[VALIDATION] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[VALIDATION] WARNING: NaN found in log_softmax outputs:
[VALIDATION]   - C_qvim_log: 0/1 NaN values
[VALIDATION]   - C_ref_clap_log: 1/1 NaN values
[VALIDATION]   - C_im_clap_log: 1/1 NaN values
Sanity Checking DataLoader 0:  50%|███████████████████████████████████████████████████████▌                                                       | 1/2 [00:01<00:01,  0.52it/s]
[VALIDATION] Batch 1 - Processing validation data
[VALIDATION] Imitation shape: torch.Size([1, 320000]), Reference shape: torch.Size([1, 320000])
[VALIDATION] Imitation: min=-0.9942, max=0.9894
[VALIDATION] Reference: min=-0.9515, max=1.0022
[CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0: 150811/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[VALIDATION] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[VALIDATION] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[VALIDATION] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[VALIDATION] WARNING: NaN found in log_softmax outputs:
[VALIDATION]   - C_qvim_log: 0/1 NaN values
[VALIDATION]   - C_ref_clap_log: 1/1 NaN values
[VALIDATION]   - C_im_clap_log: 1/1 NaN values
Epoch 0:   0%|                                                                                                                                        | 0/10586 [00:00<?, ?it/s][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9448, max=0.7345, mean=0.0000, std=0.0662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8896, max=0.5645, mean=0.0000, std=0.0489
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8896, max=0.5645, mean=0.0000, std=0.0489
[CLAP PIPELINE] Sample 0: 75420/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                     | 1/10586 [00:02<8:08:10,  0.36it/s, v_num=c9zf, train/loss=nan.0, lr=2.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8902, max=1.0016, mean=-0.0000, std=0.1195
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8936, max=1.0020, mean=-0.0000, std=0.1195
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8936, max=1.0020, mean=-0.0000, std=0.1195
[CLAP PIPELINE] Sample 0: 117088/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                     | 2/10586 [00:02<3:51:14,  0.76it/s, v_num=c9zf, train/loss=nan.0, lr=2.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9285, max=0.9992, mean=-0.0009, std=0.0743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Sample 0: 129718/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 3/10586 [00:02<2:47:19,  1.05it/s, v_num=c9zf, train/loss=nan.0, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9876, max=1.0462, mean=-0.0003, std=0.0745
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0: 92412/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 4/10586 [00:03<2:16:11,  1.29it/s, v_num=c9zf, train/loss=nan.0, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0043, max=0.8394, mean=-0.0000, std=0.0324
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5513, max=0.6128, mean=-0.0000, std=0.0288
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5513, max=0.6128, mean=-0.0000, std=0.0288
[CLAP PIPELINE] Sample 0: 152754/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 5/10586 [00:03<1:59:12,  1.48it/s, v_num=c9zf, train/loss=nan.0, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8219, max=0.8306, mean=-0.0000, std=0.0854
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7202, max=0.7139, mean=-0.0000, std=0.0742
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7202, max=0.7139, mean=-0.0000, std=0.0742
[CLAP PIPELINE] Sample 0: 7/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1782, max=0.1287, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.9382, max=-0.9382, mean=-0.9382
[SIMILARITY] C_im_clap: min=-0.9033, max=-0.9033, mean=-0.9033
Epoch 0:   0%|                                                                                    | 6/10586 [00:03<1:51:30,  1.58it/s, v_num=c9zf, train/loss=-0.00, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9915, max=0.7667, mean=-0.0008, std=0.0252
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Sample 0: 104639/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 7/10586 [00:04<1:45:07,  1.68it/s, v_num=c9zf, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.9017, mean=0.0002, std=0.1192
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9956, max=0.9019, mean=0.0002, std=0.1191
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9956, max=0.9019, mean=0.0002, std=0.1191
[CLAP PIPELINE] Sample 0: 131309/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 8/10586 [00:04<1:37:35,  1.81it/s, v_num=c9zf, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9986, max=0.9916, mean=0.0004, std=0.1325
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0: 99635/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 9/10586 [00:04<1:34:14,  1.87it/s, v_num=c9zf, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9860, max=0.6366, mean=-0.0001, std=0.0639
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Sample 0: 91800/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 10/10586 [00:05<1:28:48,  1.98it/s, v_num=c9zf, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9877, max=0.9983, mean=0.0004, std=0.0661
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Sample 0: 87724/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 11/10586 [00:05<1:25:43,  2.06it/s, v_num=c9zf, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8575, mean=-0.0000, std=0.0995
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Sample 0: 66892/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 12/10586 [00:05<1:22:41,  2.13it/s, v_num=c9zf, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8927, max=1.0000, mean=0.0000, std=0.0786
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8281, max=0.7896, mean=0.0000, std=0.0775
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8281, max=0.7896, mean=0.0000, std=0.0775
[CLAP PIPELINE] Sample 0: 150521/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 13/10586 [00:05<1:19:44,  2.21it/s, v_num=c9zf, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=1.0032, mean=-0.0030, std=0.1216
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9224, max=0.9990, mean=-0.0030, std=0.1213
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9224, max=0.9990, mean=-0.0030, std=0.1213
[CLAP PIPELINE] Sample 0: 123215/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 14/10586 [00:06<1:16:19,  2.31it/s, v_num=c9zf, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6593, max=0.6695, mean=0.0131, std=0.0694
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6108, max=0.6440, mean=0.0131, std=0.0693
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6108, max=0.6440, mean=0.0131, std=0.0693
[CLAP PIPELINE] Sample 0: 95673/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 15/10586 [00:06<1:13:37,  2.39it/s, v_num=c9zf, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8606, max=1.0119, mean=0.0192, std=0.2174
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8516, max=0.9956, mean=0.0192, std=0.2169
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8516, max=0.9956, mean=0.0192, std=0.2169
[CLAP PIPELINE] Sample 0: 3118/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2160, max=14.2160, mean=14.2160
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 16/10586 [00:06<1:11:46,  2.45it/s, v_num=c9zf, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9965, mean=-0.0004, std=0.1406
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0: 27046/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 17/10586 [00:06<1:10:31,  2.50it/s, v_num=c9zf, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9966, max=1.0181, mean=-0.0001, std=0.0736
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=1.0098, mean=-0.0001, std=0.0735
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=1.0098, mean=-0.0001, std=0.0735
[CLAP PIPELINE] Sample 0: 103091/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 18/10586 [00:07<1:09:35,  2.53it/s, v_num=c9zf, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8297, max=1.0162, mean=-0.0000, std=0.0370
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Sample 0: 64456/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 19/10586 [00:07<1:08:25,  2.57it/s, v_num=c9zf, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9877, max=0.9983, mean=0.0004, std=0.0661
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Sample 0: 87724/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 20/10586 [00:07<1:07:21,  2.61it/s, v_num=c9zf, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9285, max=0.9992, mean=-0.0009, std=0.0743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Sample 0: 129718/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 21/10586 [00:07<1:06:37,  2.64it/s, v_num=c9zf, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9624, max=1.0002, mean=-0.0004, std=0.1298
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Sample 0: 144175/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 22/10586 [00:07<1:01:27,  2.86it/s, v_num=c9zf, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0033, max=1.0005, mean=-0.0001, std=0.0460
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9839, max=0.9712, mean=-0.0001, std=0.0460
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9839, max=0.9712, mean=-0.0001, std=0.0460
[CLAP PIPELINE] Sample 0: 154779/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 23/10586 [00:07<1:00:56,  2.89it/s, v_num=c9zf, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9982, max=0.8683, mean=-0.0054, std=0.0859
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.8677, mean=-0.0054, std=0.0859
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.8677, mean=-0.0054, std=0.0859
[CLAP PIPELINE] Sample 0: 127269/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 24/10586 [00:08<1:00:19,  2.92it/s, v_num=c9zf, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0084, mean=-0.0001, std=0.1935
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Sample 0: 39111/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 25/10586 [00:08<59:09,  2.98it/s, v_num=c9zf, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0007, max=0.9729, mean=-0.0113, std=0.4417
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.9702, mean=-0.0113, std=0.4414
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.9702, mean=-0.0113, std=0.4414
[CLAP PIPELINE] Sample 0: 20050/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 26/10586 [00:08<59:05,  2.98it/s, v_num=c9zf, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9992, max=0.9896, mean=-0.0005, std=0.0853
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9897, mean=-0.0005, std=0.0853
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9897, mean=-0.0005, std=0.0853
[CLAP PIPELINE] Sample 0: 137825/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 27/10586 [00:09<59:10,  2.97it/s, v_num=c9zf, train/loss=nan.0, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9676, max=1.0002, mean=-0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0: 25933/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 28/10586 [00:09<58:59,  2.98it/s, v_num=c9zf, train/loss=nan.0, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9931, max=0.9308, mean=-0.0000, std=0.1076
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9946, max=0.9312, mean=-0.0000, std=0.1076
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9946, max=0.9312, mean=-0.0000, std=0.1076
[CLAP PIPELINE] Sample 0: 50682/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 29/10586 [00:09<58:40,  3.00it/s, v_num=c9zf, train/loss=nan.0, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7830, max=0.7911, mean=-0.0000, std=0.0254
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.2476, max=0.2393, mean=-0.0000, std=0.0161
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.2476, max=0.2393, mean=-0.0000, std=0.0161
[CLAP PIPELINE] Sample 0: 63125/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 30/10586 [00:09<58:04,  3.03it/s, v_num=c9zf, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8381, max=1.0082, mean=0.0006, std=0.1664
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8252, max=0.9849, mean=0.0006, std=0.1655
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8252, max=0.9849, mean=0.0006, std=0.1655
[CLAP PIPELINE] Sample 0: 88665/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 31/10586 [00:10<57:47,  3.04it/s, v_num=c9zf, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8002, max=1.0032, mean=-0.0002, std=0.0756
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0: 131203/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 32/10586 [00:10<57:14,  3.07it/s, v_num=c9zf, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8584, max=0.9898, mean=-0.0015, std=0.2105
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8594, max=0.8374, mean=-0.0015, std=0.2020
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8594, max=0.8374, mean=-0.0015, std=0.2020
[CLAP PIPELINE] Sample 0: 104006/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 33/10586 [00:10<56:36,  3.11it/s, v_num=c9zf, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8739, max=0.9986, mean=-0.0000, std=0.0380
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0: 61484/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 34/10586 [00:10<56:37,  3.11it/s, v_num=c9zf, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8882, max=0.8452, mean=-0.0001, std=0.0777
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5093, max=0.4824, mean=-0.0001, std=0.0479
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5093, max=0.4824, mean=-0.0001, std=0.0479
[CLAP PIPELINE] Sample 0: 63204/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 35/10586 [00:11<56:07,  3.13it/s, v_num=c9zf, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9034, max=1.0000, mean=-0.0000, std=0.1119
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0: 63474/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 36/10586 [00:11<55:48,  3.15it/s, v_num=c9zf, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1079, max=1.1765, mean=-0.0007, std=0.1651
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.1914, max=1.1514, mean=-0.0007, std=0.1642
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.1914, max=1.1514, mean=-0.0007, std=0.1642
[CLAP PIPELINE] Sample 0: 6096/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 37/10586 [00:11<55:54,  3.14it/s, v_num=c9zf, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9734, max=0.9692, mean=-0.0008, std=0.4332
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=1.0049, mean=-0.0008, std=0.4280
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=1.0049, mean=-0.0008, std=0.4280
[CLAP PIPELINE] Sample 0: 88002/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                     | 38/10586 [00:12<55:46,  3.15it/s, v_num=c9zf, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0: 150811/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2229, max=14.2229, mean=14.2229
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                     | 39/10586 [00:12<55:29,  3.17it/s, v_num=c9zf, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.7777, mean=0.0000, std=0.0404
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.7783, mean=0.0000, std=0.0404
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.7783, mean=0.0000, std=0.0404
[CLAP PIPELINE] Sample 0: 113254/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                     | 40/10586 [00:12<55:17,  3.18it/s, v_num=c9zf, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0082, max=0.9617, mean=-0.0000, std=0.0235
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6812, max=0.6206, mean=-0.0000, std=0.0112
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6812, max=0.6206, mean=-0.0000, std=0.0112
[CLAP PIPELINE] Sample 0: 655/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                     | 41/10586 [00:12<54:51,  3.20it/s, v_num=c9zf, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9410, max=0.9987, mean=0.0008, std=0.1114
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8623, max=0.8574, mean=0.0008, std=0.1111
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8623, max=0.8574, mean=0.0008, std=0.1111
[CLAP PIPELINE] Sample 0: 129901/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 42/10586 [00:13<54:29,  3.22it/s, v_num=c9zf, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8091, mean=-0.0016, std=0.1717
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9570, max=0.8037, mean=-0.0016, std=0.1718
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9570, max=0.8037, mean=-0.0016, std=0.1718
[CLAP PIPELINE] Sample 0: 84771/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 43/10586 [00:12<52:19,  3.36it/s, v_num=c9zf, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8915, max=0.9999, mean=-0.0001, std=0.0858
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8911, max=1.0000, mean=-0.0001, std=0.0858
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8911, max=1.0000, mean=-0.0001, std=0.0858
[CLAP PIPELINE] Sample 0: 134117/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 44/10586 [00:13<52:22,  3.35it/s, v_num=c9zf, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8860, max=0.8689, mean=-0.0001, std=0.1662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Sample 0: 111849/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 45/10586 [00:13<52:20,  3.36it/s, v_num=c9zf, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.9639, mean=0.0002, std=0.0911
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9595, max=0.9629, mean=0.0002, std=0.0911
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9595, max=0.9629, mean=0.0002, std=0.0911
[CLAP PIPELINE] Sample 0: 101845/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 46/10586 [00:13<52:32,  3.34it/s, v_num=c9zf, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9609, max=0.9900, mean=-0.0000, std=0.1076
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9565, max=0.9468, mean=-0.0000, std=0.1075
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9565, max=0.9468, mean=-0.0000, std=0.1075
[CLAP PIPELINE] Sample 0: 52094/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 47/10586 [00:13<52:05,  3.37it/s, v_num=c9zf, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9793, max=0.9997, mean=0.0003, std=0.1813
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9644, max=0.9917, mean=0.0003, std=0.1813
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9644, max=0.9917, mean=0.0003, std=0.1813
[CLAP PIPELINE] Sample 0: 69162/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 48/10586 [00:14<51:55,  3.38it/s, v_num=c9zf, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.5452, max=1.0035, mean=0.0012, std=0.1025
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5444, max=1.0029, mean=0.0012, std=0.1025
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5444, max=1.0029, mean=0.0012, std=0.1025
[CLAP PIPELINE] Sample 0: 119345/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 49/10586 [00:14<51:59,  3.38it/s, v_num=c9zf, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9915, max=0.7667, mean=-0.0008, std=0.0252
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Sample 0: 104639/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 50/10586 [00:14<52:04,  3.37it/s, v_num=c9zf, train/loss=nan.0, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0745, max=1.0750, mean=0.0004, std=0.1761
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0283, max=1.0527, mean=0.0004, std=0.1761
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0283, max=1.0527, mean=0.0004, std=0.1761
[CLAP PIPELINE] Sample 0: 38948/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 51/10586 [00:14<51:35,  3.40it/s, v_num=c9zf, train/loss=nan.0, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=0.8610, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0: 148709/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 52/10586 [00:15<51:26,  3.41it/s, v_num=c9zf, train/loss=nan.0, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8210, max=1.0005, mean=0.0000, std=0.0306
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8125, max=0.9976, mean=0.0000, std=0.0306
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8125, max=0.9976, mean=0.0000, std=0.0306
[CLAP PIPELINE] Sample 0: 139913/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 53/10586 [00:15<51:25,  3.41it/s, v_num=c9zf, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9396, mean=-0.0023, std=0.0912
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0127, max=0.9453, mean=-0.0023, std=0.0912
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0127, max=0.9453, mean=-0.0023, std=0.0912
[CLAP PIPELINE] Sample 0: 113653/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 54/10586 [00:15<51:14,  3.43it/s, v_num=c9zf, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.5613, mean=-0.0000, std=0.0136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0: 18388/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 55/10586 [00:16<51:23,  3.42it/s, v_num=c9zf, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0767, max=1.0207, mean=-0.0001, std=0.2233
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8193, max=0.8135, mean=-0.0001, std=0.2155
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8193, max=0.8135, mean=-0.0001, std=0.2155
[CLAP PIPELINE] Sample 0: 35158/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 56/10586 [00:16<51:19,  3.42it/s, v_num=c9zf, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9968, max=0.9985, mean=0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0166, max=1.0215, mean=0.0000, std=0.1312
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0166, max=1.0215, mean=0.0000, std=0.1312
[CLAP PIPELINE] Sample 0: 86803/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 57/10586 [00:16<51:01,  3.44it/s, v_num=c9zf, train/loss=nan.0, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9992, max=0.9896, mean=-0.0005, std=0.0853
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9897, mean=-0.0005, std=0.0853
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9897, mean=-0.0005, std=0.0853
[CLAP PIPELINE] Sample 0: 137825/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 58/10586 [00:16<50:53,  3.45it/s, v_num=c9zf, train/loss=nan.0, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8806, max=0.7257, mean=-0.0001, std=0.0129
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6597, max=0.4893, mean=-0.0001, std=0.0100
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6597, max=0.4893, mean=-0.0001, std=0.0100
[CLAP PIPELINE] Sample 0: 47912/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 59/10586 [00:17<50:56,  3.44it/s, v_num=c9zf, train/loss=nan.0, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9983, mean=0.0005, std=0.1294
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Sample 0: 140620/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 60/10586 [00:17<50:55,  3.44it/s, v_num=c9zf, train/loss=nan.0, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0024, max=0.6007, mean=-0.0007, std=0.0930
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0: 133106/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 61/10586 [00:17<50:45,  3.46it/s, v_num=c9zf, train/loss=nan.0, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9958, max=0.9515, mean=-0.0001, std=0.0717
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9600, max=0.8765, mean=-0.0001, std=0.0699
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9600, max=0.8765, mean=-0.0001, std=0.0699
[CLAP PIPELINE] Sample 0: 36790/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 62/10586 [00:17<50:47,  3.45it/s, v_num=c9zf, train/loss=nan.0, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9835, max=0.8843, mean=-0.0000, std=0.2600
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9609, max=0.8481, mean=-0.0000, std=0.2581
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9609, max=0.8481, mean=-0.0000, std=0.2581
[CLAP PIPELINE] Sample 0: 26255/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 63/10586 [00:17<49:27,  3.55it/s, v_num=c9zf, train/loss=nan.0, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9953, max=0.9269, mean=-0.0007, std=0.0796
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Sample 0: 119923/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 64/10586 [00:18<49:31,  3.54it/s, v_num=c9zf, train/loss=nan.0, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9969, max=0.9446, mean=-0.0009, std=0.0720
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9927, max=0.9448, mean=-0.0009, std=0.0720
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9927, max=0.9448, mean=-0.0009, std=0.0720
[CLAP PIPELINE] Sample 0: 129531/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 65/10586 [00:18<49:27,  3.55it/s, v_num=c9zf, train/loss=nan.0, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7351, max=0.7890, mean=-0.0000, std=0.0251
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.3665, max=0.3438, mean=-0.0000, std=0.0220
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.3665, max=0.3438, mean=-0.0000, std=0.0220
[CLAP PIPELINE] Sample 0: 17026/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 66/10586 [00:18<49:14,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9456, max=0.9481, mean=-0.0003, std=0.0849
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8320, max=0.8291, mean=-0.0003, std=0.0840
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8320, max=0.8291, mean=-0.0003, std=0.0840
[CLAP PIPELINE] Sample 0: 144094/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 67/10586 [00:18<49:18,  3.55it/s, v_num=c9zf, train/loss=nan.0, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9955, max=0.8652, mean=-0.0000, std=0.0358
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9419, max=0.7778, mean=-0.0000, std=0.0352
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9419, max=0.7778, mean=-0.0000, std=0.0352
[CLAP PIPELINE] Sample 0: 130987/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 68/10586 [00:19<49:21,  3.55it/s, v_num=c9zf, train/loss=nan.0, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9680, max=0.9256, mean=-0.0028, std=0.1273
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7139, max=0.8584, mean=-0.0028, std=0.1180
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7139, max=0.8584, mean=-0.0028, std=0.1180
[CLAP PIPELINE] Sample 0: 14928/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 69/10586 [00:19<49:15,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9638, max=0.9958, mean=-0.0016, std=0.1701
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9717, max=0.9648, mean=-0.0016, std=0.1698
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9717, max=0.9648, mean=-0.0016, std=0.1698
[CLAP PIPELINE] Sample 0: 82212/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 70/10586 [00:19<49:16,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8680, max=0.9994, mean=0.0003, std=0.0444
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8667, max=0.9995, mean=0.0003, std=0.0444
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8667, max=0.9995, mean=0.0003, std=0.0444
[CLAP PIPELINE] Sample 0: 153881/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 71/10586 [00:20<49:26,  3.54it/s, v_num=c9zf, train/loss=nan.0, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0068, max=0.9876, mean=-0.0000, std=0.1221
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0068, max=0.9868, mean=-0.0000, std=0.1221
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0068, max=0.9868, mean=-0.0000, std=0.1221
[CLAP PIPELINE] Sample 0: 115119/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 72/10586 [00:20<49:26,  3.54it/s, v_num=c9zf, train/loss=nan.0, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8416, max=1.0099, mean=-0.0003, std=0.0124
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8896, max=0.8569, mean=-0.0003, std=0.0116
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8896, max=0.8569, mean=-0.0003, std=0.0116
[CLAP PIPELINE] Sample 0: 150683/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 73/10586 [00:20<49:18,  3.55it/s, v_num=c9zf, train/loss=nan.0, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9876, max=1.0462, mean=-0.0003, std=0.0745
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0: 92412/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 74/10586 [00:20<49:11,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.5613, mean=-0.0000, std=0.0136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0: 18388/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 75/10586 [00:20<49:02,  3.57it/s, v_num=c9zf, train/loss=nan.0, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0118, max=1.0184, mean=0.0002, std=0.1353
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0156, max=1.0693, mean=0.0002, std=0.1348
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0156, max=1.0693, mean=0.0002, std=0.1348
[CLAP PIPELINE] Sample 0: 30825/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 76/10586 [00:21<49:06,  3.57it/s, v_num=c9zf, train/loss=nan.0, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9280, max=0.9990, mean=-0.0000, std=0.0785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0: 50632/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                     | 77/10586 [00:21<49:18,  3.55it/s, v_num=c9zf, train/loss=nan.0, lr=2.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9035, mean=-0.0000, std=0.1785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Sample 0: 78920/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                     | 78/10586 [00:21<49:12,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9512, max=1.0018, mean=-0.0001, std=0.0323
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8413, max=0.8599, mean=-0.0001, std=0.0254
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8413, max=0.8599, mean=-0.0001, std=0.0254
[CLAP PIPELINE] Sample 0: 156966/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2229, max=14.2229, mean=14.2229
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                     | 79/10586 [00:22<49:19,  3.55it/s, v_num=c9zf, train/loss=nan.0, lr=2.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9952, max=0.9767, mean=-0.0001, std=0.0806
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0: 140476/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 80/10586 [00:22<49:23,  3.54it/s, v_num=c9zf, train/loss=nan.0, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9555, max=0.9872, mean=-0.0000, std=0.2328
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9482, max=0.9893, mean=-0.0000, std=0.2327
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9482, max=0.9893, mean=-0.0000, std=0.2327
[CLAP PIPELINE] Sample 0: 80486/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 81/10586 [00:22<49:28,  3.54it/s, v_num=c9zf, train/loss=nan.0, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9952, max=0.9767, mean=-0.0001, std=0.0806
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0: 140476/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 82/10586 [00:22<48:23,  3.62it/s, v_num=c9zf, train/loss=nan.0, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0640, max=0.6390, mean=0.0000, std=0.0066
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5913, max=0.5054, mean=0.0000, std=0.0059
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5913, max=0.5054, mean=0.0000, std=0.0059
[CLAP PIPELINE] Sample 0: 152282/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 83/10586 [00:22<48:24,  3.62it/s, v_num=c9zf, train/loss=nan.0, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8165, max=0.7935, mean=-0.0002, std=0.1749
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6182, max=0.5903, mean=-0.0002, std=0.1547
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6182, max=0.5903, mean=-0.0002, std=0.1547
[CLAP PIPELINE] Sample 0: 47653/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 84/10586 [00:23<48:33,  3.60it/s, v_num=c9zf, train/loss=nan.0, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0084, mean=-0.0001, std=0.1935
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Sample 0: 39111/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 85/10586 [00:23<48:30,  3.61it/s, v_num=c9zf, train/loss=nan.0, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9893, max=1.0040, mean=0.0002, std=0.0502
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9502, max=0.9209, mean=0.0002, std=0.0501
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9502, max=0.9209, mean=0.0002, std=0.0501
[CLAP PIPELINE] Sample 0: 157789/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 86/10586 [00:23<48:30,  3.61it/s, v_num=c9zf, train/loss=nan.0, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9057, max=0.9394, mean=0.0000, std=0.0707
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0: 51621/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 87/10586 [00:24<48:32,  3.60it/s, v_num=c9zf, train/loss=nan.0, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7803, max=0.9980, mean=-0.0001, std=0.1884
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7729, max=0.8696, mean=-0.0001, std=0.1881
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7729, max=0.8696, mean=-0.0001, std=0.1881
[CLAP PIPELINE] Sample 0: 0/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1710, max=0.1165, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.9617, max=-0.9617, mean=-0.9617
[SIMILARITY] C_im_clap: min=-0.9661, max=-0.9661, mean=-0.9661
Epoch 0:   1%|▋                                                                                    | 88/10586 [00:24<48:43,  3.59it/s, v_num=c9zf, train/loss=-0.00, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9847, mean=-0.0013, std=0.1384
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9766, max=0.9678, mean=-0.0013, std=0.1385
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9766, max=0.9678, mean=-0.0013, std=0.1385
[CLAP PIPELINE] Sample 0: 145784/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 89/10586 [00:24<48:57,  3.57it/s, v_num=c9zf, train/loss=nan.0, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9994, mean=-0.0028, std=0.1001
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9595, max=0.9692, mean=-0.0028, std=0.1001
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9595, max=0.9692, mean=-0.0028, std=0.1001
[CLAP PIPELINE] Sample 0: 127126/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 90/10586 [00:25<49:05,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=0.9759, mean=-0.0001, std=0.0569
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9688, max=0.9858, mean=-0.0001, std=0.0569
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9688, max=0.9858, mean=-0.0001, std=0.0569
[CLAP PIPELINE] Sample 0: 138091/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 91/10586 [00:25<49:06,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9872, mean=0.0001, std=0.5035
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0186, max=1.0156, mean=0.0001, std=0.5015
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0186, max=1.0156, mean=0.0001, std=0.5015
[CLAP PIPELINE] Sample 0: 48080/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 92/10586 [00:25<49:09,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9034, max=1.0000, mean=-0.0000, std=0.1119
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0: 63474/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 93/10586 [00:26<49:03,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9234, max=0.9909, mean=-0.0019, std=0.1012
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8613, max=0.9893, mean=-0.0019, std=0.1000
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8613, max=0.9893, mean=-0.0019, std=0.1000
[CLAP PIPELINE] Sample 0: 115693/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 94/10586 [00:26<49:02,  3.57it/s, v_num=c9zf, train/loss=nan.0, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9149, max=0.9979, mean=-0.0002, std=0.1847
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9126, max=0.9985, mean=-0.0002, std=0.1847
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9126, max=0.9985, mean=-0.0002, std=0.1847
[CLAP PIPELINE] Sample 0: 126339/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 95/10586 [00:26<48:59,  3.57it/s, v_num=c9zf, train/loss=nan.0, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9921, max=0.6521, mean=-0.0003, std=0.1274
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0107, max=0.6460, mean=-0.0003, std=0.1272
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0107, max=0.6460, mean=-0.0003, std=0.1272
[CLAP PIPELINE] Sample 0: 70436/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 96/10586 [00:26<49:02,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0097, max=0.9844, mean=0.0002, std=0.0780
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0264, max=0.9795, mean=0.0002, std=0.0779
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0264, max=0.9795, mean=0.0002, std=0.0779
[CLAP PIPELINE] Sample 0: 53226/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2787, max=14.2787, mean=14.2787
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 97/10586 [00:27<49:03,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9991, mean=-0.0000, std=0.2045
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9912, max=0.9946, mean=-0.0000, std=0.2046
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9912, max=0.9946, mean=-0.0000, std=0.2046
[CLAP PIPELINE] Sample 0: 45695/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 98/10586 [00:27<49:06,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9035, mean=-0.0000, std=0.1785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Sample 0: 78920/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 99/10586 [00:27<49:06,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9433, max=0.9782, mean=-0.0005, std=0.0523
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8989, max=0.8970, mean=-0.0005, std=0.0514
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8989, max=0.8970, mean=-0.0005, std=0.0514
[CLAP PIPELINE] Sample 0: 129997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 100/10586 [00:28<49:01,  3.56it/s, v_num=c9zf, train/loss=nan.0, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0004, max=0.8341, mean=-0.0000, std=0.0354
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9868, max=0.8340, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9868, max=0.8340, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Sample 0: 65626/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 101/10586 [00:27<48:04,  3.63it/s, v_num=c9zf, train/loss=nan.0, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8739, max=0.9986, mean=-0.0000, std=0.0380
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0: 61484/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 102/10586 [00:28<48:02,  3.64it/s, v_num=c9zf, train/loss=nan.0, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6688, max=0.7300, mean=-0.0000, std=0.0359
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6216, max=0.6060, mean=-0.0000, std=0.0346
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6216, max=0.6060, mean=-0.0000, std=0.0346
[CLAP PIPELINE] Sample 0: 127682/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 103/10586 [00:28<48:05,  3.63it/s, v_num=c9zf, train/loss=nan.0, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0005, max=0.9991, mean=-0.0000, std=0.0704
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9990, mean=-0.0000, std=0.0704
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9990, mean=-0.0000, std=0.0704
[CLAP PIPELINE] Sample 0: 95839/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 104/10586 [00:28<48:02,  3.64it/s, v_num=c9zf, train/loss=nan.0, lr=2.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9232, max=1.0015, mean=-0.0001, std=0.1578
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9556, mean=-0.0001, std=0.1578
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9556, mean=-0.0001, std=0.1578
[CLAP PIPELINE] Sample 0: 40567/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 105/10586 [00:28<47:59,  3.64it/s, v_num=c9zf, train/loss=nan.0, lr=2.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.9991, mean=-0.0015, std=0.0866
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9985, mean=-0.0015, std=0.0866
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.9985, mean=-0.0015, std=0.0866
[CLAP PIPELINE] Sample 0: 85296/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 106/10586 [00:29<47:59,  3.64it/s, v_num=c9zf, train/loss=nan.0, lr=2.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9216, max=0.9771, mean=-0.0000, std=0.0947
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8647, max=0.9453, mean=-0.0000, std=0.0916
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8647, max=0.9453, mean=-0.0000, std=0.0916
[CLAP PIPELINE] Sample 0: 77930/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 107/10586 [00:29<48:02,  3.64it/s, v_num=c9zf, train/loss=nan.0, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0137, max=1.0162, mean=-0.0000, std=0.1196
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9604, max=0.9634, mean=-0.0000, std=0.1135
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9604, max=0.9634, mean=-0.0000, std=0.1135
[CLAP PIPELINE] Sample 0: 151829/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 108/10586 [00:29<47:59,  3.64it/s, v_num=c9zf, train/loss=nan.0, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0: 47237/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 109/10586 [00:29<47:57,  3.64it/s, v_num=c9zf, train/loss=nan.0, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0: 47237/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 110/10586 [00:30<47:56,  3.64it/s, v_num=c9zf, train/loss=nan.0, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7465, max=0.9657, mean=-0.0000, std=0.0711
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7476, max=0.9722, mean=-0.0000, std=0.0708
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7476, max=0.9722, mean=-0.0000, std=0.0708
[CLAP PIPELINE] Sample 0: 65192/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 111/10586 [00:30<47:55,  3.64it/s, v_num=c9zf, train/loss=nan.0, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8718, max=0.9936, mean=-0.0003, std=0.0920
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8423, max=0.9917, mean=-0.0003, std=0.0919
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8423, max=0.9917, mean=-0.0003, std=0.0919
[CLAP PIPELINE] Sample 0: 57622/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 112/10586 [00:30<47:50,  3.65it/s, v_num=c9zf, train/loss=nan.0, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=0.8367, mean=-0.0003, std=0.0839
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9971, max=0.8145, mean=-0.0003, std=0.0839
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9971, max=0.8145, mean=-0.0003, std=0.0839
[CLAP PIPELINE] Sample 0: 120295/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 113/10586 [00:30<47:51,  3.65it/s, v_num=c9zf, train/loss=nan.0, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9967, max=0.8892, mean=-0.0005, std=0.1574
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=0.8481, mean=-0.0005, std=0.1570
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=0.8481, mean=-0.0005, std=0.1570
[CLAP PIPELINE] Sample 0: 12270/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 114/10586 [00:31<47:48,  3.65it/s, v_num=c9zf, train/loss=nan.0, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9042, max=0.9983, mean=-0.0000, std=0.1139
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9048, max=0.9966, mean=-0.0000, std=0.1140
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9048, max=0.9966, mean=-0.0000, std=0.1140
[CLAP PIPELINE] Sample 0: 124314/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                    | 115/10586 [00:31<47:46,  3.65it/s, v_num=c9zf, train/loss=nan.0, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9555, max=0.9872, mean=-0.0000, std=0.2328
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9482, max=0.9893, mean=-0.0000, std=0.2327
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9482, max=0.9893, mean=-0.0000, std=0.2327
[CLAP PIPELINE] Sample 0: 80486/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                    | 116/10586 [00:31<47:47,  3.65it/s, v_num=c9zf, train/loss=nan.0, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=0.7591, mean=-0.0002, std=0.1469
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9961, max=0.7500, mean=-0.0002, std=0.1467
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9961, max=0.7500, mean=-0.0002, std=0.1467
[CLAP PIPELINE] Sample 0: 8290/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                    | 117/10586 [00:32<47:47,  3.65it/s, v_num=c9zf, train/loss=nan.0, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9953, max=0.9269, mean=-0.0007, std=0.0796
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Sample 0: 119923/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                    | 118/10586 [00:32<47:49,  3.65it/s, v_num=c9zf, train/loss=nan.0, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9713, max=0.9290, mean=-0.0000, std=0.1432
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7432, max=0.7358, mean=-0.0000, std=0.1395
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7432, max=0.7358, mean=-0.0000, std=0.1395
[CLAP PIPELINE] Sample 0: 67822/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 119/10586 [00:32<47:45,  3.65it/s, v_num=c9zf, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9976, max=0.9832, mean=-0.0000, std=0.0619
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0039, max=1.0049, mean=-0.0000, std=0.0618
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0039, max=1.0049, mean=-0.0000, std=0.0618
[CLAP PIPELINE] Sample 0: 139443/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 120/10586 [00:32<47:44,  3.65it/s, v_num=c9zf, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9559, max=0.6156, mean=0.0001, std=0.0153
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0: 150065/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 121/10586 [00:32<47:05,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0003, max=0.8273, mean=-0.0025, std=0.1678
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0: 24967/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 122/10586 [00:32<47:00,  3.71it/s, v_num=c9zf, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9729, max=0.8797, mean=-0.0026, std=0.1368
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9287, max=0.8765, mean=-0.0026, std=0.1364
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9287, max=0.8765, mean=-0.0026, std=0.1364
[CLAP PIPELINE] Sample 0: 100571/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2299, max=14.2299, mean=14.2299
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 123/10586 [00:33<47:02,  3.71it/s, v_num=c9zf, train/loss=nan.0, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.8496, mean=-0.0000, std=0.0451
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0: 101891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 124/10586 [00:33<47:02,  3.71it/s, v_num=c9zf, train/loss=nan.0, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.9657, mean=0.0000, std=0.2225
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9663, mean=0.0000, std=0.2227
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9663, mean=0.0000, std=0.2227
[CLAP PIPELINE] Sample 0: 41608/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 125/10586 [00:33<47:10,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.6950, mean=-0.0000, std=0.0353
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.6948, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.6948, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Sample 0: 125328/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 126/10586 [00:34<47:08,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9178, max=0.9936, mean=0.1238, std=0.4039
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=0.9927, mean=0.1239, std=0.4033
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=0.9927, mean=0.1239, std=0.4033
[CLAP PIPELINE] Sample 0: 22882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 127/10586 [00:34<47:05,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9983, mean=0.0005, std=0.1294
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Sample 0: 140620/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 128/10586 [00:34<47:00,  3.71it/s, v_num=c9zf, train/loss=nan.0, lr=2.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9908, max=1.0015, mean=-0.0000, std=0.0681
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9399, max=0.9409, mean=-0.0000, std=0.0680
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9399, max=0.9409, mean=-0.0000, std=0.0680
[CLAP PIPELINE] Sample 0: 109221/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 129/10586 [00:34<47:06,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=0.8610, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0: 148709/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 130/10586 [00:35<47:11,  3.69it/s, v_num=c9zf, train/loss=nan.0, lr=2.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0007, max=0.9994, mean=0.0000, std=0.2048
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9995, mean=0.0000, std=0.2048
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9995, mean=0.0000, std=0.2048
[CLAP PIPELINE] Sample 0: 88055/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 131/10586 [00:35<47:04,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9133, max=0.9307, mean=0.0004, std=0.0840
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7100, max=0.7236, mean=0.0004, std=0.0832
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7100, max=0.7236, mean=0.0004, std=0.0832
[CLAP PIPELINE] Sample 0: 113946/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 132/10586 [00:35<47:05,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7768, max=0.9973, mean=0.0008, std=0.1139
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7759, max=1.0000, mean=0.0008, std=0.1140
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7759, max=1.0000, mean=0.0008, std=0.1140
[CLAP PIPELINE] Sample 0: 124926/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 133/10586 [00:35<47:06,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9972, max=0.7821, mean=-0.0000, std=0.0092
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9771, max=0.6641, mean=-0.0000, std=0.0087
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9771, max=0.6641, mean=-0.0000, std=0.0087
[CLAP PIPELINE] Sample 0: 151821/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 134/10586 [00:36<47:05,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0018, max=1.0100, mean=-0.0002, std=0.1225
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=1.0078, mean=-0.0002, std=0.1226
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=1.0078, mean=-0.0002, std=0.1226
[CLAP PIPELINE] Sample 0: 150303/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 135/10586 [00:36<47:04,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9904, max=0.9272, mean=0.0027, std=0.1281
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9814, max=0.9062, mean=0.0027, std=0.1281
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9814, max=0.9062, mean=0.0027, std=0.1281
[CLAP PIPELINE] Sample 0: 95003/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 136/10586 [00:36<47:05,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8497, max=0.9994, mean=-0.0007, std=0.2589
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8501, max=1.0000, mean=-0.0008, std=0.2590
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=1.0000, mean=-0.0008, std=0.2590
[CLAP PIPELINE] Sample 0: 86957/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 137/10586 [00:37<47:05,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8902, max=1.0016, mean=-0.0000, std=0.1195
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8936, max=1.0020, mean=-0.0000, std=0.1195
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8936, max=1.0020, mean=-0.0000, std=0.1195
[CLAP PIPELINE] Sample 0: 117088/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 138/10586 [00:37<47:05,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9280, max=0.9990, mean=-0.0000, std=0.0785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0: 50632/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 139/10586 [00:37<47:06,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9044, mean=-0.0008, std=0.2235
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.8965, mean=-0.0008, std=0.2236
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.8965, mean=-0.0008, std=0.2236
[CLAP PIPELINE] Sample 0: 15391/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 140/10586 [00:37<47:06,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8628, max=0.9054, mean=-0.0000, std=0.0504
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.3704, max=0.3982, mean=-0.0000, std=0.0234
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.3704, max=0.3982, mean=-0.0000, std=0.0234
[CLAP PIPELINE] Sample 0: 20380/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 141/10586 [00:38<47:00,  3.70it/s, v_num=c9zf, train/loss=nan.0, lr=2.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9621, max=0.9844, mean=-0.0000, std=0.0176
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8643, max=0.8418, mean=-0.0000, std=0.0169
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8643, max=0.8418, mean=-0.0000, std=0.0169
[CLAP PIPELINE] Sample 0: 148157/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 142/10586 [00:37<46:18,  3.76it/s, v_num=c9zf, train/loss=nan.0, lr=2.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.7417, mean=-0.0004, std=0.1304
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9053, max=0.7163, mean=-0.0004, std=0.1290
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9053, max=0.7163, mean=-0.0004, std=0.1290
[CLAP PIPELINE] Sample 0: 111905/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 143/10586 [00:38<46:18,  3.76it/s, v_num=c9zf, train/loss=nan.0, lr=2.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9456, max=0.9481, mean=-0.0003, std=0.0849
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8320, max=0.8291, mean=-0.0003, std=0.0840
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8320, max=0.8291, mean=-0.0003, std=0.0840
[CLAP PIPELINE] Sample 0: 144094/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 144/10586 [00:38<46:14,  3.76it/s, v_num=c9zf, train/loss=nan.0, lr=2.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9329, max=0.9947, mean=-0.0000, std=0.1836
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9053, max=0.9912, mean=-0.0000, std=0.1829
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9053, max=0.9912, mean=-0.0000, std=0.1829
[CLAP PIPELINE] Sample 0: 34328/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 145/10586 [00:38<46:08,  3.77it/s, v_num=c9zf, train/loss=nan.0, lr=2.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8896, max=0.9998, mean=-0.0000, std=0.1505
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8853, max=0.9961, mean=-0.0000, std=0.1505
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8853, max=0.9961, mean=-0.0000, std=0.1505
[CLAP PIPELINE] Sample 0: 86549/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 146/10586 [00:38<46:06,  3.77it/s, v_num=c9zf, train/loss=nan.0, lr=2.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9600, max=1.0005, mean=-0.0000, std=0.2673
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9585, max=0.9824, mean=-0.0000, std=0.2673
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9585, max=0.9824, mean=-0.0000, std=0.2673
[CLAP PIPELINE] Sample 0: 55875/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 147/10586 [00:38<46:01,  3.78it/s, v_num=c9zf, train/loss=nan.0, lr=2.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9823, max=0.6926, mean=-0.0307, std=0.0572
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8652, max=0.5552, mean=-0.0308, std=0.0558
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8652, max=0.5552, mean=-0.0308, std=0.0558
[CLAP PIPELINE] Sample 0: 79775/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 148/10586 [00:39<45:57,  3.78it/s, v_num=c9zf, train/loss=nan.0, lr=2.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0: 47237/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 149/10586 [00:39<45:58,  3.78it/s, v_num=c9zf, train/loss=nan.0, lr=2.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9676, max=1.0002, mean=-0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0: 25933/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 150/10586 [00:39<45:50,  3.79it/s, v_num=c9zf, train/loss=nan.0, lr=2.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8999, max=1.0054, mean=-0.0001, std=0.0841
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8789, max=0.7700, mean=-0.0001, std=0.0834
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8789, max=0.7700, mean=-0.0001, std=0.0834
[CLAP PIPELINE] Sample 0: 51329/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 151/10586 [00:39<45:44,  3.80it/s, v_num=c9zf, train/loss=nan.0, lr=2.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9621, max=0.9844, mean=-0.0000, std=0.0176
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8643, max=0.8418, mean=-0.0000, std=0.0169
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8643, max=0.8418, mean=-0.0000, std=0.0169
[CLAP PIPELINE] Sample 0: 148157/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 152/10586 [00:39<45:38,  3.81it/s, v_num=c9zf, train/loss=nan.0, lr=2.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=1.0036, mean=-0.0002, std=0.5019
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0859, max=1.0859, mean=-0.0002, std=0.5000
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0859, max=1.0859, mean=-0.0002, std=0.5000
[CLAP PIPELINE] Sample 0: 87995/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 153/10586 [00:40<45:36,  3.81it/s, v_num=c9zf, train/loss=nan.0, lr=2.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9953, max=0.7769, mean=-0.0001, std=0.0524
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9912, max=0.7764, mean=-0.0001, std=0.0524
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9912, max=0.7764, mean=-0.0001, std=0.0524
[CLAP PIPELINE] Sample 0: 80309/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                   | 154/10586 [00:40<45:35,  3.81it/s, v_num=c9zf, train/loss=nan.0, lr=2.9e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9965, mean=-0.0004, std=0.1406
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0: 27046/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                   | 155/10586 [00:40<45:38,  3.81it/s, v_num=c9zf, train/loss=nan.0, lr=2.9e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9950, mean=0.0001, std=0.2440
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9956, mean=0.0001, std=0.2441
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9956, mean=0.0001, std=0.2441
[CLAP PIPELINE] Sample 0: 93115/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▎                                                                                   | 156/10586 [00:40<45:35,  3.81it/s, v_num=c9zf, train/loss=nan.0, lr=2.9e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9344, max=0.9998, mean=0.0001, std=0.3089
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9346, max=1.0000, mean=0.0001, std=0.3091
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9346, max=1.0000, mean=0.0001, std=0.3091
[CLAP PIPELINE] Sample 0: 48232/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 157/10586 [00:41<45:46,  3.80it/s, v_num=c9zf, train/loss=nan.0, lr=2.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8681, max=0.9757, mean=0.0000, std=0.1035
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7969, max=0.9199, mean=0.0000, std=0.0999
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7969, max=0.9199, mean=0.0000, std=0.0999
[CLAP PIPELINE] Sample 0: 93552/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▎                                                                                  | 158/10586 [00:41<45:41,  3.80it/s, v_num=c9zf, train/loss=nan.0, lr=2.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0416, max=1.0553, mean=0.0000, std=0.0665
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9888, max=0.8647, mean=0.0000, std=0.0650
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9888, max=0.8647, mean=0.0000, std=0.0650
[CLAP PIPELINE] Sample 0: 49257/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 159/10586 [00:41<45:50,  3.79it/s, v_num=c9zf, train/loss=nan.0, lr=2.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0030, max=1.0159, mean=0.0009, std=0.4255
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0264, max=1.0391, mean=0.0009, std=0.4180
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0264, max=1.0391, mean=0.0009, std=0.4180
[CLAP PIPELINE] Sample 0: 88003/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 160/10586 [00:42<45:49,  3.79it/s, v_num=c9zf, train/loss=nan.0, lr=2.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9998, max=0.9477, mean=-0.0004, std=0.0566
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9590, max=0.8774, mean=-0.0004, std=0.0565
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9590, max=0.8774, mean=-0.0004, std=0.0565
[CLAP PIPELINE] Sample 0: 143465/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 161/10586 [00:42<45:49,  3.79it/s, v_num=c9zf, train/loss=nan.0, lr=2.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9955, max=0.8652, mean=-0.0000, std=0.0358
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9419, max=0.7778, mean=-0.0000, std=0.0352
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9419, max=0.7778, mean=-0.0000, std=0.0352
[CLAP PIPELINE] Sample 0: 130987/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 162/10586 [00:42<45:43,  3.80it/s, v_num=c9zf, train/loss=nan.0, lr=2.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9634, mean=0.0000, std=0.0657
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9917, max=0.9473, mean=0.0000, std=0.0657
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9917, max=0.9473, mean=0.0000, std=0.0657
[CLAP PIPELINE] Sample 0: 76982/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 163/10586 [00:42<45:45,  3.80it/s, v_num=c9zf, train/loss=nan.0, lr=2.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9899, max=0.9946, mean=-0.0022, std=0.1272
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9858, max=0.9927, mean=-0.0022, std=0.1272
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9858, max=0.9927, mean=-0.0022, std=0.1272
[CLAP PIPELINE] Sample 0: 109744/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 164/10586 [00:42<45:15,  3.84it/s, v_num=c9zf, train/loss=nan.0, lr=2.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9947, mean=-0.0000, std=0.2093
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9951, mean=-0.0000, std=0.2094
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9951, mean=-0.0000, std=0.2094
[CLAP PIPELINE] Sample 0: 95891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 165/10586 [00:43<45:17,  3.83it/s, v_num=c9zf, train/loss=nan.0, lr=2.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.8706, mean=-0.0054, std=0.1878
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.8677, mean=-0.0054, std=0.1879
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.8677, mean=-0.0054, std=0.1879
[CLAP PIPELINE] Sample 0: 79853/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 166/10586 [00:43<45:16,  3.84it/s, v_num=c9zf, train/loss=nan.0, lr=2.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9280, max=0.9990, mean=-0.0000, std=0.0785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0: 50632/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 167/10586 [00:43<45:16,  3.83it/s, v_num=c9zf, train/loss=nan.0, lr=2.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0018, max=0.9247, mean=-0.0000, std=0.4909
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0059, max=0.9419, mean=-0.0000, std=0.4905
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0059, max=0.9419, mean=-0.0000, std=0.4905
[CLAP PIPELINE] Sample 0: 17289/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 168/10586 [00:43<45:19,  3.83it/s, v_num=c9zf, train/loss=nan.0, lr=2.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0003, max=0.8273, mean=-0.0025, std=0.1678
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0: 24967/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 169/10586 [00:44<45:18,  3.83it/s, v_num=c9zf, train/loss=nan.0, lr=2.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0570, max=1.0064, mean=0.0063, std=0.1807
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9360, max=0.9819, mean=0.0063, std=0.1780
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9360, max=0.9819, mean=0.0063, std=0.1780
[CLAP PIPELINE] Sample 0: 6/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 170/10586 [00:44<45:19,  3.83it/s, v_num=c9zf, train/loss=nan.0, lr=2.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9951, max=1.0012, mean=0.0000, std=0.1204
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9902, max=1.0068, mean=0.0000, std=0.1204
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9902, max=1.0068, mean=0.0000, std=0.1204
[CLAP PIPELINE] Sample 0: 45365/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 171/10586 [00:44<45:20,  3.83it/s, v_num=c9zf, train/loss=nan.0, lr=2.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.8496, mean=-0.0000, std=0.0451
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0: 101891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 172/10586 [00:44<45:23,  3.82it/s, v_num=c9zf, train/loss=nan.0, lr=2.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9980, max=0.8624, mean=-0.0004, std=0.1807
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.8608, mean=-0.0004, std=0.1807
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.8608, mean=-0.0004, std=0.1807
[CLAP PIPELINE] Sample 0: 90997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 173/10586 [00:45<45:25,  3.82it/s, v_num=c9zf, train/loss=nan.0, lr=2.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9783, max=0.9732, mean=-0.0000, std=0.0987
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9448, max=0.9648, mean=-0.0000, std=0.0987
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9448, max=0.9648, mean=-0.0000, std=0.0987
[CLAP PIPELINE] Sample 0: 50929/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 174/10586 [00:45<45:30,  3.81it/s, v_num=c9zf, train/loss=nan.0, lr=2.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9621, max=0.9983, mean=-0.0000, std=0.0337
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9614, max=0.9976, mean=-0.0000, std=0.0329
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9614, max=0.9976, mean=-0.0000, std=0.0329
[CLAP PIPELINE] Sample 0: 40001/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 175/10586 [00:45<45:32,  3.81it/s, v_num=c9zf, train/loss=nan.0, lr=2.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9904, max=0.9272, mean=0.0027, std=0.1281
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9814, max=0.9062, mean=0.0027, std=0.1281
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9814, max=0.9062, mean=0.0027, std=0.1281
[CLAP PIPELINE] Sample 0: 95003/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 176/10586 [00:46<45:39,  3.80it/s, v_num=c9zf, train/loss=nan.0, lr=2.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0000, max=0.9631, mean=-0.0008, std=0.0653
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9619, mean=-0.0008, std=0.0653
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9619, mean=-0.0008, std=0.0653
[CLAP PIPELINE] Sample 0: 111981/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 177/10586 [00:46<45:35,  3.80it/s, v_num=c9zf, train/loss=nan.0, lr=2.96e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0640, max=0.6390, mean=0.0000, std=0.0066
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5913, max=0.5054, mean=0.0000, std=0.0059
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5913, max=0.5054, mean=0.0000, std=0.0059
[CLAP PIPELINE] Sample 0: 152282/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 178/10586 [00:46<45:34,  3.81it/s, v_num=c9zf, train/loss=nan.0, lr=2.96e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9057, max=0.9394, mean=0.0000, std=0.0707
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0: 51621/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000

Detected KeyboardInterrupt, attempting graceful shutdown ...
