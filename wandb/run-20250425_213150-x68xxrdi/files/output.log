Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs

----- Training -----
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name              | Type                               | Params | Mode
---------------------------------------------------------------------------------
0 | mel               | AugmentMelSTFT                     | 0      | train
1 | imitation_encoder | MobileNetV3                        | 2.7 M  | train
2 | reference_encoder | MobileNetV3                        | 2.7 M  | train
3 | clap_model        | CLAPAudioEmbeddingClassifierFreev2 | 158 M  | eval
  | other params      | n/a                                | 2      | n/a
---------------------------------------------------------------------------------
5.4 M     Trainable params
158 M     Non-trainable params
163 M     Total params
655.085   Total estimated model params size (MB)
419       Modules in train mode
465       Modules in eval mode
Sanity Checking DataLoader 0:   0%|                                                                                                                       | 0/2 [00:00<?, ?it/s]
[VALIDATION] Batch 0 - Processing validation data
[VALIDATION] Imitation shape: torch.Size([8, 320000]), Reference shape: torch.Size([8, 320000])
[VALIDATION] Imitation: min=-1.0061, max=1.0018
[VALIDATION] Reference: min=-0.9994, max=1.0061
[CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9994, max=1.0061, mean=0.0023, std=0.2029
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9897, max=1.0088, mean=0.0023, std=0.2026
[CLAP PIPELINE] Removed 0 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Sample 0: 25/160000 near-zero values
[CLAP PIPELINE] Removed 150802 trailing zeros from sample 1
[CLAP PIPELINE] Removed 50913 trailing zeros from sample 2
[CLAP PIPELINE] Removed 39817 trailing zeros from sample 3
[CLAP PIPELINE] Removed 63823 trailing zeros from sample 4
[CLAP PIPELINE] Removed 53128 trailing zeros from sample 5
[CLAP PIPELINE] Removed 29570 trailing zeros from sample 6
[CLAP PIPELINE] Removed 157790 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[VALIDATION] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[VALIDATION] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[VALIDATION] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[VALIDATION] WARNING: NaN found in log_softmax outputs:
[VALIDATION]   - C_qvim_log: 0/64 NaN values
[VALIDATION]   - C_ref_clap_log: 64/64 NaN values
[VALIDATION]   - C_im_clap_log: 64/64 NaN values
Sanity Checking DataLoader 0:  50%|███████████████████████████████████████████████████████▌                                                       | 1/2 [00:01<00:01,  0.58it/s]
[VALIDATION] Batch 1 - Processing validation data
[VALIDATION] Imitation shape: torch.Size([8, 320000]), Reference shape: torch.Size([8, 320000])
[VALIDATION] Imitation: min=-1.0314, max=1.0148
[VALIDATION] Reference: min=-1.0045, max=1.0056
[CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0045, max=1.0056, mean=-0.0040, std=0.2030
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0049, max=1.0166, mean=-0.0040, std=0.2028
[CLAP PIPELINE] Removed 129717 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([30283])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=1.0010, mean=-0.0048, std=0.1708
[CLAP PIPELINE] Sample 0: 2/30283 near-zero values
[CLAP PIPELINE] Removed 64498 trailing zeros from sample 1
[CLAP PIPELINE] Removed 127115 trailing zeros from sample 2
[CLAP PIPELINE] Removed 69162 trailing zeros from sample 3
[CLAP PIPELINE] Removed 48054 trailing zeros from sample 4
[CLAP PIPELINE] Removed 26857 trailing zeros from sample 5
[CLAP PIPELINE] Removed 142007 trailing zeros from sample 6
[CLAP PIPELINE] Removed 88566 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[VALIDATION] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[VALIDATION] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[VALIDATION] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[VALIDATION] WARNING: NaN found in log_softmax outputs:
[VALIDATION]   - C_qvim_log: 0/64 NaN values
[VALIDATION]   - C_ref_clap_log: 64/64 NaN values
[VALIDATION]   - C_im_clap_log: 64/64 NaN values
Epoch 0:   0%|                                                                                                                                         | 0/1324 [00:00<?, ?it/s][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0043, max=1.0462, mean=-0.0002, std=0.0813
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9956, max=1.0020, mean=-0.0002, std=0.0781
[CLAP PIPELINE] Removed 75421 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([84579])
[CLAP PIPELINE] Sample 0 stats: min=-0.8896, max=0.5645, mean=0.0001, std=0.0673
[CLAP PIPELINE] Sample 0: 0/84579 near-zero values
[CLAP PIPELINE] Removed 117084 trailing zeros from sample 1
[CLAP PIPELINE] Removed 129717 trailing zeros from sample 2
[CLAP PIPELINE] Removed 92399 trailing zeros from sample 3
[CLAP PIPELINE] Removed 152753 trailing zeros from sample 4
[CLAP PIPELINE] Removed 0 trailing zeros from sample 5
[CLAP PIPELINE] Removed 104632 trailing zeros from sample 6
[CLAP PIPELINE] Removed 131310 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 3584/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 3584/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=2.7012, max=11.9838, mean=9.2057
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   0%|                                                                                      | 1/1324 [00:04<1:36:18,  0.23it/s, v_num=xrdi, train/loss=nan.0, lr=2.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=1.0119, mean=0.0038, std=0.1169
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9878, max=0.9990, mean=0.0038, std=0.1166
[CLAP PIPELINE] Removed 99636 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([60364])
[CLAP PIPELINE] Sample 0 stats: min=-0.9878, max=0.9771, mean=0.0012, std=0.2157
[CLAP PIPELINE] Sample 0: 0/60364 near-zero values
[CLAP PIPELINE] Removed 91799 trailing zeros from sample 1
[CLAP PIPELINE] Removed 87726 trailing zeros from sample 2
[CLAP PIPELINE] Removed 66889 trailing zeros from sample 3
[CLAP PIPELINE] Removed 150522 trailing zeros from sample 4
[CLAP PIPELINE] Removed 123214 trailing zeros from sample 5
[CLAP PIPELINE] Removed 95672 trailing zeros from sample 6
[CLAP PIPELINE] Removed 3117 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=2.3263, max=12.6186, mean=9.2786
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   0%|▏                                                                                      | 2/1324 [00:05<56:11,  0.39it/s, v_num=xrdi, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0033, max=1.0181, mean=-0.0009, std=0.0886
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=1.0098, mean=-0.0009, std=0.0886
[CLAP PIPELINE] Removed 27044 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([132956])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1542
[CLAP PIPELINE] Sample 0: 3/132956 near-zero values
[CLAP PIPELINE] Removed 103089 trailing zeros from sample 1
[CLAP PIPELINE] Removed 64230 trailing zeros from sample 2
[CLAP PIPELINE] Removed 87726 trailing zeros from sample 3
[CLAP PIPELINE] Removed 129717 trailing zeros from sample 4
[CLAP PIPELINE] Removed 144171 trailing zeros from sample 5
[CLAP PIPELINE] Removed 154783 trailing zeros from sample 6
[CLAP PIPELINE] Removed 127115 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 3584/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 3584/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=2.3403, max=12.4233, mean=8.3475
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   0%|▏                                                                                      | 3/1324 [00:05<42:22,  0.52it/s, v_num=xrdi, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0084, mean=-0.0014, std=0.1946
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=1.0176, mean=-0.0014, std=0.1923
[CLAP PIPELINE] Removed 39111 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([120889])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.2036
[CLAP PIPELINE] Sample 0: 1/120889 near-zero values
[CLAP PIPELINE] Removed 20050 trailing zeros from sample 1
[CLAP PIPELINE] Removed 137707 trailing zeros from sample 2
[CLAP PIPELINE] Removed 25925 trailing zeros from sample 3
[CLAP PIPELINE] Removed 50682 trailing zeros from sample 4
[CLAP PIPELINE] Removed 63026 trailing zeros from sample 5
[CLAP PIPELINE] Removed 88664 trailing zeros from sample 6
[CLAP PIPELINE] Removed 131203 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=3.6482, max=12.7372, mean=8.6773
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   0%|▎                                                                                      | 4/1324 [00:07<38:49,  0.57it/s, v_num=xrdi, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1079, max=1.1765, mean=-0.0004, std=0.1875
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.1914, max=1.1514, mean=-0.0004, std=0.1833
[CLAP PIPELINE] Removed 104008 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([55992])
[CLAP PIPELINE] Sample 0 stats: min=-0.8594, max=0.8374, mean=-0.0043, std=0.3416
[CLAP PIPELINE] Sample 0: 1/55992 near-zero values
[CLAP PIPELINE] Removed 59395 trailing zeros from sample 1
[CLAP PIPELINE] Removed 63203 trailing zeros from sample 2
[CLAP PIPELINE] Removed 63472 trailing zeros from sample 3
[CLAP PIPELINE] Removed 6092 trailing zeros from sample 4
[CLAP PIPELINE] Removed 88005 trailing zeros from sample 5
[CLAP PIPELINE] Removed 150802 trailing zeros from sample 6
[CLAP PIPELINE] Removed 112894 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=4.0562, max=13.1836, mean=8.8589
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   0%|▎                                                                                      | 5/1324 [00:08<36:02,  0.61it/s, v_num=xrdi, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0082, max=0.9999, mean=-0.0001, std=0.1276
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9644, max=1.0000, mean=-0.0001, std=0.1242
[CLAP PIPELINE] Removed 0 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6812, max=0.6206, mean=-0.0000, std=0.0112
[CLAP PIPELINE] Sample 0: 655/160000 near-zero values
[CLAP PIPELINE] Removed 129901 trailing zeros from sample 1
[CLAP PIPELINE] Removed 84577 trailing zeros from sample 2
[CLAP PIPELINE] Removed 134118 trailing zeros from sample 3
[CLAP PIPELINE] Removed 111848 trailing zeros from sample 4
[CLAP PIPELINE] Removed 101675 trailing zeros from sample 5
[CLAP PIPELINE] Removed 51816 trailing zeros from sample 6
[CLAP PIPELINE] Removed 69162 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=2.8198, max=12.7441, mean=8.2176
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   0%|▍                                                                                       | 6/1324 [00:09<35:06,  0.63it/s, v_num=xrdi, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0767, max=1.0750, mean=-0.0002, std=0.1135
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0283, max=1.0527, mean=-0.0002, std=0.1116
[CLAP PIPELINE] Removed 119300 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([40700])
[CLAP PIPELINE] Sample 0 stats: min=-0.5444, max=1.0029, mean=0.0049, std=0.2032
[CLAP PIPELINE] Sample 0: 45/40700 near-zero values
[CLAP PIPELINE] Removed 104632 trailing zeros from sample 1
[CLAP PIPELINE] Removed 35332 trailing zeros from sample 2
[CLAP PIPELINE] Removed 148710 trailing zeros from sample 3
[CLAP PIPELINE] Removed 139914 trailing zeros from sample 4
[CLAP PIPELINE] Removed 113648 trailing zeros from sample 5
[CLAP PIPELINE] Removed 18175 trailing zeros from sample 6
[CLAP PIPELINE] Removed 35158 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=7.3870, max=12.3535, mean=9.8208
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|▍                                                                                      | 7/1324 [00:10<33:16,  0.66it/s, v_num=xrdi, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0024, max=0.9985, mean=-0.0002, std=0.1271
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0166, max=1.0215, mean=-0.0002, std=0.1265
[CLAP PIPELINE] Removed 86794 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([73206])
[CLAP PIPELINE] Sample 0 stats: min=-1.0166, max=1.0215, mean=0.0000, std=0.1941
[CLAP PIPELINE] Sample 0: 11/73206 near-zero values
[CLAP PIPELINE] Removed 137707 trailing zeros from sample 1
[CLAP PIPELINE] Removed 47899 trailing zeros from sample 2
[CLAP PIPELINE] Removed 134275 trailing zeros from sample 3
[CLAP PIPELINE] Removed 133108 trailing zeros from sample 4
[CLAP PIPELINE] Removed 36782 trailing zeros from sample 5
[CLAP PIPELINE] Removed 26255 trailing zeros from sample 6
[CLAP PIPELINE] Removed 119925 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=5.3746, max=11.8792, mean=9.1025
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|▌                                                                                      | 8/1324 [00:11<31:49,  0.69it/s, v_num=xrdi, train/loss=nan.0, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0068, max=0.9994, mean=-0.0007, std=0.0977
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0068, max=0.9995, mean=-0.0007, std=0.0959
[CLAP PIPELINE] Removed 129530 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([30470])
[CLAP PIPELINE] Sample 0 stats: min=-0.9927, max=0.9448, mean=-0.0048, std=0.1648
[CLAP PIPELINE] Sample 0: 1/30470 near-zero values
[CLAP PIPELINE] Removed 17016 trailing zeros from sample 1
[CLAP PIPELINE] Removed 143687 trailing zeros from sample 2
[CLAP PIPELINE] Removed 130986 trailing zeros from sample 3
[CLAP PIPELINE] Removed 14922 trailing zeros from sample 4
[CLAP PIPELINE] Removed 82181 trailing zeros from sample 5
[CLAP PIPELINE] Removed 153870 trailing zeros from sample 6
[CLAP PIPELINE] Removed 115119 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=2.1380, max=12.7302, mean=8.7681
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|▌                                                                                      | 9/1324 [00:12<31:36,  0.69it/s, v_num=xrdi, train/loss=nan.0, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0118, max=1.0462, mean=-0.0001, std=0.0934
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0234, max=1.0693, mean=-0.0001, std=0.0930
[CLAP PIPELINE] Removed 150684 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([9316])
[CLAP PIPELINE] Sample 0 stats: min=-0.8896, max=0.8569, mean=-0.0051, std=0.0476
[CLAP PIPELINE] Sample 0: 0/9316 near-zero values
[CLAP PIPELINE] Removed 92399 trailing zeros from sample 1
[CLAP PIPELINE] Removed 18175 trailing zeros from sample 2
[CLAP PIPELINE] Removed 30825 trailing zeros from sample 3
[CLAP PIPELINE] Removed 50628 trailing zeros from sample 4
[CLAP PIPELINE] Removed 78921 trailing zeros from sample 5
[CLAP PIPELINE] Removed 156967 trailing zeros from sample 6
[CLAP PIPELINE] Removed 140475 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=3.7946, max=12.5070, mean=8.8403
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|▋                                                                                     | 10/1324 [00:13<30:34,  0.72it/s, v_num=xrdi, train/loss=nan.0, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0640, max=1.0084, mean=-0.0000, std=0.1465
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9990, mean=-0.0000, std=0.1395
[CLAP PIPELINE] Removed 80485 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([79515])
[CLAP PIPELINE] Sample 0 stats: min=-0.9482, max=0.9893, mean=-0.0001, std=0.3301
[CLAP PIPELINE] Sample 0: 1/79515 near-zero values
[CLAP PIPELINE] Removed 140475 trailing zeros from sample 1
[CLAP PIPELINE] Removed 152279 trailing zeros from sample 2
[CLAP PIPELINE] Removed 47652 trailing zeros from sample 3
[CLAP PIPELINE] Removed 39111 trailing zeros from sample 4
[CLAP PIPELINE] Removed 157790 trailing zeros from sample 5
[CLAP PIPELINE] Removed 51575 trailing zeros from sample 6
[CLAP PIPELINE] Removed 0 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 3584/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 3584/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=4.9072, max=12.4930, mean=9.2422
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|▋                                                                                     | 11/1324 [00:15<31:07,  0.70it/s, v_num=xrdi, train/loss=nan.0, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=1.0000, mean=-0.0008, std=0.2119
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0186, max=1.0156, mean=-0.0008, std=0.2112
[CLAP PIPELINE] Removed 145785 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([14215])
[CLAP PIPELINE] Sample 0 stats: min=-0.9766, max=0.9678, mean=-0.0143, std=0.4646
[CLAP PIPELINE] Sample 0: 0/14215 near-zero values
[CLAP PIPELINE] Removed 126649 trailing zeros from sample 1
[CLAP PIPELINE] Removed 138074 trailing zeros from sample 2
[CLAP PIPELINE] Removed 48079 trailing zeros from sample 3
[CLAP PIPELINE] Removed 63472 trailing zeros from sample 4
[CLAP PIPELINE] Removed 115693 trailing zeros from sample 5
[CLAP PIPELINE] Removed 126336 trailing zeros from sample 6
[CLAP PIPELINE] Removed 70434 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=5.5315, max=12.8139, mean=9.3451
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|▊                                                                                     | 12/1324 [00:16<30:01,  0.73it/s, v_num=xrdi, train/loss=nan.0, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0097, max=0.9991, mean=-0.0001, std=0.1069
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0264, max=0.9990, mean=-0.0001, std=0.1068
[CLAP PIPELINE] Removed 53205 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([106795])
[CLAP PIPELINE] Sample 0 stats: min=-1.0264, max=0.9795, mean=0.0003, std=0.0954
[CLAP PIPELINE] Sample 0: 22/106795 near-zero values
[CLAP PIPELINE] Removed 45495 trailing zeros from sample 1
[CLAP PIPELINE] Removed 78921 trailing zeros from sample 2
[CLAP PIPELINE] Removed 129995 trailing zeros from sample 3
[CLAP PIPELINE] Removed 64757 trailing zeros from sample 4
[CLAP PIPELINE] Removed 59395 trailing zeros from sample 5
[CLAP PIPELINE] Removed 127667 trailing zeros from sample 6
[CLAP PIPELINE] Removed 95843 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=4.8200, max=12.2489, mean=9.5657
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|▊                                                                                     | 13/1324 [00:18<31:41,  0.69it/s, v_num=xrdi, train/loss=nan.0, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0137, max=1.0162, mean=-0.0003, std=0.0942
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9985, mean=-0.0003, std=0.0923
[CLAP PIPELINE] Removed 40568 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([119432])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9556, mean=-0.0001, std=0.1826
[CLAP PIPELINE] Sample 0: 0/119432 near-zero values
[CLAP PIPELINE] Removed 84761 trailing zeros from sample 1
[CLAP PIPELINE] Removed 77920 trailing zeros from sample 2
[CLAP PIPELINE] Removed 151832 trailing zeros from sample 3
[CLAP PIPELINE] Removed 47228 trailing zeros from sample 4
[CLAP PIPELINE] Removed 47228 trailing zeros from sample 5
[CLAP PIPELINE] Removed 65190 trailing zeros from sample 6
[CLAP PIPELINE] Removed 57594 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=6.2500, max=12.2628, mean=9.8566
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|▉                                                                                     | 14/1324 [00:19<30:06,  0.73it/s, v_num=xrdi, train/loss=nan.0, lr=2.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=0.9983, mean=-0.0002, std=0.1375
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0039, max=1.0049, mean=-0.0002, std=0.1370
[CLAP PIPELINE] Removed 120295 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([39705])
[CLAP PIPELINE] Sample 0 stats: min=-0.9971, max=0.8145, mean=-0.0012, std=0.1683
[CLAP PIPELINE] Sample 0: 1/39705 near-zero values
[CLAP PIPELINE] Removed 12271 trailing zeros from sample 1
[CLAP PIPELINE] Removed 124315 trailing zeros from sample 2
[CLAP PIPELINE] Removed 80485 trailing zeros from sample 3
[CLAP PIPELINE] Removed 8289 trailing zeros from sample 4
[CLAP PIPELINE] Removed 119925 trailing zeros from sample 5
[CLAP PIPELINE] Removed 67813 trailing zeros from sample 6
[CLAP PIPELINE] Removed 139329 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=2.6489, max=13.1278, mean=8.9112
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|▉                                                                                     | 15/1324 [00:21<31:07,  0.70it/s, v_num=xrdi, train/loss=nan.0, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.9983, mean=0.0149, std=0.1915
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0176, max=0.9985, mean=0.0149, std=0.1914
[CLAP PIPELINE] Removed 150049 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([9951])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=0.6035, mean=0.0008, std=0.0610
[CLAP PIPELINE] Sample 0: 17/9951 near-zero values
[CLAP PIPELINE] Removed 24965 trailing zeros from sample 1
[CLAP PIPELINE] Removed 100083 trailing zeros from sample 2
[CLAP PIPELINE] Removed 101885 trailing zeros from sample 3
[CLAP PIPELINE] Removed 41608 trailing zeros from sample 4
[CLAP PIPELINE] Removed 125324 trailing zeros from sample 5
[CLAP PIPELINE] Removed 22881 trailing zeros from sample 6
[CLAP PIPELINE] Removed 134275 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=4.9665, max=12.8488, mean=9.4602
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|█                                                                                     | 16/1324 [00:21<29:55,  0.73it/s, v_num=xrdi, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0018, max=1.0100, mean=0.0005, std=0.1117
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=1.0078, mean=0.0005, std=0.1116
[CLAP PIPELINE] Removed 109222 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([50778])
[CLAP PIPELINE] Sample 0 stats: min=-0.9399, max=0.9409, mean=-0.0001, std=0.1207
[CLAP PIPELINE] Sample 0: 0/50778 near-zero values
[CLAP PIPELINE] Removed 148710 trailing zeros from sample 1
[CLAP PIPELINE] Removed 88054 trailing zeros from sample 2
[CLAP PIPELINE] Removed 113947 trailing zeros from sample 3
[CLAP PIPELINE] Removed 124926 trailing zeros from sample 4
[CLAP PIPELINE] Removed 151821 trailing zeros from sample 5
[CLAP PIPELINE] Removed 149986 trailing zeros from sample 6
[CLAP PIPELINE] Removed 95003 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=5.3432, max=12.0884, mean=9.6818
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|█                                                                                     | 17/1324 [00:23<29:50,  0.73it/s, v_num=xrdi, train/loss=nan.0, lr=2.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=1.0016, mean=-0.0003, std=0.1434
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=1.0020, mean=-0.0003, std=0.1423
[CLAP PIPELINE] Removed 86958 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([73042])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=1.0000, mean=-0.0016, std=0.3835
[CLAP PIPELINE] Sample 0: 0/73042 near-zero values
[CLAP PIPELINE] Removed 117084 trailing zeros from sample 1
[CLAP PIPELINE] Removed 50628 trailing zeros from sample 2
[CLAP PIPELINE] Removed 15391 trailing zeros from sample 3
[CLAP PIPELINE] Removed 20360 trailing zeros from sample 4
[CLAP PIPELINE] Removed 148155 trailing zeros from sample 5
[CLAP PIPELINE] Removed 111906 trailing zeros from sample 6
[CLAP PIPELINE] Removed 143687 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=7.9590, max=12.0884, mean=9.7855
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|█▏                                                                                    | 18/1324 [00:24<29:34,  0.74it/s, v_num=xrdi, train/loss=nan.0, lr=2.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9823, max=1.0054, mean=-0.0039, std=0.1403
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9658, max=0.9980, mean=-0.0039, std=0.1399
[CLAP PIPELINE] Removed 34327 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([125673])
[CLAP PIPELINE] Sample 0 stats: min=-0.9053, max=0.9912, mean=-0.0001, std=0.2063
[CLAP PIPELINE] Sample 0: 1/125673 near-zero values
[CLAP PIPELINE] Removed 86551 trailing zeros from sample 1
[CLAP PIPELINE] Removed 55875 trailing zeros from sample 2
[CLAP PIPELINE] Removed 79774 trailing zeros from sample 3
[CLAP PIPELINE] Removed 47228 trailing zeros from sample 4
[CLAP PIPELINE] Removed 25925 trailing zeros from sample 5
[CLAP PIPELINE] Removed 51325 trailing zeros from sample 6
[CLAP PIPELINE] Removed 148155 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=5.8908, max=12.8139, mean=9.6444
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   1%|█▏                                                                                    | 19/1324 [00:26<30:45,  0.71it/s, v_num=xrdi, train/loss=nan.0, lr=2.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0416, max=1.0553, mean=0.0001, std=0.2796
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0859, max=1.0859, mean=0.0001, std=0.2776
[CLAP PIPELINE] Removed 87998 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([72002])
[CLAP PIPELINE] Sample 0 stats: min=-1.0859, max=1.0859, mean=-0.0005, std=0.7451
[CLAP PIPELINE] Sample 0: 0/72002 near-zero values
[CLAP PIPELINE] Removed 80312 trailing zeros from sample 1
[CLAP PIPELINE] Removed 27044 trailing zeros from sample 2
[CLAP PIPELINE] Removed 93111 trailing zeros from sample 3
[CLAP PIPELINE] Removed 48224 trailing zeros from sample 4
[CLAP PIPELINE] Removed 93385 trailing zeros from sample 5
[CLAP PIPELINE] Removed 49251 trailing zeros from sample 6
[CLAP PIPELINE] Removed 88005 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=2.5914, max=12.2140, mean=9.2379
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   2%|█▎                                                                                    | 20/1324 [00:27<29:47,  0.73it/s, v_num=xrdi, train/loss=nan.0, lr=2.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0018, max=0.9990, mean=-0.0010, std=0.2095
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0059, max=0.9951, mean=-0.0010, std=0.2095
[CLAP PIPELINE] Removed 143465 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([16535])
[CLAP PIPELINE] Sample 0 stats: min=-0.9590, max=0.8774, mean=-0.0035, std=0.1759
[CLAP PIPELINE] Sample 0: 2/16535 near-zero values
[CLAP PIPELINE] Removed 130986 trailing zeros from sample 1
[CLAP PIPELINE] Removed 76980 trailing zeros from sample 2
[CLAP PIPELINE] Removed 109469 trailing zeros from sample 3
[CLAP PIPELINE] Removed 95875 trailing zeros from sample 4
[CLAP PIPELINE] Removed 79852 trailing zeros from sample 5
[CLAP PIPELINE] Removed 50628 trailing zeros from sample 6
[CLAP PIPELINE] Removed 17289 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=1.0324, max=12.5767, mean=8.6150
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   2%|█▎                                                                                    | 21/1324 [00:28<29:30,  0.74it/s, v_num=xrdi, train/loss=nan.0, lr=2.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0570, max=1.0064, mean=0.0008, std=0.1310
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0176, max=1.0068, mean=0.0008, std=0.1305
[CLAP PIPELINE] Removed 24965 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([135035])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.8276, mean=-0.0029, std=0.1827
[CLAP PIPELINE] Sample 0: 4/135035 near-zero values
[CLAP PIPELINE] Removed 0 trailing zeros from sample 1
[CLAP PIPELINE] Removed 45361 trailing zeros from sample 2
[CLAP PIPELINE] Removed 101885 trailing zeros from sample 3
[CLAP PIPELINE] Removed 90985 trailing zeros from sample 4
[CLAP PIPELINE] Removed 50913 trailing zeros from sample 5
[CLAP PIPELINE] Removed 39995 trailing zeros from sample 6
[CLAP PIPELINE] Removed 95003 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=2.5269, max=12.1164, mean=9.1810
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   2%|█▍                                                                                    | 22/1324 [00:29<29:34,  0.73it/s, v_num=xrdi, train/loss=nan.0, lr=2.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([8, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0640, max=0.9898, mean=-0.0006, std=0.0934
[CLAP PIPELINE] After resampling: shape=torch.Size([8, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9980, mean=-0.0006, std=0.0903
[CLAP PIPELINE] Removed 111975 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([48025])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9619, mean=-0.0025, std=0.1192
[CLAP PIPELINE] Sample 0: 9/48025 near-zero values
[CLAP PIPELINE] Removed 152279 trailing zeros from sample 1
[CLAP PIPELINE] Removed 51575 trailing zeros from sample 2
[CLAP PIPELINE] Removed 143465 trailing zeros from sample 3
[CLAP PIPELINE] Removed 138934 trailing zeros from sample 4
[CLAP PIPELINE] Removed 94052 trailing zeros from sample 5
[CLAP PIPELINE] Removed 108171 trailing zeros from sample 6
[CLAP PIPELINE] Removed 132887 trailing zeros from sample 7
[CLAP PIPELINE] About to call get_audio_embedding with 8 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([8, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 4096/4096 NaN values and 0/4096 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([8, 512]), y_ref torch.Size([8, 512]), y_clap torch.Size([8, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 4096/4096 NaN and 0/4096 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([8, 8]), C_ref_clap torch.Size([8, 8]), C_im_clap torch.Size([8, 8])
[SIMILARITY] C_qvim: min=5.4269, max=11.9141, mean=9.5165
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 64/64 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 64/64 NaN values
Epoch 0:   2%|█▍                                                                                    | 23/1324 [00:30<28:57,  0.75it/s, v_num=xrdi, train/loss=nan.0, lr=2.96e-5]

Detected KeyboardInterrupt, attempting graceful shutdown ...
