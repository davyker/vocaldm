Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs

----- Training -----
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name              | Type                               | Params | Mode
---------------------------------------------------------------------------------
0 | mel               | AugmentMelSTFT                     | 0      | train
1 | imitation_encoder | MobileNetV3                        | 2.7 M  | train
2 | reference_encoder | MobileNetV3                        | 2.7 M  | train
3 | clap_model        | CLAPAudioEmbeddingClassifierFreev2 | 158 M  | eval
  | other params      | n/a                                | 2      | n/a
---------------------------------------------------------------------------------
5.4 M     Trainable params
158 M     Non-trainable params
163 M     Total params
655.085   Total estimated model params size (MB)
419       Modules in train mode
465       Modules in eval mode
Sanity Checking DataLoader 0:   0%|                                                                                                                       | 0/2 [00:00<?, ?it/s]
[VALIDATION] Batch 0 - Processing validation data
[VALIDATION] Imitation shape: torch.Size([1, 320000]), Reference shape: torch.Size([1, 320000])
[VALIDATION] Imitation: min=-0.8460, max=1.0018
[VALIDATION] Reference: min=-0.9751, max=0.9976
[CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9751, max=0.9976, mean=-0.0001, std=0.2598
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Sample 0: 25/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[VALIDATION] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[VALIDATION] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[VALIDATION] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[VALIDATION] WARNING: NaN found in log_softmax outputs:
[VALIDATION]   - C_qvim_log: 0/1 NaN values
[VALIDATION]   - C_ref_clap_log: 1/1 NaN values
[VALIDATION]   - C_im_clap_log: 1/1 NaN values
Sanity Checking DataLoader 0:  50%|███████████████████████████████████████████████████████▌                                                       | 1/2 [00:00<00:00,  1.07it/s]
[VALIDATION] Batch 1 - Processing validation data
[VALIDATION] Imitation shape: torch.Size([1, 320000]), Reference shape: torch.Size([1, 320000])
[VALIDATION] Imitation: min=-0.9942, max=0.9894
[VALIDATION] Reference: min=-0.9515, max=1.0022
[CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0: 150811/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[VALIDATION] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[VALIDATION] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[VALIDATION] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[VALIDATION] WARNING: NaN found in log_softmax outputs:
[VALIDATION]   - C_qvim_log: 0/1 NaN values
[VALIDATION]   - C_ref_clap_log: 1/1 NaN values
[VALIDATION]   - C_im_clap_log: 1/1 NaN values
Epoch 0:   0%|                                                                                                                                        | 0/10586 [00:00<?, ?it/s][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9448, max=0.7345, mean=0.0000, std=0.0662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8896, max=0.5645, mean=0.0000, std=0.0489
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8896, max=0.5645, mean=0.0000, std=0.0489
[CLAP PIPELINE] Sample 0: 75420/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                     | 1/10586 [00:02<7:35:55,  0.39it/s, v_num=hdbx, train/loss=nan.0, lr=2.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8902, max=1.0016, mean=-0.0000, std=0.1195
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8936, max=1.0020, mean=-0.0000, std=0.1195
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8936, max=1.0020, mean=-0.0000, std=0.1195
[CLAP PIPELINE] Sample 0: 117088/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                     | 2/10586 [00:02<4:23:41,  0.67it/s, v_num=hdbx, train/loss=nan.0, lr=2.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9285, max=0.9992, mean=-0.0009, std=0.0743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Sample 0: 129718/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 3/10586 [00:03<3:16:07,  0.90it/s, v_num=hdbx, train/loss=nan.0, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9876, max=1.0462, mean=-0.0003, std=0.0745
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0: 92412/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 4/10586 [00:03<2:39:33,  1.11it/s, v_num=hdbx, train/loss=nan.0, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0043, max=0.8394, mean=-0.0000, std=0.0324
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5513, max=0.6128, mean=-0.0000, std=0.0288
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5513, max=0.6128, mean=-0.0000, std=0.0288
[CLAP PIPELINE] Sample 0: 152754/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 5/10586 [00:03<2:18:56,  1.27it/s, v_num=hdbx, train/loss=nan.0, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8219, max=0.8306, mean=-0.0000, std=0.0854
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7202, max=0.7139, mean=-0.0000, std=0.0742
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7202, max=0.7139, mean=-0.0000, std=0.0742
[CLAP PIPELINE] Sample 0: 7/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1782, max=0.1287, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.9382, max=-0.9382, mean=-0.9382
[SIMILARITY] C_im_clap: min=-0.9033, max=-0.9033, mean=-0.9033
Epoch 0:   0%|                                                                                    | 6/10586 [00:04<2:03:52,  1.42it/s, v_num=hdbx, train/loss=-0.00, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9915, max=0.7667, mean=-0.0008, std=0.0252
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Sample 0: 104639/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 7/10586 [00:04<1:54:25,  1.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.9017, mean=0.0002, std=0.1192
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9956, max=0.9019, mean=0.0002, std=0.1191
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9956, max=0.9019, mean=0.0002, std=0.1191
[CLAP PIPELINE] Sample 0: 131309/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 8/10586 [00:04<1:45:15,  1.67it/s, v_num=hdbx, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9986, max=0.9916, mean=0.0004, std=0.1325
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0: 99635/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 9/10586 [00:04<1:29:28,  1.97it/s, v_num=hdbx, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9860, max=0.6366, mean=-0.0001, std=0.0639
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Sample 0: 91800/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 10/10586 [00:04<1:24:36,  2.08it/s, v_num=hdbx, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9877, max=0.9983, mean=0.0004, std=0.0661
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Sample 0: 87724/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 11/10586 [00:05<1:21:46,  2.16it/s, v_num=hdbx, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8575, mean=-0.0000, std=0.0995
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Sample 0: 66892/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 12/10586 [00:05<1:18:47,  2.24it/s, v_num=hdbx, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8927, max=1.0000, mean=0.0000, std=0.0786
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8281, max=0.7896, mean=0.0000, std=0.0775
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8281, max=0.7896, mean=0.0000, std=0.0775
[CLAP PIPELINE] Sample 0: 150521/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 13/10586 [00:05<1:16:55,  2.29it/s, v_num=hdbx, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=1.0032, mean=-0.0030, std=0.1216
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9224, max=0.9990, mean=-0.0030, std=0.1213
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9224, max=0.9990, mean=-0.0030, std=0.1213
[CLAP PIPELINE] Sample 0: 123215/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 14/10586 [00:05<1:14:37,  2.36it/s, v_num=hdbx, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6593, max=0.6695, mean=0.0131, std=0.0694
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6108, max=0.6440, mean=0.0131, std=0.0693
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6108, max=0.6440, mean=0.0131, std=0.0693
[CLAP PIPELINE] Sample 0: 95673/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 15/10586 [00:06<1:13:25,  2.40it/s, v_num=hdbx, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8606, max=1.0119, mean=0.0192, std=0.2174
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8516, max=0.9956, mean=0.0192, std=0.2169
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8516, max=0.9956, mean=0.0192, std=0.2169
[CLAP PIPELINE] Sample 0: 3118/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2160, max=14.2160, mean=14.2160
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 16/10586 [00:06<1:11:48,  2.45it/s, v_num=hdbx, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9965, mean=-0.0004, std=0.1406
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0: 27046/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 17/10586 [00:06<1:09:39,  2.53it/s, v_num=hdbx, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9966, max=1.0181, mean=-0.0001, std=0.0736
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=1.0098, mean=-0.0001, std=0.0735
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=1.0098, mean=-0.0001, std=0.0735
[CLAP PIPELINE] Sample 0: 103091/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 18/10586 [00:07<1:08:37,  2.57it/s, v_num=hdbx, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8297, max=1.0162, mean=-0.0000, std=0.0370
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Sample 0: 64456/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 19/10586 [00:07<1:07:37,  2.60it/s, v_num=hdbx, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9877, max=0.9983, mean=0.0004, std=0.0661
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Sample 0: 87724/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 20/10586 [00:07<1:06:47,  2.64it/s, v_num=hdbx, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9285, max=0.9992, mean=-0.0009, std=0.0743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Sample 0: 129718/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 21/10586 [00:07<1:05:34,  2.69it/s, v_num=hdbx, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9624, max=1.0002, mean=-0.0004, std=0.1298
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Sample 0: 144175/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 22/10586 [00:08<1:04:33,  2.73it/s, v_num=hdbx, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0033, max=1.0005, mean=-0.0001, std=0.0460
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9839, max=0.9712, mean=-0.0001, std=0.0460
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9839, max=0.9712, mean=-0.0001, std=0.0460
[CLAP PIPELINE] Sample 0: 154779/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 23/10586 [00:08<1:03:34,  2.77it/s, v_num=hdbx, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9982, max=0.8683, mean=-0.0054, std=0.0859
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.8677, mean=-0.0054, std=0.0859
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.8677, mean=-0.0054, std=0.0859
[CLAP PIPELINE] Sample 0: 127269/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 24/10586 [00:08<1:02:38,  2.81it/s, v_num=hdbx, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0084, mean=-0.0001, std=0.1935
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Sample 0: 39111/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 25/10586 [00:08<1:01:44,  2.85it/s, v_num=hdbx, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0007, max=0.9729, mean=-0.0113, std=0.4417
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.9702, mean=-0.0113, std=0.4414
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.9702, mean=-0.0113, std=0.4414
[CLAP PIPELINE] Sample 0: 20050/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 26/10586 [00:09<1:03:21,  2.78it/s, v_num=hdbx, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9992, max=0.9896, mean=-0.0005, std=0.0853
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9897, mean=-0.0005, std=0.0853
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9897, mean=-0.0005, std=0.0853
[CLAP PIPELINE] Sample 0: 137825/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 27/10586 [00:09<1:02:55,  2.80it/s, v_num=hdbx, train/loss=nan.0, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9676, max=1.0002, mean=-0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0: 25933/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 28/10586 [00:09<1:01:05,  2.88it/s, v_num=hdbx, train/loss=nan.0, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9931, max=0.9308, mean=-0.0000, std=0.1076
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9946, max=0.9312, mean=-0.0000, std=0.1076
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9946, max=0.9312, mean=-0.0000, std=0.1076
[CLAP PIPELINE] Sample 0: 50682/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 29/10586 [00:09<1:00:39,  2.90it/s, v_num=hdbx, train/loss=nan.0, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7830, max=0.7911, mean=-0.0000, std=0.0254
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.2476, max=0.2393, mean=-0.0000, std=0.0161
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.2476, max=0.2393, mean=-0.0000, std=0.0161
[CLAP PIPELINE] Sample 0: 63125/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 30/10586 [00:10<1:02:18,  2.82it/s, v_num=hdbx, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8381, max=1.0082, mean=0.0006, std=0.1664
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8252, max=0.9849, mean=0.0006, std=0.1655
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8252, max=0.9849, mean=0.0006, std=0.1655
[CLAP PIPELINE] Sample 0: 88665/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 31/10586 [00:10<1:01:29,  2.86it/s, v_num=hdbx, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8002, max=1.0032, mean=-0.0002, std=0.0756
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0: 131203/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 32/10586 [00:11<1:01:09,  2.88it/s, v_num=hdbx, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8584, max=0.9898, mean=-0.0015, std=0.2105
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8594, max=0.8374, mean=-0.0015, std=0.2020
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8594, max=0.8374, mean=-0.0015, std=0.2020
[CLAP PIPELINE] Sample 0: 104006/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 33/10586 [00:11<1:00:27,  2.91it/s, v_num=hdbx, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8739, max=0.9986, mean=-0.0000, std=0.0380
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0: 61484/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 34/10586 [00:12<1:02:21,  2.82it/s, v_num=hdbx, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8882, max=0.8452, mean=-0.0001, std=0.0777
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5093, max=0.4824, mean=-0.0001, std=0.0479
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5093, max=0.4824, mean=-0.0001, std=0.0479
[CLAP PIPELINE] Sample 0: 63204/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 35/10586 [00:12<1:01:47,  2.85it/s, v_num=hdbx, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9034, max=1.0000, mean=-0.0000, std=0.1119
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0: 63474/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 36/10586 [00:12<1:02:25,  2.82it/s, v_num=hdbx, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1079, max=1.1765, mean=-0.0007, std=0.1651
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.1914, max=1.1514, mean=-0.0007, std=0.1642
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.1914, max=1.1514, mean=-0.0007, std=0.1642
[CLAP PIPELINE] Sample 0: 6096/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 37/10586 [00:13<1:01:51,  2.84it/s, v_num=hdbx, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9734, max=0.9692, mean=-0.0008, std=0.4332
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=1.0049, mean=-0.0008, std=0.4280
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=1.0049, mean=-0.0008, std=0.4280
[CLAP PIPELINE] Sample 0: 88002/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                   | 38/10586 [00:13<1:03:34,  2.77it/s, v_num=hdbx, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0: 150811/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2229, max=14.2229, mean=14.2229
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                   | 39/10586 [00:14<1:03:08,  2.78it/s, v_num=hdbx, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.7777, mean=0.0000, std=0.0404
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.7783, mean=0.0000, std=0.0404
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.7783, mean=0.0000, std=0.0404
[CLAP PIPELINE] Sample 0: 113254/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                   | 40/10586 [00:14<1:03:22,  2.77it/s, v_num=hdbx, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0082, max=0.9617, mean=-0.0000, std=0.0235
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6812, max=0.6206, mean=-0.0000, std=0.0112
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6812, max=0.6206, mean=-0.0000, std=0.0112
[CLAP PIPELINE] Sample 0: 655/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                   | 41/10586 [00:14<1:02:54,  2.79it/s, v_num=hdbx, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9410, max=0.9987, mean=0.0008, std=0.1114
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8623, max=0.8574, mean=0.0008, std=0.1111
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8623, max=0.8574, mean=0.0008, std=0.1111
[CLAP PIPELINE] Sample 0: 129901/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 42/10586 [00:14<1:02:18,  2.82it/s, v_num=hdbx, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8091, mean=-0.0016, std=0.1717
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9570, max=0.8037, mean=-0.0016, std=0.1718
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9570, max=0.8037, mean=-0.0016, std=0.1718
[CLAP PIPELINE] Sample 0: 84771/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 43/10586 [00:14<1:00:00,  2.93it/s, v_num=hdbx, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8915, max=0.9999, mean=-0.0001, std=0.0858
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8911, max=1.0000, mean=-0.0001, std=0.0858
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8911, max=1.0000, mean=-0.0001, std=0.0858
[CLAP PIPELINE] Sample 0: 134117/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 44/10586 [00:14<59:38,  2.95it/s, v_num=hdbx, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8860, max=0.8689, mean=-0.0001, std=0.1662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Sample 0: 111849/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 45/10586 [00:15<59:11,  2.97it/s, v_num=hdbx, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.9639, mean=0.0002, std=0.0911
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9595, max=0.9629, mean=0.0002, std=0.0911
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9595, max=0.9629, mean=0.0002, std=0.0911
[CLAP PIPELINE] Sample 0: 101845/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 46/10586 [00:15<58:44,  2.99it/s, v_num=hdbx, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9609, max=0.9900, mean=-0.0000, std=0.1076
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9565, max=0.9468, mean=-0.0000, std=0.1075
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9565, max=0.9468, mean=-0.0000, std=0.1075
[CLAP PIPELINE] Sample 0: 52094/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 47/10586 [00:15<58:18,  3.01it/s, v_num=hdbx, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9793, max=0.9997, mean=0.0003, std=0.1813
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9644, max=0.9917, mean=0.0003, std=0.1813
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9644, max=0.9917, mean=0.0003, std=0.1813
[CLAP PIPELINE] Sample 0: 69162/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 48/10586 [00:16<58:49,  2.99it/s, v_num=hdbx, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.5452, max=1.0035, mean=0.0012, std=0.1025
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5444, max=1.0029, mean=0.0012, std=0.1025
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5444, max=1.0029, mean=0.0012, std=0.1025
[CLAP PIPELINE] Sample 0: 119345/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 49/10586 [00:16<58:58,  2.98it/s, v_num=hdbx, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9915, max=0.7667, mean=-0.0008, std=0.0252
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Sample 0: 104639/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 50/10586 [00:16<58:57,  2.98it/s, v_num=hdbx, train/loss=nan.0, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0745, max=1.0750, mean=0.0004, std=0.1761
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0283, max=1.0527, mean=0.0004, std=0.1761
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0283, max=1.0527, mean=0.0004, std=0.1761
[CLAP PIPELINE] Sample 0: 38948/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 51/10586 [00:17<59:43,  2.94it/s, v_num=hdbx, train/loss=nan.0, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=0.8610, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0: 148709/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 52/10586 [00:17<59:31,  2.95it/s, v_num=hdbx, train/loss=nan.0, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8210, max=1.0005, mean=0.0000, std=0.0306
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8125, max=0.9976, mean=0.0000, std=0.0306
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8125, max=0.9976, mean=0.0000, std=0.0306
[CLAP PIPELINE] Sample 0: 139913/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 53/10586 [00:17<59:35,  2.95it/s, v_num=hdbx, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9396, mean=-0.0023, std=0.0912
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0127, max=0.9453, mean=-0.0023, std=0.0912
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0127, max=0.9453, mean=-0.0023, std=0.0912
[CLAP PIPELINE] Sample 0: 113653/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 54/10586 [00:18<59:19,  2.96it/s, v_num=hdbx, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.5613, mean=-0.0000, std=0.0136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0: 18388/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 55/10586 [00:18<59:55,  2.93it/s, v_num=hdbx, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0767, max=1.0207, mean=-0.0001, std=0.2233
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8193, max=0.8135, mean=-0.0001, std=0.2155
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8193, max=0.8135, mean=-0.0001, std=0.2155
[CLAP PIPELINE] Sample 0: 35158/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 56/10586 [00:18<59:31,  2.95it/s, v_num=hdbx, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9968, max=0.9985, mean=0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0166, max=1.0215, mean=0.0000, std=0.1312
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0166, max=1.0215, mean=0.0000, std=0.1312
[CLAP PIPELINE] Sample 0: 86803/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 57/10586 [00:19<59:53,  2.93it/s, v_num=hdbx, train/loss=nan.0, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9992, max=0.9896, mean=-0.0005, std=0.0853
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9897, mean=-0.0005, std=0.0853
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9897, mean=-0.0005, std=0.0853
[CLAP PIPELINE] Sample 0: 137825/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 58/10586 [00:19<59:32,  2.95it/s, v_num=hdbx, train/loss=nan.0, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8806, max=0.7257, mean=-0.0001, std=0.0129
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6597, max=0.4893, mean=-0.0001, std=0.0100
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6597, max=0.4893, mean=-0.0001, std=0.0100
[CLAP PIPELINE] Sample 0: 47912/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 59/10586 [00:19<58:28,  3.00it/s, v_num=hdbx, train/loss=nan.0, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9983, mean=0.0005, std=0.1294
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Sample 0: 140620/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 60/10586 [00:19<58:09,  3.02it/s, v_num=hdbx, train/loss=nan.0, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0024, max=0.6007, mean=-0.0007, std=0.0930
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0: 133106/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 61/10586 [00:20<58:00,  3.02it/s, v_num=hdbx, train/loss=nan.0, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9958, max=0.9515, mean=-0.0001, std=0.0717
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9600, max=0.8765, mean=-0.0001, std=0.0699
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9600, max=0.8765, mean=-0.0001, std=0.0699
[CLAP PIPELINE] Sample 0: 36790/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 62/10586 [00:20<57:44,  3.04it/s, v_num=hdbx, train/loss=nan.0, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9835, max=0.8843, mean=-0.0000, std=0.2600
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9609, max=0.8481, mean=-0.0000, std=0.2581
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9609, max=0.8481, mean=-0.0000, std=0.2581
[CLAP PIPELINE] Sample 0: 26255/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 63/10586 [00:21<58:49,  2.98it/s, v_num=hdbx, train/loss=nan.0, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9953, max=0.9269, mean=-0.0007, std=0.0796
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Sample 0: 119923/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 64/10586 [00:21<58:25,  3.00it/s, v_num=hdbx, train/loss=nan.0, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9969, max=0.9446, mean=-0.0009, std=0.0720
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9927, max=0.9448, mean=-0.0009, std=0.0720
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9927, max=0.9448, mean=-0.0009, std=0.0720
[CLAP PIPELINE] Sample 0: 129531/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 65/10586 [00:21<58:10,  3.01it/s, v_num=hdbx, train/loss=nan.0, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7351, max=0.7890, mean=-0.0000, std=0.0251
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.3665, max=0.3438, mean=-0.0000, std=0.0220
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.3665, max=0.3438, mean=-0.0000, std=0.0220
[CLAP PIPELINE] Sample 0: 17026/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 66/10586 [00:21<58:00,  3.02it/s, v_num=hdbx, train/loss=nan.0, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9456, max=0.9481, mean=-0.0003, std=0.0849
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8320, max=0.8291, mean=-0.0003, std=0.0840
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8320, max=0.8291, mean=-0.0003, std=0.0840
[CLAP PIPELINE] Sample 0: 144094/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 67/10586 [00:22<57:45,  3.04it/s, v_num=hdbx, train/loss=nan.0, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9955, max=0.8652, mean=-0.0000, std=0.0358
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9419, max=0.7778, mean=-0.0000, std=0.0352
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9419, max=0.7778, mean=-0.0000, std=0.0352
[CLAP PIPELINE] Sample 0: 130987/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 68/10586 [00:22<57:28,  3.05it/s, v_num=hdbx, train/loss=nan.0, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9680, max=0.9256, mean=-0.0028, std=0.1273
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7139, max=0.8584, mean=-0.0028, std=0.1180
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7139, max=0.8584, mean=-0.0028, std=0.1180
[CLAP PIPELINE] Sample 0: 14928/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 69/10586 [00:22<58:12,  3.01it/s, v_num=hdbx, train/loss=nan.0, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9638, max=0.9958, mean=-0.0016, std=0.1701
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9717, max=0.9648, mean=-0.0016, std=0.1698
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9717, max=0.9648, mean=-0.0016, std=0.1698
[CLAP PIPELINE] Sample 0: 82212/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 70/10586 [00:23<58:14,  3.01it/s, v_num=hdbx, train/loss=nan.0, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8680, max=0.9994, mean=0.0003, std=0.0444
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8667, max=0.9995, mean=0.0003, std=0.0444
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8667, max=0.9995, mean=0.0003, std=0.0444
[CLAP PIPELINE] Sample 0: 153881/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 71/10586 [00:23<58:02,  3.02it/s, v_num=hdbx, train/loss=nan.0, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0068, max=0.9876, mean=-0.0000, std=0.1221
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0068, max=0.9868, mean=-0.0000, std=0.1221
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0068, max=0.9868, mean=-0.0000, std=0.1221
[CLAP PIPELINE] Sample 0: 115119/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 72/10586 [00:23<57:54,  3.03it/s, v_num=hdbx, train/loss=nan.0, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8416, max=1.0099, mean=-0.0003, std=0.0124
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8896, max=0.8569, mean=-0.0003, std=0.0116
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8896, max=0.8569, mean=-0.0003, std=0.0116
[CLAP PIPELINE] Sample 0: 150683/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 73/10586 [00:24<57:54,  3.03it/s, v_num=hdbx, train/loss=nan.0, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9876, max=1.0462, mean=-0.0003, std=0.0745
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0: 92412/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 74/10586 [00:24<57:38,  3.04it/s, v_num=hdbx, train/loss=nan.0, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.5613, mean=-0.0000, std=0.0136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0: 18388/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 75/10586 [00:24<57:36,  3.04it/s, v_num=hdbx, train/loss=nan.0, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0118, max=1.0184, mean=0.0002, std=0.1353
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0156, max=1.0693, mean=0.0002, std=0.1348
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0156, max=1.0693, mean=0.0002, std=0.1348
[CLAP PIPELINE] Sample 0: 30825/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 76/10586 [00:24<57:30,  3.05it/s, v_num=hdbx, train/loss=nan.0, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9280, max=0.9990, mean=-0.0000, std=0.0785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0: 50632/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                     | 77/10586 [00:24<56:07,  3.12it/s, v_num=hdbx, train/loss=nan.0, lr=2.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9035, mean=-0.0000, std=0.1785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Sample 0: 78920/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                     | 78/10586 [00:24<56:04,  3.12it/s, v_num=hdbx, train/loss=nan.0, lr=2.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9512, max=1.0018, mean=-0.0001, std=0.0323
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8413, max=0.8599, mean=-0.0001, std=0.0254
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8413, max=0.8599, mean=-0.0001, std=0.0254
[CLAP PIPELINE] Sample 0: 156966/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2229, max=14.2229, mean=14.2229
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                     | 79/10586 [00:25<55:46,  3.14it/s, v_num=hdbx, train/loss=nan.0, lr=2.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9952, max=0.9767, mean=-0.0001, std=0.0806
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0: 140476/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 80/10586 [00:25<55:36,  3.15it/s, v_num=hdbx, train/loss=nan.0, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9555, max=0.9872, mean=-0.0000, std=0.2328
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9482, max=0.9893, mean=-0.0000, std=0.2327
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9482, max=0.9893, mean=-0.0000, std=0.2327
[CLAP PIPELINE] Sample 0: 80486/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 81/10586 [00:25<55:21,  3.16it/s, v_num=hdbx, train/loss=nan.0, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9952, max=0.9767, mean=-0.0001, std=0.0806
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0: 140476/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 82/10586 [00:25<55:12,  3.17it/s, v_num=hdbx, train/loss=nan.0, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0640, max=0.6390, mean=0.0000, std=0.0066
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5913, max=0.5054, mean=0.0000, std=0.0059
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5913, max=0.5054, mean=0.0000, std=0.0059
[CLAP PIPELINE] Sample 0: 152282/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 83/10586 [00:26<54:59,  3.18it/s, v_num=hdbx, train/loss=nan.0, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8165, max=0.7935, mean=-0.0002, std=0.1749
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6182, max=0.5903, mean=-0.0002, std=0.1547
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6182, max=0.5903, mean=-0.0002, std=0.1547
[CLAP PIPELINE] Sample 0: 47653/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 84/10586 [00:26<54:56,  3.19it/s, v_num=hdbx, train/loss=nan.0, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0084, mean=-0.0001, std=0.1935
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Sample 0: 39111/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 85/10586 [00:26<54:44,  3.20it/s, v_num=hdbx, train/loss=nan.0, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9893, max=1.0040, mean=0.0002, std=0.0502
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9502, max=0.9209, mean=0.0002, std=0.0501
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9502, max=0.9209, mean=0.0002, std=0.0501
[CLAP PIPELINE] Sample 0: 157789/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 86/10586 [00:26<54:34,  3.21it/s, v_num=hdbx, train/loss=nan.0, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9057, max=0.9394, mean=0.0000, std=0.0707
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0: 51621/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 87/10586 [00:27<54:26,  3.21it/s, v_num=hdbx, train/loss=nan.0, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7803, max=0.9980, mean=-0.0001, std=0.1884
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7729, max=0.8696, mean=-0.0001, std=0.1881
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7729, max=0.8696, mean=-0.0001, std=0.1881
[CLAP PIPELINE] Sample 0: 0/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1710, max=0.1165, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.9617, max=-0.9617, mean=-0.9617
[SIMILARITY] C_im_clap: min=-0.9661, max=-0.9661, mean=-0.9661
Epoch 0:   1%|▋                                                                                    | 88/10586 [00:27<54:23,  3.22it/s, v_num=hdbx, train/loss=-0.00, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9847, mean=-0.0013, std=0.1384
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9766, max=0.9678, mean=-0.0013, std=0.1385
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9766, max=0.9678, mean=-0.0013, std=0.1385
[CLAP PIPELINE] Sample 0: 145784/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 89/10586 [00:27<54:21,  3.22it/s, v_num=hdbx, train/loss=nan.0, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9994, mean=-0.0028, std=0.1001
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9595, max=0.9692, mean=-0.0028, std=0.1001
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9595, max=0.9692, mean=-0.0028, std=0.1001
[CLAP PIPELINE] Sample 0: 127126/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 90/10586 [00:27<54:13,  3.23it/s, v_num=hdbx, train/loss=nan.0, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=0.9759, mean=-0.0001, std=0.0569
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9688, max=0.9858, mean=-0.0001, std=0.0569
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9688, max=0.9858, mean=-0.0001, std=0.0569
[CLAP PIPELINE] Sample 0: 138091/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 91/10586 [00:28<54:26,  3.21it/s, v_num=hdbx, train/loss=nan.0, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9872, mean=0.0001, std=0.5035
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0186, max=1.0156, mean=0.0001, std=0.5015
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0186, max=1.0156, mean=0.0001, std=0.5015
[CLAP PIPELINE] Sample 0: 48080/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 92/10586 [00:28<54:26,  3.21it/s, v_num=hdbx, train/loss=nan.0, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9034, max=1.0000, mean=-0.0000, std=0.1119
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0: 63474/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 93/10586 [00:28<54:20,  3.22it/s, v_num=hdbx, train/loss=nan.0, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9234, max=0.9909, mean=-0.0019, std=0.1012
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8613, max=0.9893, mean=-0.0019, std=0.1000
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8613, max=0.9893, mean=-0.0019, std=0.1000
[CLAP PIPELINE] Sample 0: 115693/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 94/10586 [00:29<54:14,  3.22it/s, v_num=hdbx, train/loss=nan.0, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9149, max=0.9979, mean=-0.0002, std=0.1847
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9126, max=0.9985, mean=-0.0002, std=0.1847
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9126, max=0.9985, mean=-0.0002, std=0.1847
[CLAP PIPELINE] Sample 0: 126339/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 95/10586 [00:29<54:17,  3.22it/s, v_num=hdbx, train/loss=nan.0, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9921, max=0.6521, mean=-0.0003, std=0.1274
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0107, max=0.6460, mean=-0.0003, std=0.1272
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0107, max=0.6460, mean=-0.0003, std=0.1272
[CLAP PIPELINE] Sample 0: 70436/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 96/10586 [00:29<54:15,  3.22it/s, v_num=hdbx, train/loss=nan.0, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0097, max=0.9844, mean=0.0002, std=0.0780
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0264, max=0.9795, mean=0.0002, std=0.0779
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0264, max=0.9795, mean=0.0002, std=0.0779
[CLAP PIPELINE] Sample 0: 53226/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2787, max=14.2787, mean=14.2787
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 97/10586 [00:30<54:12,  3.23it/s, v_num=hdbx, train/loss=nan.0, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9991, mean=-0.0000, std=0.2045
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9912, max=0.9946, mean=-0.0000, std=0.2046
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9912, max=0.9946, mean=-0.0000, std=0.2046
[CLAP PIPELINE] Sample 0: 45695/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 98/10586 [00:29<53:10,  3.29it/s, v_num=hdbx, train/loss=nan.0, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9035, mean=-0.0000, std=0.1785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Sample 0: 78920/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 99/10586 [00:30<53:09,  3.29it/s, v_num=hdbx, train/loss=nan.0, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9433, max=0.9782, mean=-0.0005, std=0.0523
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8989, max=0.8970, mean=-0.0005, std=0.0514
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8989, max=0.8970, mean=-0.0005, std=0.0514
[CLAP PIPELINE] Sample 0: 129997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 100/10586 [00:30<53:04,  3.29it/s, v_num=hdbx, train/loss=nan.0, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0004, max=0.8341, mean=-0.0000, std=0.0354
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9868, max=0.8340, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9868, max=0.8340, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Sample 0: 65626/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 101/10586 [00:30<53:07,  3.29it/s, v_num=hdbx, train/loss=nan.0, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8739, max=0.9986, mean=-0.0000, std=0.0380
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0: 61484/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 102/10586 [00:30<53:05,  3.29it/s, v_num=hdbx, train/loss=nan.0, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6688, max=0.7300, mean=-0.0000, std=0.0359
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6216, max=0.6060, mean=-0.0000, std=0.0346
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6216, max=0.6060, mean=-0.0000, std=0.0346
[CLAP PIPELINE] Sample 0: 127682/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 103/10586 [00:31<53:07,  3.29it/s, v_num=hdbx, train/loss=nan.0, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0005, max=0.9991, mean=-0.0000, std=0.0704
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9990, mean=-0.0000, std=0.0704
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9990, mean=-0.0000, std=0.0704
[CLAP PIPELINE] Sample 0: 95839/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 104/10586 [00:31<53:03,  3.29it/s, v_num=hdbx, train/loss=nan.0, lr=2.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9232, max=1.0015, mean=-0.0001, std=0.1578
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9556, mean=-0.0001, std=0.1578
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9556, mean=-0.0001, std=0.1578
[CLAP PIPELINE] Sample 0: 40567/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 105/10586 [00:31<52:56,  3.30it/s, v_num=hdbx, train/loss=nan.0, lr=2.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.9991, mean=-0.0015, std=0.0866
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9985, mean=-0.0015, std=0.0866
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.9985, mean=-0.0015, std=0.0866
[CLAP PIPELINE] Sample 0: 85296/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 106/10586 [00:32<52:55,  3.30it/s, v_num=hdbx, train/loss=nan.0, lr=2.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9216, max=0.9771, mean=-0.0000, std=0.0947
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8647, max=0.9453, mean=-0.0000, std=0.0916
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8647, max=0.9453, mean=-0.0000, std=0.0916
[CLAP PIPELINE] Sample 0: 77930/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 107/10586 [00:32<52:50,  3.30it/s, v_num=hdbx, train/loss=nan.0, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0137, max=1.0162, mean=-0.0000, std=0.1196
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9604, max=0.9634, mean=-0.0000, std=0.1135
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9604, max=0.9634, mean=-0.0000, std=0.1135
[CLAP PIPELINE] Sample 0: 151829/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 108/10586 [00:32<52:50,  3.30it/s, v_num=hdbx, train/loss=nan.0, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0: 47237/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 109/10586 [00:32<52:49,  3.31it/s, v_num=hdbx, train/loss=nan.0, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0: 47237/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 110/10586 [00:33<53:04,  3.29it/s, v_num=hdbx, train/loss=nan.0, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7465, max=0.9657, mean=-0.0000, std=0.0711
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7476, max=0.9722, mean=-0.0000, std=0.0708
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7476, max=0.9722, mean=-0.0000, std=0.0708
[CLAP PIPELINE] Sample 0: 65192/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 111/10586 [00:33<53:07,  3.29it/s, v_num=hdbx, train/loss=nan.0, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8718, max=0.9936, mean=-0.0003, std=0.0920
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8423, max=0.9917, mean=-0.0003, std=0.0919
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8423, max=0.9917, mean=-0.0003, std=0.0919
[CLAP PIPELINE] Sample 0: 57622/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 112/10586 [00:34<53:08,  3.28it/s, v_num=hdbx, train/loss=nan.0, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=0.8367, mean=-0.0003, std=0.0839
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9971, max=0.8145, mean=-0.0003, std=0.0839
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9971, max=0.8145, mean=-0.0003, std=0.0839
[CLAP PIPELINE] Sample 0: 120295/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 113/10586 [00:34<53:16,  3.28it/s, v_num=hdbx, train/loss=nan.0, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9967, max=0.8892, mean=-0.0005, std=0.1574
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=0.8481, mean=-0.0005, std=0.1570
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=0.8481, mean=-0.0005, std=0.1570
[CLAP PIPELINE] Sample 0: 12270/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 114/10586 [00:34<53:18,  3.27it/s, v_num=hdbx, train/loss=nan.0, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9042, max=0.9983, mean=-0.0000, std=0.1139
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9048, max=0.9966, mean=-0.0000, std=0.1140
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9048, max=0.9966, mean=-0.0000, std=0.1140
[CLAP PIPELINE] Sample 0: 124314/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                    | 115/10586 [00:35<53:12,  3.28it/s, v_num=hdbx, train/loss=nan.0, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9555, max=0.9872, mean=-0.0000, std=0.2328
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9482, max=0.9893, mean=-0.0000, std=0.2327
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9482, max=0.9893, mean=-0.0000, std=0.2327
[CLAP PIPELINE] Sample 0: 80486/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                    | 116/10586 [00:34<52:20,  3.33it/s, v_num=hdbx, train/loss=nan.0, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=0.7591, mean=-0.0002, std=0.1469
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9961, max=0.7500, mean=-0.0002, std=0.1467
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9961, max=0.7500, mean=-0.0002, std=0.1467
[CLAP PIPELINE] Sample 0: 8290/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                    | 117/10586 [00:35<52:18,  3.34it/s, v_num=hdbx, train/loss=nan.0, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9953, max=0.9269, mean=-0.0007, std=0.0796
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Sample 0: 119923/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                    | 118/10586 [00:35<52:13,  3.34it/s, v_num=hdbx, train/loss=nan.0, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9713, max=0.9290, mean=-0.0000, std=0.1432
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7432, max=0.7358, mean=-0.0000, std=0.1395
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7432, max=0.7358, mean=-0.0000, std=0.1395
[CLAP PIPELINE] Sample 0: 67822/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 119/10586 [00:35<52:11,  3.34it/s, v_num=hdbx, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9976, max=0.9832, mean=-0.0000, std=0.0619
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0039, max=1.0049, mean=-0.0000, std=0.0618
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0039, max=1.0049, mean=-0.0000, std=0.0618
[CLAP PIPELINE] Sample 0: 139443/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 120/10586 [00:35<52:07,  3.35it/s, v_num=hdbx, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9559, max=0.6156, mean=0.0001, std=0.0153
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0: 150065/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 121/10586 [00:36<52:01,  3.35it/s, v_num=hdbx, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0003, max=0.8273, mean=-0.0025, std=0.1678
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0: 24967/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 122/10586 [00:36<51:57,  3.36it/s, v_num=hdbx, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9729, max=0.8797, mean=-0.0026, std=0.1368
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9287, max=0.8765, mean=-0.0026, std=0.1364
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9287, max=0.8765, mean=-0.0026, std=0.1364
[CLAP PIPELINE] Sample 0: 100571/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2299, max=14.2299, mean=14.2299
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 123/10586 [00:36<51:55,  3.36it/s, v_num=hdbx, train/loss=nan.0, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.8496, mean=-0.0000, std=0.0451
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0: 101891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 124/10586 [00:36<51:53,  3.36it/s, v_num=hdbx, train/loss=nan.0, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.9657, mean=0.0000, std=0.2225
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9663, mean=0.0000, std=0.2227
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9663, mean=0.0000, std=0.2227
[CLAP PIPELINE] Sample 0: 41608/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 125/10586 [00:37<51:50,  3.36it/s, v_num=hdbx, train/loss=nan.0, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.6950, mean=-0.0000, std=0.0353
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.6948, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.6948, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Sample 0: 125328/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 126/10586 [00:37<51:54,  3.36it/s, v_num=hdbx, train/loss=nan.0, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9178, max=0.9936, mean=0.1238, std=0.4039
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=0.9927, mean=0.1239, std=0.4033
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=0.9927, mean=0.1239, std=0.4033
[CLAP PIPELINE] Sample 0: 22882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 127/10586 [00:37<51:50,  3.36it/s, v_num=hdbx, train/loss=nan.0, lr=2.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9983, mean=0.0005, std=0.1294
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Sample 0: 140620/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 128/10586 [00:38<51:51,  3.36it/s, v_num=hdbx, train/loss=nan.0, lr=2.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9908, max=1.0015, mean=-0.0000, std=0.0681
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9399, max=0.9409, mean=-0.0000, std=0.0680
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9399, max=0.9409, mean=-0.0000, std=0.0680
[CLAP PIPELINE] Sample 0: 109221/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 129/10586 [00:38<51:41,  3.37it/s, v_num=hdbx, train/loss=nan.0, lr=2.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=0.8610, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0: 148709/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 130/10586 [00:38<51:33,  3.38it/s, v_num=hdbx, train/loss=nan.0, lr=2.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0007, max=0.9994, mean=0.0000, std=0.2048
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9995, mean=0.0000, std=0.2048
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9995, mean=0.0000, std=0.2048
[CLAP PIPELINE] Sample 0: 88055/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 131/10586 [00:38<51:25,  3.39it/s, v_num=hdbx, train/loss=nan.0, lr=2.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9133, max=0.9307, mean=0.0004, std=0.0840
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7100, max=0.7236, mean=0.0004, std=0.0832
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7100, max=0.7236, mean=0.0004, std=0.0832
[CLAP PIPELINE] Sample 0: 113946/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 132/10586 [00:38<51:18,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=2.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7768, max=0.9973, mean=0.0008, std=0.1139
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7759, max=1.0000, mean=0.0008, std=0.1140
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7759, max=1.0000, mean=0.0008, std=0.1140
[CLAP PIPELINE] Sample 0: 124926/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 133/10586 [00:39<51:11,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=2.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9972, max=0.7821, mean=-0.0000, std=0.0092
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9771, max=0.6641, mean=-0.0000, std=0.0087
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9771, max=0.6641, mean=-0.0000, std=0.0087
[CLAP PIPELINE] Sample 0: 151821/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 134/10586 [00:39<51:02,  3.41it/s, v_num=hdbx, train/loss=nan.0, lr=2.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0018, max=1.0100, mean=-0.0002, std=0.1225
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=1.0078, mean=-0.0002, std=0.1226
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=1.0078, mean=-0.0002, std=0.1226
[CLAP PIPELINE] Sample 0: 150303/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 135/10586 [00:39<50:56,  3.42it/s, v_num=hdbx, train/loss=nan.0, lr=2.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9904, max=0.9272, mean=0.0027, std=0.1281
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9814, max=0.9062, mean=0.0027, std=0.1281
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9814, max=0.9062, mean=0.0027, std=0.1281
[CLAP PIPELINE] Sample 0: 95003/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 136/10586 [00:39<50:49,  3.43it/s, v_num=hdbx, train/loss=nan.0, lr=2.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8497, max=0.9994, mean=-0.0007, std=0.2589
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8501, max=1.0000, mean=-0.0008, std=0.2590
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=1.0000, mean=-0.0008, std=0.2590
[CLAP PIPELINE] Sample 0: 86957/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 137/10586 [00:39<50:39,  3.44it/s, v_num=hdbx, train/loss=nan.0, lr=2.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8902, max=1.0016, mean=-0.0000, std=0.1195
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8936, max=1.0020, mean=-0.0000, std=0.1195
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8936, max=1.0020, mean=-0.0000, std=0.1195
[CLAP PIPELINE] Sample 0: 117088/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 138/10586 [00:40<50:29,  3.45it/s, v_num=hdbx, train/loss=nan.0, lr=2.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9280, max=0.9990, mean=-0.0000, std=0.0785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0: 50632/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 139/10586 [00:39<49:47,  3.50it/s, v_num=hdbx, train/loss=nan.0, lr=2.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9044, mean=-0.0008, std=0.2235
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.8965, mean=-0.0008, std=0.2236
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.8965, mean=-0.0008, std=0.2236
[CLAP PIPELINE] Sample 0: 15391/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 140/10586 [00:40<49:44,  3.50it/s, v_num=hdbx, train/loss=nan.0, lr=2.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8628, max=0.9054, mean=-0.0000, std=0.0504
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.3704, max=0.3982, mean=-0.0000, std=0.0234
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.3704, max=0.3982, mean=-0.0000, std=0.0234
[CLAP PIPELINE] Sample 0: 20380/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 141/10586 [00:40<49:44,  3.50it/s, v_num=hdbx, train/loss=nan.0, lr=2.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9621, max=0.9844, mean=-0.0000, std=0.0176
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8643, max=0.8418, mean=-0.0000, std=0.0169
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8643, max=0.8418, mean=-0.0000, std=0.0169
[CLAP PIPELINE] Sample 0: 148157/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 142/10586 [00:40<49:37,  3.51it/s, v_num=hdbx, train/loss=nan.0, lr=2.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.7417, mean=-0.0004, std=0.1304
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9053, max=0.7163, mean=-0.0004, std=0.1290
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9053, max=0.7163, mean=-0.0004, std=0.1290
[CLAP PIPELINE] Sample 0: 111905/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 143/10586 [00:40<49:29,  3.52it/s, v_num=hdbx, train/loss=nan.0, lr=2.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9456, max=0.9481, mean=-0.0003, std=0.0849
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8320, max=0.8291, mean=-0.0003, std=0.0840
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8320, max=0.8291, mean=-0.0003, std=0.0840
[CLAP PIPELINE] Sample 0: 144094/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 144/10586 [00:40<49:29,  3.52it/s, v_num=hdbx, train/loss=nan.0, lr=2.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9329, max=0.9947, mean=-0.0000, std=0.1836
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9053, max=0.9912, mean=-0.0000, std=0.1829
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9053, max=0.9912, mean=-0.0000, std=0.1829
[CLAP PIPELINE] Sample 0: 34328/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 145/10586 [00:41<49:29,  3.52it/s, v_num=hdbx, train/loss=nan.0, lr=2.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8896, max=0.9998, mean=-0.0000, std=0.1505
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8853, max=0.9961, mean=-0.0000, std=0.1505
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8853, max=0.9961, mean=-0.0000, std=0.1505
[CLAP PIPELINE] Sample 0: 86549/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 146/10586 [00:41<49:29,  3.52it/s, v_num=hdbx, train/loss=nan.0, lr=2.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9600, max=1.0005, mean=-0.0000, std=0.2673
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9585, max=0.9824, mean=-0.0000, std=0.2673
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9585, max=0.9824, mean=-0.0000, std=0.2673
[CLAP PIPELINE] Sample 0: 55875/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 147/10586 [00:41<49:32,  3.51it/s, v_num=hdbx, train/loss=nan.0, lr=2.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9823, max=0.6926, mean=-0.0307, std=0.0572
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8652, max=0.5552, mean=-0.0308, std=0.0558
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8652, max=0.5552, mean=-0.0308, std=0.0558
[CLAP PIPELINE] Sample 0: 79775/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 148/10586 [00:42<49:25,  3.52it/s, v_num=hdbx, train/loss=nan.0, lr=2.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0: 47237/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 149/10586 [00:42<49:21,  3.52it/s, v_num=hdbx, train/loss=nan.0, lr=2.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9676, max=1.0002, mean=-0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0: 25933/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 150/10586 [00:42<49:25,  3.52it/s, v_num=hdbx, train/loss=nan.0, lr=2.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8999, max=1.0054, mean=-0.0001, std=0.0841
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8789, max=0.7700, mean=-0.0001, std=0.0834
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8789, max=0.7700, mean=-0.0001, std=0.0834
[CLAP PIPELINE] Sample 0: 51329/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 151/10586 [00:43<49:38,  3.50it/s, v_num=hdbx, train/loss=nan.0, lr=2.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9621, max=0.9844, mean=-0.0000, std=0.0176
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8643, max=0.8418, mean=-0.0000, std=0.0169
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8643, max=0.8418, mean=-0.0000, std=0.0169
[CLAP PIPELINE] Sample 0: 148157/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 152/10586 [00:43<49:41,  3.50it/s, v_num=hdbx, train/loss=nan.0, lr=2.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=1.0036, mean=-0.0002, std=0.5019
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0859, max=1.0859, mean=-0.0002, std=0.5000
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0859, max=1.0859, mean=-0.0002, std=0.5000
[CLAP PIPELINE] Sample 0: 87995/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 153/10586 [00:43<49:38,  3.50it/s, v_num=hdbx, train/loss=nan.0, lr=2.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9953, max=0.7769, mean=-0.0001, std=0.0524
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9912, max=0.7764, mean=-0.0001, std=0.0524
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9912, max=0.7764, mean=-0.0001, std=0.0524
[CLAP PIPELINE] Sample 0: 80309/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                   | 154/10586 [00:44<49:49,  3.49it/s, v_num=hdbx, train/loss=nan.0, lr=2.9e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9965, mean=-0.0004, std=0.1406
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0: 27046/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                   | 155/10586 [00:44<49:46,  3.49it/s, v_num=hdbx, train/loss=nan.0, lr=2.9e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9950, mean=0.0001, std=0.2440
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9956, mean=0.0001, std=0.2441
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9956, mean=0.0001, std=0.2441
[CLAP PIPELINE] Sample 0: 93115/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▎                                                                                   | 156/10586 [00:44<49:56,  3.48it/s, v_num=hdbx, train/loss=nan.0, lr=2.9e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9344, max=0.9998, mean=0.0001, std=0.3089
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9346, max=1.0000, mean=0.0001, std=0.3091
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9346, max=1.0000, mean=0.0001, std=0.3091
[CLAP PIPELINE] Sample 0: 48232/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▏                                                                                  | 157/10586 [00:44<49:30,  3.51it/s, v_num=hdbx, train/loss=nan.0, lr=2.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8681, max=0.9757, mean=0.0000, std=0.1035
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7969, max=0.9199, mean=0.0000, std=0.0999
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7969, max=0.9199, mean=0.0000, std=0.0999
[CLAP PIPELINE] Sample 0: 93552/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█▎                                                                                  | 158/10586 [00:45<49:45,  3.49it/s, v_num=hdbx, train/loss=nan.0, lr=2.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0416, max=1.0553, mean=0.0000, std=0.0665
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9888, max=0.8647, mean=0.0000, std=0.0650
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9888, max=0.8647, mean=0.0000, std=0.0650
[CLAP PIPELINE] Sample 0: 49257/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 159/10586 [00:45<49:50,  3.49it/s, v_num=hdbx, train/loss=nan.0, lr=2.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0030, max=1.0159, mean=0.0009, std=0.4255
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0264, max=1.0391, mean=0.0009, std=0.4180
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0264, max=1.0391, mean=0.0009, std=0.4180
[CLAP PIPELINE] Sample 0: 88003/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 160/10586 [00:45<49:53,  3.48it/s, v_num=hdbx, train/loss=nan.0, lr=2.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9998, max=0.9477, mean=-0.0004, std=0.0566
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9590, max=0.8774, mean=-0.0004, std=0.0565
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9590, max=0.8774, mean=-0.0004, std=0.0565
[CLAP PIPELINE] Sample 0: 143465/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 161/10586 [00:46<49:51,  3.48it/s, v_num=hdbx, train/loss=nan.0, lr=2.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9955, max=0.8652, mean=-0.0000, std=0.0358
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9419, max=0.7778, mean=-0.0000, std=0.0352
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9419, max=0.7778, mean=-0.0000, std=0.0352
[CLAP PIPELINE] Sample 0: 130987/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 162/10586 [00:46<49:54,  3.48it/s, v_num=hdbx, train/loss=nan.0, lr=2.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9634, mean=0.0000, std=0.0657
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9917, max=0.9473, mean=0.0000, std=0.0657
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9917, max=0.9473, mean=0.0000, std=0.0657
[CLAP PIPELINE] Sample 0: 76982/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 163/10586 [00:46<49:53,  3.48it/s, v_num=hdbx, train/loss=nan.0, lr=2.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9899, max=0.9946, mean=-0.0022, std=0.1272
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9858, max=0.9927, mean=-0.0022, std=0.1272
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9858, max=0.9927, mean=-0.0022, std=0.1272
[CLAP PIPELINE] Sample 0: 109744/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 164/10586 [00:47<49:53,  3.48it/s, v_num=hdbx, train/loss=nan.0, lr=2.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9947, mean=-0.0000, std=0.2093
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9951, mean=-0.0000, std=0.2094
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9951, mean=-0.0000, std=0.2094
[CLAP PIPELINE] Sample 0: 95891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 165/10586 [00:47<49:47,  3.49it/s, v_num=hdbx, train/loss=nan.0, lr=2.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.8706, mean=-0.0054, std=0.1878
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.8677, mean=-0.0054, std=0.1879
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.8677, mean=-0.0054, std=0.1879
[CLAP PIPELINE] Sample 0: 79853/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 166/10586 [00:47<49:46,  3.49it/s, v_num=hdbx, train/loss=nan.0, lr=2.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9280, max=0.9990, mean=-0.0000, std=0.0785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0: 50632/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 167/10586 [00:47<49:43,  3.49it/s, v_num=hdbx, train/loss=nan.0, lr=2.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0018, max=0.9247, mean=-0.0000, std=0.4909
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0059, max=0.9419, mean=-0.0000, std=0.4905
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0059, max=0.9419, mean=-0.0000, std=0.4905
[CLAP PIPELINE] Sample 0: 17289/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 168/10586 [00:48<49:40,  3.49it/s, v_num=hdbx, train/loss=nan.0, lr=2.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0003, max=0.8273, mean=-0.0025, std=0.1678
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0: 24967/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 169/10586 [00:48<49:37,  3.50it/s, v_num=hdbx, train/loss=nan.0, lr=2.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0570, max=1.0064, mean=0.0063, std=0.1807
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9360, max=0.9819, mean=0.0063, std=0.1780
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9360, max=0.9819, mean=0.0063, std=0.1780
[CLAP PIPELINE] Sample 0: 6/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 170/10586 [00:48<49:41,  3.49it/s, v_num=hdbx, train/loss=nan.0, lr=2.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9951, max=1.0012, mean=0.0000, std=0.1204
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9902, max=1.0068, mean=0.0000, std=0.1204
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9902, max=1.0068, mean=0.0000, std=0.1204
[CLAP PIPELINE] Sample 0: 45365/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 171/10586 [00:48<49:40,  3.49it/s, v_num=hdbx, train/loss=nan.0, lr=2.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.8496, mean=-0.0000, std=0.0451
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0: 101891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 172/10586 [00:49<49:38,  3.50it/s, v_num=hdbx, train/loss=nan.0, lr=2.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9980, max=0.8624, mean=-0.0004, std=0.1807
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.8608, mean=-0.0004, std=0.1807
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.8608, mean=-0.0004, std=0.1807
[CLAP PIPELINE] Sample 0: 90997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▎                                                                                  | 173/10586 [00:49<49:38,  3.50it/s, v_num=hdbx, train/loss=nan.0, lr=2.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9783, max=0.9732, mean=-0.0000, std=0.0987
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9448, max=0.9648, mean=-0.0000, std=0.0987
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9448, max=0.9648, mean=-0.0000, std=0.0987
[CLAP PIPELINE] Sample 0: 50929/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 174/10586 [00:49<49:38,  3.50it/s, v_num=hdbx, train/loss=nan.0, lr=2.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9621, max=0.9983, mean=-0.0000, std=0.0337
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9614, max=0.9976, mean=-0.0000, std=0.0329
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9614, max=0.9976, mean=-0.0000, std=0.0329
[CLAP PIPELINE] Sample 0: 40001/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 175/10586 [00:49<49:30,  3.50it/s, v_num=hdbx, train/loss=nan.0, lr=2.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9904, max=0.9272, mean=0.0027, std=0.1281
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9814, max=0.9062, mean=0.0027, std=0.1281
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9814, max=0.9062, mean=0.0027, std=0.1281
[CLAP PIPELINE] Sample 0: 95003/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 176/10586 [00:49<49:03,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0000, max=0.9631, mean=-0.0008, std=0.0653
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9619, mean=-0.0008, std=0.0653
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9619, mean=-0.0008, std=0.0653
[CLAP PIPELINE] Sample 0: 111981/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 177/10586 [00:50<49:03,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.96e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0640, max=0.6390, mean=0.0000, std=0.0066
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5913, max=0.5054, mean=0.0000, std=0.0059
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5913, max=0.5054, mean=0.0000, std=0.0059
[CLAP PIPELINE] Sample 0: 152282/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 178/10586 [00:50<49:06,  3.53it/s, v_num=hdbx, train/loss=nan.0, lr=2.96e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9057, max=0.9394, mean=0.0000, std=0.0707
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0: 51621/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 179/10586 [00:50<49:01,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.96e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9998, max=0.9477, mean=-0.0004, std=0.0566
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9590, max=0.8774, mean=-0.0004, std=0.0565
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9590, max=0.8774, mean=-0.0004, std=0.0565
[CLAP PIPELINE] Sample 0: 143465/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 180/10586 [00:50<48:59,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.97e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0170, max=0.7277, mean=-0.0000, std=0.0304
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8125, max=0.6289, mean=-0.0000, std=0.0293
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8125, max=0.6289, mean=-0.0000, std=0.0293
[CLAP PIPELINE] Sample 0: 138935/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 181/10586 [00:51<49:01,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.97e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9993, max=0.8699, mean=-0.0019, std=0.1727
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.8672, mean=-0.0019, std=0.1727
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.8672, mean=-0.0019, std=0.1727
[CLAP PIPELINE] Sample 0: 94052/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 182/10586 [00:51<49:02,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.97e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9994, max=0.9898, mean=-0.0013, std=0.0614
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9946, max=0.9980, mean=-0.0013, std=0.0609
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9946, max=0.9980, mean=-0.0013, std=0.0609
[CLAP PIPELINE] Sample 0: 108172/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2299, max=14.2299, mean=14.2299
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 183/10586 [00:51<49:00,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.97e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7094, max=0.7200, mean=-0.0003, std=0.1508
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7905, max=0.7876, mean=-0.0003, std=0.1461
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7905, max=0.7876, mean=-0.0003, std=0.1461
[CLAP PIPELINE] Sample 0: 132885/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 184/10586 [00:51<48:55,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.98e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0011, max=0.9936, mean=0.0003, std=0.1463
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0117, max=0.9497, mean=0.0003, std=0.1462
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0117, max=0.9497, mean=0.0003, std=0.1462
[CLAP PIPELINE] Sample 0: 107501/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 185/10586 [00:52<48:53,  3.55it/s, v_num=hdbx, train/loss=nan.0, lr=2.98e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0016, max=0.8597, mean=-0.0001, std=0.1638
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9995, max=0.8486, mean=-0.0001, std=0.1638
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9995, max=0.8486, mean=-0.0001, std=0.1638
[CLAP PIPELINE] Sample 0: 107573/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 186/10586 [00:52<48:56,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.98e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8310, max=0.9703, mean=0.0000, std=0.1648
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8350, max=0.9697, mean=0.0000, std=0.1648
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8350, max=0.9697, mean=0.0000, std=0.1648
[CLAP PIPELINE] Sample 0: 1934/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 187/10586 [00:52<48:59,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.98e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0008, max=0.9175, mean=0.0012, std=0.1449
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9883, max=0.9146, mean=0.0012, std=0.1450
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9883, max=0.9146, mean=0.0012, std=0.1450
[CLAP PIPELINE] Sample 0: 123215/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 188/10586 [00:53<48:57,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.99e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9433, max=0.9782, mean=-0.0005, std=0.0523
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8989, max=0.8970, mean=-0.0005, std=0.0514
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8989, max=0.8970, mean=-0.0005, std=0.0514
[CLAP PIPELINE] Sample 0: 129997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▍                                                                                  | 189/10586 [00:53<48:56,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.99e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9998, max=0.9477, mean=-0.0004, std=0.0566
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9590, max=0.8774, mean=-0.0004, std=0.0565
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9590, max=0.8774, mean=-0.0004, std=0.0565
[CLAP PIPELINE] Sample 0: 143465/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                  | 190/10586 [00:53<48:54,  3.54it/s, v_num=hdbx, train/loss=nan.0, lr=2.99e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0004, max=0.9998, mean=-0.0000, std=0.2500
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=1.0000, mean=-0.0000, std=0.2500
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=1.0000, mean=-0.0000, std=0.2500
[CLAP PIPELINE] Sample 0: 48256/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                  | 191/10586 [00:53<48:50,  3.55it/s, v_num=hdbx, train/loss=nan.0, lr=2.99e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9542, max=0.9992, mean=-0.0002, std=0.0233
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9482, max=1.0039, mean=-0.0002, std=0.0233
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9482, max=1.0039, mean=-0.0002, std=0.0233
[CLAP PIPELINE] Sample 0: 126895/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                     | 192/10586 [00:54<48:51,  3.55it/s, v_num=hdbx, train/loss=nan.0, lr=3e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0097, max=1.0043, mean=0.0000, std=0.0573
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0127, max=1.0068, mean=0.0000, std=0.0574
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0127, max=1.0068, mean=0.0000, std=0.0574
[CLAP PIPELINE] Sample 0: 49207/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                     | 193/10586 [00:54<48:48,  3.55it/s, v_num=hdbx, train/loss=nan.0, lr=3e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9396, mean=-0.0023, std=0.0912
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0127, max=0.9453, mean=-0.0023, std=0.0912
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0127, max=0.9453, mean=-0.0023, std=0.0912
[CLAP PIPELINE] Sample 0: 113653/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                     | 194/10586 [00:54<48:46,  3.55it/s, v_num=hdbx, train/loss=nan.0, lr=3e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9465, max=0.9757, mean=-0.0003, std=0.2705
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0479, max=1.0293, mean=-0.0003, std=0.2649
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0479, max=1.0293, mean=-0.0003, std=0.2649
[CLAP PIPELINE] Sample 0: 95846/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                     | 195/10586 [00:54<48:47,  3.55it/s, v_num=hdbx, train/loss=nan.0, lr=3e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8210, max=1.0005, mean=0.0000, std=0.0306
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8125, max=0.9976, mean=0.0000, std=0.0306
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8125, max=0.9976, mean=0.0000, std=0.0306
[CLAP PIPELINE] Sample 0: 139913/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                  | 196/10586 [00:54<48:17,  3.59it/s, v_num=hdbx, train/loss=nan.0, lr=3.01e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0308, max=1.0208, mean=0.0000, std=0.2349
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0732, max=1.0781, mean=0.0000, std=0.2347
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0732, max=1.0781, mean=0.0000, std=0.2347
[CLAP PIPELINE] Sample 0: 110848/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                  | 197/10586 [00:54<48:16,  3.59it/s, v_num=hdbx, train/loss=nan.0, lr=3.01e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=1.0015, mean=-0.0003, std=0.0648
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9487, max=1.0020, mean=-0.0003, std=0.0648
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9487, max=1.0020, mean=-0.0003, std=0.0648
[CLAP PIPELINE] Sample 0: 142440/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                  | 198/10586 [00:55<48:12,  3.59it/s, v_num=hdbx, train/loss=nan.0, lr=3.01e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9563, max=1.0000, mean=-0.0006, std=0.0685
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9741, max=0.9819, mean=-0.0006, std=0.0685
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9741, max=0.9819, mean=-0.0006, std=0.0685
[CLAP PIPELINE] Sample 0: 133060/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                  | 199/10586 [00:55<48:14,  3.59it/s, v_num=hdbx, train/loss=nan.0, lr=3.01e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.9914, mean=-0.0011, std=0.0500
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9736, max=1.0029, mean=-0.0011, std=0.0500
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9736, max=1.0029, mean=-0.0011, std=0.0500
[CLAP PIPELINE] Sample 0: 116250/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                  | 200/10586 [00:55<48:11,  3.59it/s, v_num=hdbx, train/loss=nan.0, lr=3.02e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7779, max=0.9992, mean=-0.0001, std=0.0556
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7656, max=0.9976, mean=-0.0001, std=0.0554
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7656, max=0.9976, mean=-0.0001, std=0.0554
[CLAP PIPELINE] Sample 0: 55114/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                  | 201/10586 [00:55<48:12,  3.59it/s, v_num=hdbx, train/loss=nan.0, lr=3.02e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9602, max=0.7068, mean=-0.0000, std=0.1041
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9556, max=0.6943, mean=-0.0000, std=0.1041
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9556, max=0.6943, mean=-0.0000, std=0.1041
[CLAP PIPELINE] Sample 0: 127860/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                  | 202/10586 [00:56<48:10,  3.59it/s, v_num=hdbx, train/loss=nan.0, lr=3.02e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8002, max=1.0032, mean=-0.0002, std=0.0756
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0: 131203/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                  | 203/10586 [00:56<48:05,  3.60it/s, v_num=hdbx, train/loss=nan.0, lr=3.02e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9954, max=0.9578, mean=-0.0013, std=0.0753
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9922, max=0.9604, mean=-0.0013, std=0.0754
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9922, max=0.9604, mean=-0.0013, std=0.0754
[CLAP PIPELINE] Sample 0: 105618/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▌                                                                                  | 204/10586 [00:56<48:03,  3.60it/s, v_num=hdbx, train/loss=nan.0, lr=3.03e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0383, max=1.0489, mean=-0.0000, std=0.0767
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0078, max=0.9902, mean=-0.0000, std=0.0716
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0078, max=0.9902, mean=-0.0000, std=0.0716
[CLAP PIPELINE] Sample 0: 118847/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 205/10586 [00:56<48:00,  3.60it/s, v_num=hdbx, train/loss=nan.0, lr=3.03e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0746, max=0.9658, mean=-0.0000, std=0.0424
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6411, max=0.5557, mean=-0.0000, std=0.0312
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6411, max=0.5557, mean=-0.0000, std=0.0312
[CLAP PIPELINE] Sample 0: 126859/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 206/10586 [00:57<48:00,  3.60it/s, v_num=hdbx, train/loss=nan.0, lr=3.03e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.9314, mean=-0.0002, std=0.0972
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9316, mean=-0.0002, std=0.0973
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9316, mean=-0.0002, std=0.0973
[CLAP PIPELINE] Sample 0: 79478/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 207/10586 [00:57<47:58,  3.61it/s, v_num=hdbx, train/loss=nan.0, lr=3.04e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0045, max=1.0056, mean=-0.0264, std=0.5016
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0049, max=1.0166, mean=-0.0264, std=0.5015
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0049, max=1.0166, mean=-0.0264, std=0.5015
[CLAP PIPELINE] Sample 0: 48056/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 208/10586 [00:57<47:58,  3.61it/s, v_num=hdbx, train/loss=nan.0, lr=3.04e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0024, max=0.6007, mean=-0.0007, std=0.0930
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0: 133106/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 209/10586 [00:57<47:52,  3.61it/s, v_num=hdbx, train/loss=nan.0, lr=3.04e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7264, max=1.0099, mean=-0.0001, std=0.0313
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7222, max=0.9917, mean=-0.0001, std=0.0312
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7222, max=0.9917, mean=-0.0001, std=0.0312
[CLAP PIPELINE] Sample 0: 149131/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 210/10586 [00:58<47:50,  3.61it/s, v_num=hdbx, train/loss=nan.0, lr=3.04e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9860, max=0.6366, mean=-0.0001, std=0.0639
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Sample 0: 91800/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 211/10586 [00:58<47:45,  3.62it/s, v_num=hdbx, train/loss=nan.0, lr=3.05e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9090, max=0.9476, mean=-0.0000, std=0.0350
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9082, max=0.9351, mean=-0.0000, std=0.0350
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9082, max=0.9351, mean=-0.0000, std=0.0350
[CLAP PIPELINE] Sample 0: 90910/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 212/10586 [00:58<47:43,  3.62it/s, v_num=hdbx, train/loss=nan.0, lr=3.05e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=1.0000, mean=-0.0000, std=0.2652
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=1.0000, mean=-0.0000, std=0.2654
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=1.0000, mean=-0.0000, std=0.2654
[CLAP PIPELINE] Sample 0: 95868/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 213/10586 [00:58<47:44,  3.62it/s, v_num=hdbx, train/loss=nan.0, lr=3.05e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9874, max=0.9607, mean=-0.0000, std=0.0566
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.9512, mean=-0.0000, std=0.0565
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.9512, mean=-0.0000, std=0.0565
[CLAP PIPELINE] Sample 0: 106891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 214/10586 [00:59<47:40,  3.63it/s, v_num=hdbx, train/loss=nan.0, lr=3.05e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9890, max=0.5488, mean=-0.0000, std=0.0154
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=0.5200, mean=-0.0000, std=0.0153
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=0.5200, mean=-0.0000, std=0.0153
[CLAP PIPELINE] Sample 0: 78847/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 215/10586 [00:59<47:37,  3.63it/s, v_num=hdbx, train/loss=nan.0, lr=3.06e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9778, max=0.9985, mean=-0.0012, std=0.0540
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8291, max=0.8354, mean=-0.0012, std=0.0531
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8291, max=0.8354, mean=-0.0012, std=0.0531
[CLAP PIPELINE] Sample 0: 139561/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 216/10586 [00:59<47:34,  3.63it/s, v_num=hdbx, train/loss=nan.0, lr=3.06e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8866, max=0.9992, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7402, max=0.7954, mean=0.0000, std=0.0335
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7402, max=0.7954, mean=0.0000, std=0.0335
[CLAP PIPELINE] Sample 0: 104330/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 217/10586 [00:59<47:33,  3.63it/s, v_num=hdbx, train/loss=nan.0, lr=3.06e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0004, max=0.9938, mean=-0.0000, std=0.0471
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9932, mean=-0.0000, std=0.0471
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9932, mean=-0.0000, std=0.0471
[CLAP PIPELINE] Sample 0: 96561/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 218/10586 [01:00<47:34,  3.63it/s, v_num=hdbx, train/loss=nan.0, lr=3.06e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.9986, mean=0.0001, std=0.3426
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9990, mean=0.0001, std=0.3428
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9990, mean=0.0001, std=0.3428
[CLAP PIPELINE] Sample 0: 111848/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 219/10586 [00:59<47:07,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.07e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8575, mean=-0.0000, std=0.0995
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Sample 0: 66892/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▋                                                                                  | 220/10586 [01:00<47:07,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.07e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7515, max=0.9823, mean=-0.0000, std=0.0562
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5659, max=0.5854, mean=-0.0000, std=0.0477
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5659, max=0.5854, mean=-0.0000, std=0.0477
[CLAP PIPELINE] Sample 0: 62671/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 221/10586 [01:00<47:09,  3.66it/s, v_num=hdbx, train/loss=nan.0, lr=3.07e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0859, max=1.2781, mean=0.0005, std=0.1879
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0176, max=1.0000, mean=0.0005, std=0.1821
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0176, max=1.0000, mean=0.0005, std=0.1821
[CLAP PIPELINE] Sample 0: 37944/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 222/10586 [01:00<47:09,  3.66it/s, v_num=hdbx, train/loss=nan.0, lr=3.07e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=1.0092, mean=-0.0002, std=0.1041
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9097, max=1.0410, mean=-0.0002, std=0.1033
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9097, max=1.0410, mean=-0.0002, std=0.1033
[CLAP PIPELINE] Sample 0: 136024/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 223/10586 [01:00<47:05,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.08e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1897, max=1.2273, mean=-0.0000, std=0.1331
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9194, max=1.0537, mean=-0.0000, std=0.1292
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9194, max=1.0537, mean=-0.0000, std=0.1292
[CLAP PIPELINE] Sample 0: 65535/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 224/10586 [01:01<47:01,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.08e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9602, max=0.7068, mean=-0.0000, std=0.1041
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9556, max=0.6943, mean=-0.0000, std=0.1041
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9556, max=0.6943, mean=-0.0000, std=0.1041
[CLAP PIPELINE] Sample 0: 127860/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2299, max=14.2299, mean=14.2299
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 225/10586 [01:01<47:00,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.08e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9985, max=0.8840, mean=-0.0026, std=0.0598
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=0.8623, mean=-0.0026, std=0.0598
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=0.8623, mean=-0.0026, std=0.0598
[CLAP PIPELINE] Sample 0: 90708/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 226/10586 [01:01<47:00,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.08e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9952, max=0.9767, mean=-0.0001, std=0.0806
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0: 140476/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 227/10586 [01:01<47:01,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.09e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9808, mean=-0.0060, std=0.2939
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=0.9810, mean=-0.0060, std=0.2939
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=0.9810, mean=-0.0060, std=0.2939
[CLAP PIPELINE] Sample 0: 95725/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 228/10586 [01:02<47:00,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.09e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9998, mean=-0.0000, std=0.1118
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=1.0000, mean=-0.0000, std=0.1118
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=1.0000, mean=-0.0000, std=0.1118
[CLAP PIPELINE] Sample 0: 151882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 229/10586 [01:02<46:59,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.09e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8876, max=0.9996, mean=-0.0000, std=0.1307
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8882, max=0.9976, mean=-0.0000, std=0.1307
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8882, max=0.9976, mean=-0.0000, std=0.1307
[CLAP PIPELINE] Sample 0: 130724/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 230/10586 [01:02<46:55,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.09e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7930, max=0.9317, mean=-0.0000, std=0.0726
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7744, max=0.9214, mean=-0.0000, std=0.0726
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7744, max=0.9214, mean=-0.0000, std=0.0726
[CLAP PIPELINE] Sample 0: 110221/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                   | 231/10586 [01:02<46:52,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.1e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8500, max=1.0037, mean=-0.0000, std=0.1339
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8159, max=1.0000, mean=-0.0000, std=0.1339
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8159, max=1.0000, mean=-0.0000, std=0.1339
[CLAP PIPELINE] Sample 0: 1/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1829, max=0.1224, mean=-0.0026, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.8035, max=-0.8035, mean=-0.8035
[SIMILARITY] C_im_clap: min=-0.8327, max=-0.8327, mean=-0.8327
Epoch 0:   2%|█▊                                                                                   | 232/10586 [01:03<47:01,  3.67it/s, v_num=hdbx, train/loss=-0.00, lr=3.1e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=0.8569, mean=0.0008, std=0.2146
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.8496, mean=0.0008, std=0.2146
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.8496, mean=0.0008, std=0.2146
[CLAP PIPELINE] Sample 0: 96009/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                   | 233/10586 [01:03<46:59,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.1e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9752, max=1.0037, mean=-0.0017, std=0.0746
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9590, max=1.0029, mean=-0.0017, std=0.0746
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9590, max=1.0029, mean=-0.0017, std=0.0746
[CLAP PIPELINE] Sample 0: 104319/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 234/10586 [01:03<47:04,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.11e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7955, max=0.8804, mean=-0.0000, std=0.0121
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7207, max=0.6548, mean=-0.0000, std=0.0113
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7207, max=0.6548, mean=-0.0000, std=0.0113
[CLAP PIPELINE] Sample 0: 80040/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 235/10586 [01:04<47:03,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.11e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8165, max=0.7935, mean=-0.0002, std=0.1749
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6182, max=0.5903, mean=-0.0002, std=0.1547
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6182, max=0.5903, mean=-0.0002, std=0.1547
[CLAP PIPELINE] Sample 0: 47653/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▊                                                                                  | 236/10586 [01:04<47:00,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.11e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0003, max=0.9558, mean=-0.0000, std=0.0667
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9551, mean=-0.0000, std=0.0668
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9551, mean=-0.0000, std=0.0668
[CLAP PIPELINE] Sample 0: 82199/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 237/10586 [01:04<46:59,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.11e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9448, max=0.7345, mean=0.0000, std=0.0662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8896, max=0.5645, mean=0.0000, std=0.0489
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8896, max=0.5645, mean=0.0000, std=0.0489
[CLAP PIPELINE] Sample 0: 75420/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 238/10586 [01:05<47:10,  3.66it/s, v_num=hdbx, train/loss=nan.0, lr=3.12e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8575, mean=-0.0000, std=0.0995
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Sample 0: 66892/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 239/10586 [01:04<46:50,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.12e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9161, max=0.8852, mean=-0.0002, std=0.0118
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8423, max=0.8545, mean=-0.0002, std=0.0112
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8423, max=0.8545, mean=-0.0002, std=0.0112
[CLAP PIPELINE] Sample 0: 152915/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 240/10586 [01:05<46:48,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.12e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9023, max=1.0001, mean=-0.0002, std=0.0496
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8970, max=0.9990, mean=-0.0002, std=0.0496
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8970, max=0.9990, mean=-0.0002, std=0.0496
[CLAP PIPELINE] Sample 0: 155840/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 241/10586 [01:05<46:49,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.12e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8860, max=0.8689, mean=-0.0001, std=0.1662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Sample 0: 111849/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 242/10586 [01:05<46:49,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.13e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9986, max=0.9916, mean=0.0004, std=0.1325
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0: 99635/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 243/10586 [01:05<46:47,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.13e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0055, max=0.9462, mean=-0.0000, std=0.1227
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9702, max=0.9380, mean=-0.0000, std=0.1221
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9702, max=0.9380, mean=-0.0000, std=0.1221
[CLAP PIPELINE] Sample 0: 1/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1714, max=0.1242, mean=-0.0025, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap 1.0000
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 244/10586 [01:06<47:00,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.13e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9676, max=1.0002, mean=-0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0: 25933/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 245/10586 [01:06<46:57,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.13e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.7122, mean=0.0001, std=0.0413
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9814, max=0.7056, mean=0.0001, std=0.0413
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9814, max=0.7056, mean=0.0001, std=0.0413
[CLAP PIPELINE] Sample 0: 63882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 246/10586 [01:07<47:05,  3.66it/s, v_num=hdbx, train/loss=nan.0, lr=3.14e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0071, max=1.0056, mean=-0.0011, std=0.3486
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0244, max=1.0195, mean=-0.0011, std=0.3486
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0244, max=1.0195, mean=-0.0011, std=0.3486
[CLAP PIPELINE] Sample 0: 41639/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 247/10586 [01:07<47:02,  3.66it/s, v_num=hdbx, train/loss=nan.0, lr=3.14e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9982, max=0.8683, mean=-0.0054, std=0.0859
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.8677, mean=-0.0054, std=0.0859
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.8677, mean=-0.0054, std=0.0859
[CLAP PIPELINE] Sample 0: 127269/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 248/10586 [01:07<46:59,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.14e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9563, max=1.0000, mean=-0.0006, std=0.0685
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9741, max=0.9819, mean=-0.0006, std=0.0685
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9741, max=0.9819, mean=-0.0006, std=0.0685
[CLAP PIPELINE] Sample 0: 133060/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 249/10586 [01:07<47:01,  3.66it/s, v_num=hdbx, train/loss=nan.0, lr=3.14e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9874, max=0.9607, mean=-0.0000, std=0.0566
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.9512, mean=-0.0000, std=0.0565
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.9512, mean=-0.0000, std=0.0565
[CLAP PIPELINE] Sample 0: 106891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 250/10586 [01:08<47:01,  3.66it/s, v_num=hdbx, train/loss=nan.0, lr=3.15e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0640, max=0.6390, mean=0.0000, std=0.0066
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5913, max=0.5054, mean=0.0000, std=0.0059
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5913, max=0.5054, mean=0.0000, std=0.0059
[CLAP PIPELINE] Sample 0: 152282/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 251/10586 [01:08<47:01,  3.66it/s, v_num=hdbx, train/loss=nan.0, lr=3.15e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0018, max=0.9995, mean=-0.0000, std=0.2249
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9746, max=0.9883, mean=-0.0000, std=0.2236
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9746, max=0.9883, mean=-0.0000, std=0.2236
[CLAP PIPELINE] Sample 0: 16256/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|█▉                                                                                  | 252/10586 [01:08<47:08,  3.65it/s, v_num=hdbx, train/loss=nan.0, lr=3.15e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9149, max=0.9979, mean=-0.0002, std=0.1847
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9126, max=0.9985, mean=-0.0002, std=0.1847
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9126, max=0.9985, mean=-0.0002, std=0.1847
[CLAP PIPELINE] Sample 0: 126339/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 253/10586 [01:09<47:08,  3.65it/s, v_num=hdbx, train/loss=nan.0, lr=3.15e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.5452, max=1.0035, mean=0.0012, std=0.1025
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5444, max=1.0029, mean=0.0012, std=0.1025
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5444, max=1.0029, mean=0.0012, std=0.1025
[CLAP PIPELINE] Sample 0: 119345/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 254/10586 [01:09<47:12,  3.65it/s, v_num=hdbx, train/loss=nan.0, lr=3.16e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9950, max=0.7198, mean=-0.0000, std=0.1696
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8291, max=0.7563, mean=-0.0000, std=0.1656
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8291, max=0.7563, mean=-0.0000, std=0.1656
[CLAP PIPELINE] Sample 0: 1/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1841, max=0.1260, mean=-0.0027, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap 1.0000
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 255/10586 [01:09<47:14,  3.64it/s, v_num=hdbx, train/loss=nan.0, lr=3.16e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6800, max=0.9864, mean=-0.0001, std=0.0365
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5161, max=0.6753, mean=-0.0001, std=0.0350
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5161, max=0.6753, mean=-0.0001, std=0.0350
[CLAP PIPELINE] Sample 0: 29014/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 256/10586 [01:09<46:56,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.16e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0007, max=1.0000, mean=-0.0003, std=0.2923
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9961, max=0.9990, mean=-0.0003, std=0.2922
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9961, max=0.9990, mean=-0.0003, std=0.2922
[CLAP PIPELINE] Sample 0: 81134/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 257/10586 [01:10<46:55,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.17e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9793, max=0.6877, mean=-0.0000, std=0.0244
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8970, max=0.6709, mean=-0.0000, std=0.0239
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8970, max=0.6709, mean=-0.0000, std=0.0239
[CLAP PIPELINE] Sample 0: 142943/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 258/10586 [01:10<46:55,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.17e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9559, max=0.6156, mean=0.0001, std=0.0153
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0: 150065/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 259/10586 [01:10<46:58,  3.66it/s, v_num=hdbx, train/loss=nan.0, lr=3.17e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9178, max=0.9936, mean=0.1238, std=0.4039
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=0.9927, mean=0.1239, std=0.4033
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=0.9927, mean=0.1239, std=0.4033
[CLAP PIPELINE] Sample 0: 22882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 260/10586 [01:10<46:56,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.17e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9178, max=0.9936, mean=0.1238, std=0.4039
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=0.9927, mean=0.1239, std=0.4033
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=0.9927, mean=0.1239, std=0.4033
[CLAP PIPELINE] Sample 0: 22882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 261/10586 [01:11<46:54,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.18e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9823, max=0.6926, mean=-0.0307, std=0.0572
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8652, max=0.5552, mean=-0.0308, std=0.0558
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8652, max=0.5552, mean=-0.0308, std=0.0558
[CLAP PIPELINE] Sample 0: 79775/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 262/10586 [01:11<46:55,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.18e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9832, max=1.0024, mean=-0.0005, std=0.0982
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0049, max=0.9897, mean=-0.0005, std=0.0981
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0049, max=0.9897, mean=-0.0005, std=0.0981
[CLAP PIPELINE] Sample 0: 139602/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 263/10586 [01:11<46:53,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.18e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6688, max=0.7300, mean=-0.0000, std=0.0359
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6216, max=0.6060, mean=-0.0000, std=0.0346
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6216, max=0.6060, mean=-0.0000, std=0.0346
[CLAP PIPELINE] Sample 0: 127682/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   2%|██                                                                                  | 264/10586 [01:11<46:52,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.18e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9622, max=0.9921, mean=-0.0015, std=0.1292
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9790, mean=-0.0015, std=0.1293
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9790, mean=-0.0015, std=0.1293
[CLAP PIPELINE] Sample 0: 119765/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██                                                                                  | 265/10586 [01:12<46:49,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.19e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9969, max=0.9446, mean=-0.0009, std=0.0720
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9927, max=0.9448, mean=-0.0009, std=0.0720
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9927, max=0.9448, mean=-0.0009, std=0.0720
[CLAP PIPELINE] Sample 0: 129531/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██                                                                                  | 266/10586 [01:12<46:48,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.19e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.8496, mean=-0.0000, std=0.0451
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0: 101891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██                                                                                  | 267/10586 [01:12<46:48,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.19e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8739, max=0.9986, mean=-0.0000, std=0.0380
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0: 61484/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 268/10586 [01:12<46:45,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.19e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9974, max=0.9909, mean=-0.0001, std=0.2138
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9873, max=0.9883, mean=-0.0001, std=0.2139
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9873, max=0.9883, mean=-0.0001, std=0.2139
[CLAP PIPELINE] Sample 0: 65779/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                  | 269/10586 [01:13<46:46,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.2e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9724, max=1.0004, mean=-0.0000, std=0.1376
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9722, max=1.0000, mean=-0.0000, std=0.1376
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9722, max=1.0000, mean=-0.0000, std=0.1376
[CLAP PIPELINE] Sample 0: 67796/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                  | 270/10586 [01:13<46:45,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.2e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0003, max=0.8273, mean=-0.0025, std=0.1678
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0: 24967/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                  | 271/10586 [01:13<46:44,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.2e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9341, max=0.9025, mean=-0.0009, std=0.0382
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8545, max=0.8267, mean=-0.0009, std=0.0369
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8545, max=0.8267, mean=-0.0009, std=0.0369
[CLAP PIPELINE] Sample 0: 107183/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                  | 272/10586 [01:13<46:44,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.2e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0117, max=0.8647, mean=-0.0006, std=0.1469
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0166, max=0.8638, mean=-0.0006, std=0.1465
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0166, max=0.8638, mean=-0.0006, std=0.1465
[CLAP PIPELINE] Sample 0: 80170/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 273/10586 [01:14<46:44,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.21e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9880, max=0.8940, mean=0.0001, std=0.1081
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.8838, mean=0.0001, std=0.1069
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.8838, mean=0.0001, std=0.1069
[CLAP PIPELINE] Sample 0: 80626/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 274/10586 [01:14<46:45,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.21e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9023, max=1.0001, mean=-0.0002, std=0.0496
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8970, max=0.9990, mean=-0.0002, std=0.0496
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8970, max=0.9990, mean=-0.0002, std=0.0496
[CLAP PIPELINE] Sample 0: 155840/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 275/10586 [01:14<46:45,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.21e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9898, max=1.0118, mean=-0.0000, std=0.1608
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=1.0391, mean=-0.0000, std=0.1609
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=1.0391, mean=-0.0000, std=0.1609
[CLAP PIPELINE] Sample 0: 76110/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 276/10586 [01:15<46:46,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.21e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9970, max=0.7334, mean=-0.0008, std=0.0188
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9492, max=0.5708, mean=-0.0008, std=0.0180
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9492, max=0.5708, mean=-0.0008, std=0.0180
[CLAP PIPELINE] Sample 0: 131019/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 277/10586 [01:22<50:58,  3.37it/s, v_num=hdbx, train/loss=nan.0, lr=3.22e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0003, max=0.8273, mean=-0.0025, std=0.1678
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0: 24967/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 278/10586 [01:22<50:57,  3.37it/s, v_num=hdbx, train/loss=nan.0, lr=3.22e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9410, max=0.9987, mean=0.0008, std=0.1114
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8623, max=0.8574, mean=0.0008, std=0.1111
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8623, max=0.8574, mean=0.0008, std=0.1111
[CLAP PIPELINE] Sample 0: 129901/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 279/10586 [01:22<50:56,  3.37it/s, v_num=hdbx, train/loss=nan.0, lr=3.22e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8718, max=0.9969, mean=-0.0000, std=0.2954
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8662, max=0.9927, mean=-0.0000, std=0.2954
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8662, max=0.9927, mean=-0.0000, std=0.2954
[CLAP PIPELINE] Sample 0: 80235/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 280/10586 [01:22<50:52,  3.38it/s, v_num=hdbx, train/loss=nan.0, lr=3.22e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7392, max=1.0271, mean=0.0001, std=0.0326
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6353, max=0.8188, mean=0.0001, std=0.0320
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6353, max=0.8188, mean=0.0001, std=0.0320
[CLAP PIPELINE] Sample 0: 66956/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 281/10586 [01:23<50:49,  3.38it/s, v_num=hdbx, train/loss=nan.0, lr=3.23e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9559, max=0.6156, mean=0.0001, std=0.0153
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0: 150065/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 282/10586 [01:23<50:47,  3.38it/s, v_num=hdbx, train/loss=nan.0, lr=3.23e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.9017, mean=0.0002, std=0.1192
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9956, max=0.9019, mean=0.0002, std=0.1191
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9956, max=0.9019, mean=0.0002, std=0.1191
[CLAP PIPELINE] Sample 0: 131309/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▏                                                                                 | 283/10586 [01:23<50:46,  3.38it/s, v_num=hdbx, train/loss=nan.0, lr=3.23e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9953, max=0.7769, mean=-0.0001, std=0.0524
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9912, max=0.7764, mean=-0.0001, std=0.0524
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9912, max=0.7764, mean=-0.0001, std=0.0524
[CLAP PIPELINE] Sample 0: 80309/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 284/10586 [01:23<50:46,  3.38it/s, v_num=hdbx, train/loss=nan.0, lr=3.24e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9991, max=0.9868, mean=-0.0023, std=0.0904
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0039, max=0.9937, mean=-0.0023, std=0.0904
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0039, max=0.9937, mean=-0.0023, std=0.0904
[CLAP PIPELINE] Sample 0: 136496/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 285/10586 [01:24<50:44,  3.38it/s, v_num=hdbx, train/loss=nan.0, lr=3.24e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9965, mean=-0.0004, std=0.1406
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0: 27046/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 286/10586 [01:24<50:41,  3.39it/s, v_num=hdbx, train/loss=nan.0, lr=3.24e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.8923, mean=-0.0074, std=0.0662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9888, max=0.8657, mean=-0.0074, std=0.0661
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9888, max=0.8657, mean=-0.0074, std=0.0661
[CLAP PIPELINE] Sample 0: 137635/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 287/10586 [01:24<50:40,  3.39it/s, v_num=hdbx, train/loss=nan.0, lr=3.24e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9992, max=0.9834, mean=-0.0002, std=0.0612
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9458, max=0.9844, mean=-0.0002, std=0.0611
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9458, max=0.9844, mean=-0.0002, std=0.0611
[CLAP PIPELINE] Sample 0: 129159/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 288/10586 [01:24<50:37,  3.39it/s, v_num=hdbx, train/loss=nan.0, lr=3.25e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9835, max=1.0001, mean=-0.0079, std=0.3950
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9839, max=1.0010, mean=-0.0079, std=0.3953
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9839, max=1.0010, mean=-0.0079, std=0.3953
[CLAP PIPELINE] Sample 0: 95922/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 289/10586 [01:25<50:37,  3.39it/s, v_num=hdbx, train/loss=nan.0, lr=3.25e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0000, max=0.8992, mean=-0.0015, std=0.0570
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.8818, mean=-0.0015, std=0.0570
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.8818, mean=-0.0015, std=0.0570
[CLAP PIPELINE] Sample 0: 91147/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 290/10586 [01:25<50:33,  3.39it/s, v_num=hdbx, train/loss=nan.0, lr=3.25e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8817, max=0.9950, mean=-0.0063, std=0.0711
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8779, max=0.9580, mean=-0.0063, std=0.0710
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8779, max=0.9580, mean=-0.0063, std=0.0710
[CLAP PIPELINE] Sample 0: 135987/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 291/10586 [01:25<50:32,  3.39it/s, v_num=hdbx, train/loss=nan.0, lr=3.25e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0280, max=1.1183, mean=0.0004, std=0.0506
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8228, max=0.7773, mean=0.0004, std=0.0494
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8228, max=0.7773, mean=0.0004, std=0.0494
[CLAP PIPELINE] Sample 0: 48179/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 292/10586 [01:25<50:30,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.26e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9968, max=0.9985, mean=0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0166, max=1.0215, mean=0.0000, std=0.1312
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0166, max=1.0215, mean=0.0000, std=0.1312
[CLAP PIPELINE] Sample 0: 86803/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 293/10586 [01:26<50:29,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.26e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9512, max=1.0018, mean=-0.0001, std=0.0323
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8413, max=0.8599, mean=-0.0001, std=0.0254
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8413, max=0.8599, mean=-0.0001, std=0.0254
[CLAP PIPELINE] Sample 0: 156966/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 294/10586 [01:26<50:26,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.26e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9993, max=0.8699, mean=-0.0019, std=0.1727
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.8672, mean=-0.0019, std=0.1727
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.8672, mean=-0.0019, std=0.1727
[CLAP PIPELINE] Sample 0: 94052/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 295/10586 [01:26<50:24,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.26e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0767, max=1.0207, mean=-0.0001, std=0.2233
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8193, max=0.8135, mean=-0.0001, std=0.2155
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8193, max=0.8135, mean=-0.0001, std=0.2155
[CLAP PIPELINE] Sample 0: 35158/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 296/10586 [01:27<50:25,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.27e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8497, max=0.9994, mean=-0.0007, std=0.2589
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8501, max=1.0000, mean=-0.0008, std=0.2590
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=1.0000, mean=-0.0008, std=0.2590
[CLAP PIPELINE] Sample 0: 86957/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 297/10586 [01:27<50:26,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.27e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9910, max=1.1091, mean=0.0000, std=0.0084
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.3403, max=0.3696, mean=0.0000, std=0.0046
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.3403, max=0.3696, mean=0.0000, std=0.0046
[CLAP PIPELINE] Sample 0: 158782/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 298/10586 [01:27<50:28,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.27e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0013, max=0.9574, mean=-0.0000, std=0.0927
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9795, max=0.9463, mean=-0.0000, std=0.0925
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9795, max=0.9463, mean=-0.0000, std=0.0925
[CLAP PIPELINE] Sample 0: 90037/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▎                                                                                 | 299/10586 [01:27<50:24,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.27e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9987, max=0.9952, mean=0.0001, std=0.1110
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9878, mean=0.0001, std=0.1110
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9878, mean=0.0001, std=0.1110
[CLAP PIPELINE] Sample 0: 142670/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 300/10586 [01:28<50:24,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.28e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9974, max=0.9909, mean=-0.0001, std=0.2138
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9873, max=0.9883, mean=-0.0001, std=0.2139
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9873, max=0.9883, mean=-0.0001, std=0.2139
[CLAP PIPELINE] Sample 0: 65779/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 301/10586 [01:28<50:26,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.28e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9624, max=1.0002, mean=-0.0004, std=0.1298
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Sample 0: 144175/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 302/10586 [01:28<50:24,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.28e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0008, max=0.6007, mean=0.0002, std=0.0658
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=0.5972, mean=0.0002, std=0.0658
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=0.5972, mean=0.0002, std=0.0658
[CLAP PIPELINE] Sample 0: 92790/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 303/10586 [01:29<50:22,  3.40it/s, v_num=hdbx, train/loss=nan.0, lr=3.28e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9845, max=0.9903, mean=0.0000, std=0.0162
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9717, max=0.9814, mean=0.0000, std=0.0160
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9717, max=0.9814, mean=0.0000, std=0.0160
[CLAP PIPELINE] Sample 0: 152397/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 304/10586 [01:29<50:19,  3.41it/s, v_num=hdbx, train/loss=nan.0, lr=3.29e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9891, max=0.9227, mean=-0.0000, std=0.1719
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9766, max=0.9155, mean=-0.0000, std=0.1719
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9766, max=0.9155, mean=-0.0000, std=0.1719
[CLAP PIPELINE] Sample 0: 64060/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 305/10586 [01:29<50:17,  3.41it/s, v_num=hdbx, train/loss=nan.0, lr=3.29e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9023, max=1.0001, mean=-0.0002, std=0.0496
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8970, max=0.9990, mean=-0.0002, std=0.0496
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8970, max=0.9990, mean=-0.0002, std=0.0496
[CLAP PIPELINE] Sample 0: 155840/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 306/10586 [01:29<50:15,  3.41it/s, v_num=hdbx, train/loss=nan.0, lr=3.29e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8866, max=0.9992, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7402, max=0.7954, mean=0.0000, std=0.0335
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7402, max=0.7954, mean=0.0000, std=0.0335
[CLAP PIPELINE] Sample 0: 104330/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 307/10586 [01:29<50:11,  3.41it/s, v_num=hdbx, train/loss=nan.0, lr=3.29e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9234, max=0.9909, mean=-0.0019, std=0.1012
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8613, max=0.9893, mean=-0.0019, std=0.1000
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8613, max=0.9893, mean=-0.0019, std=0.1000
[CLAP PIPELINE] Sample 0: 115693/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                  | 308/10586 [01:30<50:11,  3.41it/s, v_num=hdbx, train/loss=nan.0, lr=3.3e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0940, max=1.0025, mean=0.0008, std=0.0431
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0020, max=0.9111, mean=0.0008, std=0.0389
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0020, max=0.9111, mean=0.0008, std=0.0389
[CLAP PIPELINE] Sample 0: 19021/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                  | 309/10586 [01:30<50:11,  3.41it/s, v_num=hdbx, train/loss=nan.0, lr=3.3e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9023, max=1.0001, mean=-0.0002, std=0.0496
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8970, max=0.9990, mean=-0.0002, std=0.0496
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8970, max=0.9990, mean=-0.0002, std=0.0496
[CLAP PIPELINE] Sample 0: 155840/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                  | 310/10586 [01:30<50:06,  3.42it/s, v_num=hdbx, train/loss=nan.0, lr=3.3e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0170, max=1.0732, mean=0.0000, std=0.0607
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0791, max=1.0430, mean=0.0000, std=0.0543
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0791, max=1.0430, mean=0.0000, std=0.0543
[CLAP PIPELINE] Sample 0: 136856/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 311/10586 [01:30<50:03,  3.42it/s, v_num=hdbx, train/loss=nan.0, lr=3.31e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9991, max=0.9868, mean=-0.0023, std=0.0904
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0039, max=0.9937, mean=-0.0023, std=0.0904
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0039, max=0.9937, mean=-0.0023, std=0.0904
[CLAP PIPELINE] Sample 0: 136496/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 312/10586 [01:31<50:00,  3.42it/s, v_num=hdbx, train/loss=nan.0, lr=3.31e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0096, max=0.9659, mean=-0.0000, std=0.1041
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9419, max=0.7036, mean=-0.0000, std=0.1002
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9419, max=0.7036, mean=-0.0000, std=0.1002
[CLAP PIPELINE] Sample 0: 64231/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 313/10586 [01:31<49:57,  3.43it/s, v_num=hdbx, train/loss=nan.0, lr=3.31e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9802, max=1.0484, mean=0.0006, std=0.2153
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9399, max=0.9727, mean=0.0006, std=0.2085
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9399, max=0.9727, mean=0.0006, std=0.2085
[CLAP PIPELINE] Sample 0: 13341/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 314/10586 [01:31<49:55,  3.43it/s, v_num=hdbx, train/loss=nan.0, lr=3.31e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.8923, mean=-0.0074, std=0.0662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9888, max=0.8657, mean=-0.0074, std=0.0661
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9888, max=0.8657, mean=-0.0074, std=0.0661
[CLAP PIPELINE] Sample 0: 137635/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▍                                                                                 | 315/10586 [01:31<49:51,  3.43it/s, v_num=hdbx, train/loss=nan.0, lr=3.32e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9668, max=0.7676, mean=-0.0030, std=0.1058
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9414, max=0.6450, mean=-0.0030, std=0.1058
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9414, max=0.6450, mean=-0.0030, std=0.1058
[CLAP PIPELINE] Sample 0: 800/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 316/10586 [01:31<49:46,  3.44it/s, v_num=hdbx, train/loss=nan.0, lr=3.32e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9034, max=1.0000, mean=-0.0000, std=0.1119
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0: 63474/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 317/10586 [01:32<49:42,  3.44it/s, v_num=hdbx, train/loss=nan.0, lr=3.32e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9998, max=0.9835, mean=-0.0003, std=0.0390
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0547, max=1.0146, mean=-0.0003, std=0.0382
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0547, max=1.0146, mean=-0.0003, std=0.0382
[CLAP PIPELINE] Sample 0: 134601/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 318/10586 [01:32<49:39,  3.45it/s, v_num=hdbx, train/loss=nan.0, lr=3.32e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9285, max=0.9992, mean=-0.0009, std=0.0743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Sample 0: 129718/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 319/10586 [01:32<49:36,  3.45it/s, v_num=hdbx, train/loss=nan.0, lr=3.33e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9487, max=0.9851, mean=0.0053, std=0.2636
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9243, max=0.9644, mean=0.0053, std=0.2637
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9243, max=0.9644, mean=0.0053, std=0.2637
[CLAP PIPELINE] Sample 0: 124477/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 320/10586 [01:32<49:34,  3.45it/s, v_num=hdbx, train/loss=nan.0, lr=3.33e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.9603, mean=-0.0011, std=0.1743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9385, max=0.9468, mean=-0.0011, std=0.1738
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9385, max=0.9468, mean=-0.0011, std=0.1738
[CLAP PIPELINE] Sample 0: 103336/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 321/10586 [01:33<49:36,  3.45it/s, v_num=hdbx, train/loss=nan.0, lr=3.33e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8575, mean=-0.0000, std=0.0995
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Sample 0: 66892/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 322/10586 [01:33<49:33,  3.45it/s, v_num=hdbx, train/loss=nan.0, lr=3.33e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8687, max=0.9427, mean=-0.0000, std=0.0148
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4741, max=0.4734, mean=-0.0000, std=0.0094
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4741, max=0.4734, mean=-0.0000, std=0.0094
[CLAP PIPELINE] Sample 0: 143717/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 323/10586 [01:33<49:34,  3.45it/s, v_num=hdbx, train/loss=nan.0, lr=3.34e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8672, max=0.9987, mean=-0.0006, std=0.1397
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8564, max=1.0049, mean=-0.0006, std=0.1396
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8564, max=1.0049, mean=-0.0006, std=0.1396
[CLAP PIPELINE] Sample 0: 77077/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 324/10586 [01:33<49:30,  3.46it/s, v_num=hdbx, train/loss=nan.0, lr=3.34e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9904, max=0.9976, mean=-0.0010, std=0.1395
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0020, max=0.9868, mean=-0.0010, std=0.1395
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0020, max=0.9868, mean=-0.0010, std=0.1395
[CLAP PIPELINE] Sample 0: 117096/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 325/10586 [01:33<49:27,  3.46it/s, v_num=hdbx, train/loss=nan.0, lr=3.34e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9986, mean=-0.0003, std=0.0470
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8184, max=0.8174, mean=-0.0003, std=0.0450
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8184, max=0.8174, mean=-0.0003, std=0.0450
[CLAP PIPELINE] Sample 0: 146392/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 326/10586 [01:34<49:23,  3.46it/s, v_num=hdbx, train/loss=nan.0, lr=3.34e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9898, max=1.0118, mean=-0.0000, std=0.1608
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=1.0391, mean=-0.0000, std=0.1609
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=1.0391, mean=-0.0000, std=0.1609
[CLAP PIPELINE] Sample 0: 76110/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 327/10586 [01:34<49:22,  3.46it/s, v_num=hdbx, train/loss=nan.0, lr=3.35e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9295, max=0.9991, mean=-0.0002, std=0.2361
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=1.0010, mean=-0.0002, std=0.2361
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=1.0010, mean=-0.0002, std=0.2361
[CLAP PIPELINE] Sample 0: 63764/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 328/10586 [01:34<49:21,  3.46it/s, v_num=hdbx, train/loss=nan.0, lr=3.35e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7567, max=0.9996, mean=-0.0000, std=0.0867
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7563, max=0.9883, mean=-0.0000, std=0.0863
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7563, max=0.9883, mean=-0.0000, std=0.0863
[CLAP PIPELINE] Sample 0: 77985/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 329/10586 [01:35<49:22,  3.46it/s, v_num=hdbx, train/loss=nan.0, lr=3.35e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0010, max=0.9703, mean=-0.0000, std=0.0720
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.9653, mean=-0.0000, std=0.0720
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.9653, mean=-0.0000, std=0.0720
[CLAP PIPELINE] Sample 0: 113177/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▌                                                                                 | 330/10586 [01:35<49:18,  3.47it/s, v_num=hdbx, train/loss=nan.0, lr=3.35e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9783, max=0.9732, mean=-0.0000, std=0.0987
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9448, max=0.9648, mean=-0.0000, std=0.0987
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9448, max=0.9648, mean=-0.0000, std=0.0987
[CLAP PIPELINE] Sample 0: 50929/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 331/10586 [01:35<49:15,  3.47it/s, v_num=hdbx, train/loss=nan.0, lr=3.36e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0832, max=1.0496, mean=0.0001, std=0.1933
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0156, max=1.0684, mean=0.0001, std=0.1931
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0156, max=1.0684, mean=0.0001, std=0.1931
[CLAP PIPELINE] Sample 0: 3917/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 332/10586 [01:35<49:12,  3.47it/s, v_num=hdbx, train/loss=nan.0, lr=3.36e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9057, max=0.9394, mean=0.0000, std=0.0707
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0: 51621/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 333/10586 [01:35<49:11,  3.47it/s, v_num=hdbx, train/loss=nan.0, lr=3.36e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9966, max=1.0236, mean=-0.0004, std=0.1075
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8091, max=0.8008, mean=-0.0004, std=0.0959
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8091, max=0.8008, mean=-0.0004, std=0.0959
[CLAP PIPELINE] Sample 0: 68168/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 334/10586 [01:36<49:07,  3.48it/s, v_num=hdbx, train/loss=nan.0, lr=3.37e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9980, max=0.8624, mean=-0.0004, std=0.1807
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.8608, mean=-0.0004, std=0.1807
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.8608, mean=-0.0004, std=0.1807
[CLAP PIPELINE] Sample 0: 90997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 335/10586 [01:36<49:06,  3.48it/s, v_num=hdbx, train/loss=nan.0, lr=3.37e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9968, max=0.9985, mean=0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0166, max=1.0215, mean=0.0000, std=0.1312
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0166, max=1.0215, mean=0.0000, std=0.1312
[CLAP PIPELINE] Sample 0: 86803/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 336/10586 [01:36<49:05,  3.48it/s, v_num=hdbx, train/loss=nan.0, lr=3.37e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9907, max=0.9676, mean=-0.0000, std=0.0810
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9761, max=0.9297, mean=-0.0000, std=0.0802
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9761, max=0.9297, mean=-0.0000, std=0.0802
[CLAP PIPELINE] Sample 0: 121137/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 337/10586 [01:36<49:01,  3.48it/s, v_num=hdbx, train/loss=nan.0, lr=3.37e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9965, mean=-0.0004, std=0.1406
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0: 27046/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 338/10586 [01:36<48:59,  3.49it/s, v_num=hdbx, train/loss=nan.0, lr=3.38e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9734, max=0.9692, mean=-0.0008, std=0.4332
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=1.0049, mean=-0.0008, std=0.4280
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=1.0049, mean=-0.0008, std=0.4280
[CLAP PIPELINE] Sample 0: 88002/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 339/10586 [01:28<44:37,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.38e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9989, max=0.9999, mean=0.0000, std=0.0898
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=1.0010, mean=0.0000, std=0.0898
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=1.0010, mean=0.0000, std=0.0898
[CLAP PIPELINE] Sample 0: 148053/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 340/10586 [01:28<44:36,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.38e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0: 47237/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 341/10586 [01:29<44:34,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.38e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0225, max=0.9850, mean=-0.0003, std=0.1048
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8037, max=0.9800, mean=-0.0003, std=0.1039
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8037, max=0.9800, mean=-0.0003, std=0.1039
[CLAP PIPELINE] Sample 0: 6847/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 342/10586 [01:29<44:34,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.39e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8297, max=1.0162, mean=-0.0000, std=0.0370
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Sample 0: 64456/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 343/10586 [01:29<44:35,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.39e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9934, max=0.7545, mean=-0.0000, std=0.0225
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5693, max=0.7412, mean=-0.0000, std=0.0215
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5693, max=0.7412, mean=-0.0000, std=0.0215
[CLAP PIPELINE] Sample 0: 119261/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 344/10586 [01:37<48:08,  3.55it/s, v_num=hdbx, train/loss=nan.0, lr=3.39e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9204, max=0.9984, mean=-0.0000, std=0.0979
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=0.9570, mean=-0.0000, std=0.0979
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=0.9570, mean=-0.0000, std=0.0979
[CLAP PIPELINE] Sample 0: 73735/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▋                                                                                 | 345/10586 [01:29<44:30,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.39e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9937, max=1.0002, mean=0.0001, std=0.1020
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9941, max=1.0000, mean=0.0001, std=0.1021
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9941, max=1.0000, mean=0.0001, std=0.1021
[CLAP PIPELINE] Sample 0: 112296/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                  | 346/10586 [01:30<44:32,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9969, max=0.9446, mean=-0.0009, std=0.0720
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9927, max=0.9448, mean=-0.0009, std=0.0720
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9927, max=0.9448, mean=-0.0009, std=0.0720
[CLAP PIPELINE] Sample 0: 129531/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                  | 347/10586 [01:30<44:29,  3.84it/s, v_num=hdbx, train/loss=nan.0, lr=3.4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9966, max=0.7492, mean=-0.0000, std=0.0625
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9888, max=0.7114, mean=-0.0000, std=0.0625
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9888, max=0.7114, mean=-0.0000, std=0.0625
[CLAP PIPELINE] Sample 0: 142208/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                  | 348/10586 [01:30<44:30,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9969, max=0.9969, mean=-0.0001, std=0.4822
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9956, max=0.9956, mean=-0.0001, std=0.4824
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9956, max=0.9956, mean=-0.0001, std=0.4824
[CLAP PIPELINE] Sample 0: 48074/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                  | 349/10586 [01:31<44:31,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9998, mean=-0.0000, std=0.1118
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=1.0000, mean=-0.0000, std=0.1118
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=1.0000, mean=-0.0000, std=0.1118
[CLAP PIPELINE] Sample 0: 151882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 350/10586 [01:31<44:31,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.41e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9880, max=0.8940, mean=0.0001, std=0.1081
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.8838, mean=0.0001, std=0.1069
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.8838, mean=0.0001, std=0.1069
[CLAP PIPELINE] Sample 0: 80626/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 351/10586 [01:31<44:28,  3.84it/s, v_num=hdbx, train/loss=nan.0, lr=3.41e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6800, max=0.9864, mean=-0.0001, std=0.0365
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5161, max=0.6753, mean=-0.0001, std=0.0350
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5161, max=0.6753, mean=-0.0001, std=0.0350
[CLAP PIPELINE] Sample 0: 29014/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 352/10586 [01:31<44:29,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.41e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9599, max=0.9711, mean=-0.0009, std=0.0730
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9487, max=0.8784, mean=-0.0009, std=0.0698
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9487, max=0.8784, mean=-0.0009, std=0.0698
[CLAP PIPELINE] Sample 0: 138518/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 353/10586 [01:32<44:28,  3.84it/s, v_num=hdbx, train/loss=nan.0, lr=3.41e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0018, max=0.9247, mean=-0.0000, std=0.4909
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0059, max=0.9419, mean=-0.0000, std=0.4905
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0059, max=0.9419, mean=-0.0000, std=0.4905
[CLAP PIPELINE] Sample 0: 17289/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 354/10586 [01:32<44:28,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.42e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9034, max=1.0000, mean=-0.0000, std=0.1119
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Sample 0: 63474/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 355/10586 [01:32<44:28,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.42e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8162, max=0.9993, mean=-0.0001, std=0.0561
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8091, max=0.9873, mean=-0.0001, std=0.0559
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8091, max=0.9873, mean=-0.0001, std=0.0559
[CLAP PIPELINE] Sample 0: 96048/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 356/10586 [01:32<44:28,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.42e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9938, max=0.9878, mean=-0.0000, std=0.1524
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9902, mean=-0.0000, std=0.1533
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9902, mean=-0.0000, std=0.1533
[CLAP PIPELINE] Sample 0: 72551/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 357/10586 [01:33<44:27,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.42e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0: 150811/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 358/10586 [01:33<44:26,  3.84it/s, v_num=hdbx, train/loss=nan.0, lr=3.43e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7296, max=1.0020, mean=-0.0000, std=0.1396
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7324, max=0.9863, mean=-0.0000, std=0.1396
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7324, max=0.9863, mean=-0.0000, std=0.1396
[CLAP PIPELINE] Sample 0: 44167/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 359/10586 [01:33<44:27,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.43e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9449, max=0.9981, mean=0.0001, std=0.1236
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9351, max=0.9951, mean=0.0001, std=0.1236
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9351, max=0.9951, mean=0.0001, std=0.1236
[CLAP PIPELINE] Sample 0: 55221/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 360/10586 [01:33<44:27,  3.83it/s, v_num=hdbx, train/loss=nan.0, lr=3.43e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=0.9948, mean=0.0001, std=0.0673
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9961, max=0.9951, mean=0.0001, std=0.0673
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9961, max=0.9951, mean=0.0001, std=0.0673
[CLAP PIPELINE] Sample 0: 141545/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 361/10586 [01:34<44:25,  3.84it/s, v_num=hdbx, train/loss=nan.0, lr=3.44e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9991, max=0.7584, mean=-0.0004, std=0.1498
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9966, max=0.7544, mean=-0.0004, std=0.1497
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9966, max=0.7544, mean=-0.0004, std=0.1497
[CLAP PIPELINE] Sample 0: 37258/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▊                                                                                 | 362/10586 [01:34<44:22,  3.84it/s, v_num=hdbx, train/loss=nan.0, lr=3.44e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9874, max=0.9607, mean=-0.0000, std=0.0566
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.9512, mean=-0.0000, std=0.0565
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.9512, mean=-0.0000, std=0.0565
[CLAP PIPELINE] Sample 0: 106891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▉                                                                                 | 363/10586 [01:34<44:21,  3.84it/s, v_num=hdbx, train/loss=nan.0, lr=3.44e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8864, max=1.0000, mean=0.0002, std=0.0629
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8867, max=1.0000, mean=0.0002, std=0.0629
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8867, max=1.0000, mean=0.0002, std=0.0629
[CLAP PIPELINE] Sample 0: 112030/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▉                                                                                 | 364/10586 [01:34<44:21,  3.84it/s, v_num=hdbx, train/loss=nan.0, lr=3.44e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0570, max=1.0064, mean=0.0063, std=0.1807
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9360, max=0.9819, mean=0.0063, std=0.1780
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9360, max=0.9819, mean=0.0063, std=0.1780
[CLAP PIPELINE] Sample 0: 6/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▉                                                                                 | 365/10586 [01:35<44:21,  3.84it/s, v_num=hdbx, train/loss=nan.0, lr=3.45e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9306, max=0.9996, mean=-0.0004, std=0.0905
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9526, max=0.9629, mean=-0.0004, std=0.0895
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9526, max=0.9629, mean=-0.0004, std=0.0895
[CLAP PIPELINE] Sample 0: 95900/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▉                                                                                 | 366/10586 [01:42<47:28,  3.59it/s, v_num=hdbx, train/loss=nan.0, lr=3.45e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9952, max=0.9767, mean=-0.0001, std=0.0806
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Sample 0: 140476/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▉                                                                                 | 367/10586 [01:34<44:02,  3.87it/s, v_num=hdbx, train/loss=nan.0, lr=3.45e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=1.0100, mean=-0.0020, std=0.3134
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0137, max=1.0205, mean=-0.0020, std=0.3135
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0137, max=1.0205, mean=-0.0020, std=0.3135
[CLAP PIPELINE] Sample 0: 93495/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▉                                                                                 | 368/10586 [01:35<44:01,  3.87it/s, v_num=hdbx, train/loss=nan.0, lr=3.45e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9951, max=0.9816, mean=0.0001, std=0.1136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9790, mean=0.0001, std=0.1136
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9790, mean=0.0001, std=0.1136
[CLAP PIPELINE] Sample 0: 133661/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▉                                                                                 | 369/10586 [01:35<43:59,  3.87it/s, v_num=hdbx, train/loss=nan.0, lr=3.46e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.5832, max=0.9461, mean=0.0001, std=0.0399
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5830, max=0.7734, mean=0.0001, std=0.0398
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5830, max=0.7734, mean=0.0001, std=0.0398
[CLAP PIPELINE] Sample 0: 83644/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   3%|██▉                                                                                 | 370/10586 [01:35<43:57,  3.87it/s, v_num=hdbx, train/loss=nan.0, lr=3.46e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9874, max=0.9935, mean=-0.0012, std=0.1613
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9878, max=0.9907, mean=-0.0012, std=0.1613
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9878, max=0.9907, mean=-0.0012, std=0.1613
[CLAP PIPELINE] Sample 0: 112254/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|██▉                                                                                 | 371/10586 [01:35<43:55,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.46e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9149, max=0.9979, mean=-0.0002, std=0.1847
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9126, max=0.9985, mean=-0.0002, std=0.1847
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9126, max=0.9985, mean=-0.0002, std=0.1847
[CLAP PIPELINE] Sample 0: 126339/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|██▉                                                                                 | 372/10586 [01:35<43:54,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.46e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9668, max=0.7676, mean=-0.0030, std=0.1058
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9414, max=0.6450, mean=-0.0030, std=0.1058
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9414, max=0.6450, mean=-0.0030, std=0.1058
[CLAP PIPELINE] Sample 0: 800/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|██▉                                                                                 | 373/10586 [01:36<43:53,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.47e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9969, max=0.9633, mean=-0.0029, std=0.0779
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9653, max=0.9424, mean=-0.0029, std=0.0772
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9653, max=0.9424, mean=-0.0029, std=0.0772
[CLAP PIPELINE] Sample 0: 124514/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|██▉                                                                                 | 374/10586 [01:36<43:51,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.47e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9976, max=0.9832, mean=-0.0000, std=0.0619
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0039, max=1.0049, mean=-0.0000, std=0.0618
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0039, max=1.0049, mean=-0.0000, std=0.0618
[CLAP PIPELINE] Sample 0: 139443/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|██▉                                                                                 | 375/10586 [01:36<43:50,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.47e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.9314, mean=-0.0002, std=0.0972
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9316, mean=-0.0002, std=0.0973
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9316, mean=-0.0002, std=0.0973
[CLAP PIPELINE] Sample 0: 79478/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|██▉                                                                                 | 376/10586 [01:36<43:48,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.47e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0007, max=0.9729, mean=-0.0113, std=0.4417
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.9702, mean=-0.0113, std=0.4414
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.9702, mean=-0.0113, std=0.4414
[CLAP PIPELINE] Sample 0: 20050/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|██▉                                                                                 | 377/10586 [01:37<43:49,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.48e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8416, max=1.0099, mean=-0.0003, std=0.0124
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8896, max=0.8569, mean=-0.0003, std=0.0116
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8896, max=0.8569, mean=-0.0003, std=0.0116
[CLAP PIPELINE] Sample 0: 150683/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|██▉                                                                                 | 378/10586 [01:37<43:47,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.48e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9952, max=0.9755, mean=-0.0015, std=0.1295
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9917, max=0.9536, mean=-0.0015, std=0.1295
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9917, max=0.9536, mean=-0.0015, std=0.1295
[CLAP PIPELINE] Sample 0: 94005/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 379/10586 [01:37<43:48,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.48e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0004, max=0.8341, mean=-0.0000, std=0.0354
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9868, max=0.8340, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9868, max=0.8340, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Sample 0: 65626/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 380/10586 [01:37<43:49,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.48e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0018, max=0.9995, mean=-0.0000, std=0.2249
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9746, max=0.9883, mean=-0.0000, std=0.2236
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9746, max=0.9883, mean=-0.0000, std=0.2236
[CLAP PIPELINE] Sample 0: 16256/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 381/10586 [01:38<43:49,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.49e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1685, max=1.2134, mean=0.0000, std=0.0425
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.2793, max=1.2266, mean=0.0000, std=0.0379
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.2793, max=1.2266, mean=0.0000, std=0.0379
[CLAP PIPELINE] Sample 0: 155547/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 382/10586 [01:38<43:51,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.49e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9631, max=0.9891, mean=-0.0001, std=0.0245
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9580, max=0.9731, mean=-0.0001, std=0.0245
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9580, max=0.9731, mean=-0.0001, std=0.0245
[CLAP PIPELINE] Sample 0: 154313/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 383/10586 [01:38<43:51,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.49e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8739, max=0.9986, mean=-0.0000, std=0.0380
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Sample 0: 61484/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 384/10586 [01:39<43:52,  3.88it/s, v_num=hdbx, train/loss=nan.0, lr=3.49e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9456, max=0.9481, mean=-0.0003, std=0.0849
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8320, max=0.8291, mean=-0.0003, std=0.0840
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8320, max=0.8291, mean=-0.0003, std=0.0840
[CLAP PIPELINE] Sample 0: 144094/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                  | 385/10586 [01:39<43:52,  3.87it/s, v_num=hdbx, train/loss=nan.0, lr=3.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.7417, mean=-0.0004, std=0.1304
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9053, max=0.7163, mean=-0.0004, std=0.1290
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9053, max=0.7163, mean=-0.0004, std=0.1290
[CLAP PIPELINE] Sample 0: 111905/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                  | 386/10586 [01:39<43:53,  3.87it/s, v_num=hdbx, train/loss=nan.0, lr=3.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0885, max=1.0489, mean=0.0000, std=0.0271
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8237, max=0.7852, mean=0.0000, std=0.0258
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8237, max=0.7852, mean=0.0000, std=0.0258
[CLAP PIPELINE] Sample 0: 66436/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                  | 387/10586 [01:39<43:54,  3.87it/s, v_num=hdbx, train/loss=nan.0, lr=3.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.5613, mean=-0.0000, std=0.0136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0: 18388/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 388/10586 [01:39<43:41,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9512, max=1.0018, mean=-0.0001, std=0.0323
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8413, max=0.8599, mean=-0.0001, std=0.0254
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8413, max=0.8599, mean=-0.0001, std=0.0254
[CLAP PIPELINE] Sample 0: 156966/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 389/10586 [01:39<43:40,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0024, max=0.6007, mean=-0.0007, std=0.0930
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0: 133106/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 390/10586 [01:40<43:40,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9959, max=1.0002, mean=-0.0000, std=0.1500
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9961, max=0.9976, mean=-0.0000, std=0.1500
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9961, max=0.9976, mean=-0.0000, std=0.1500
[CLAP PIPELINE] Sample 0: 69152/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 391/10586 [01:40<43:40,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9271, max=0.9994, mean=-0.0000, std=0.1637
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9277, max=1.0000, mean=-0.0000, std=0.1638
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9277, max=1.0000, mean=-0.0000, std=0.1638
[CLAP PIPELINE] Sample 0: 83228/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 392/10586 [01:40<43:40,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0037, mean=-0.0005, std=0.0341
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8706, max=0.9971, mean=-0.0005, std=0.0319
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8706, max=0.9971, mean=-0.0005, std=0.0319
[CLAP PIPELINE] Sample 0: 141093/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███                                                                                 | 393/10586 [01:41<43:40,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9845, max=0.9903, mean=0.0000, std=0.0162
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9717, max=0.9814, mean=0.0000, std=0.0160
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9717, max=0.9814, mean=0.0000, std=0.0160
[CLAP PIPELINE] Sample 0: 152397/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 394/10586 [01:41<43:39,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0010, max=0.9703, mean=-0.0000, std=0.0720
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.9653, mean=-0.0000, std=0.0720
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.9653, mean=-0.0000, std=0.0720
[CLAP PIPELINE] Sample 0: 113177/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 395/10586 [01:41<43:38,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0053, max=0.8300, mean=-0.0000, std=0.0187
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9824, max=0.8228, mean=-0.0000, std=0.0188
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9824, max=0.8228, mean=-0.0000, std=0.0188
[CLAP PIPELINE] Sample 0: 134619/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 396/10586 [01:41<43:39,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9150, max=0.9981, mean=0.0019, std=0.0743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9150, max=0.9985, mean=0.0019, std=0.0743
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=0.9985, mean=0.0019, std=0.0743
[CLAP PIPELINE] Sample 0: 95288/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 397/10586 [01:42<43:39,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0097, max=0.9844, mean=0.0002, std=0.0780
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0264, max=0.9795, mean=0.0002, std=0.0779
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0264, max=0.9795, mean=0.0002, std=0.0779
[CLAP PIPELINE] Sample 0: 53226/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 398/10586 [01:42<43:38,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=0.8610, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0: 148709/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 399/10586 [01:42<43:37,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9474, max=1.0005, mean=-0.0014, std=0.0640
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9453, max=0.9980, mean=-0.0014, std=0.0640
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9453, max=0.9980, mean=-0.0014, std=0.0640
[CLAP PIPELINE] Sample 0: 112816/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 400/10586 [01:42<43:36,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9950, mean=0.0001, std=0.2440
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9956, mean=0.0001, std=0.2441
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9956, mean=0.0001, std=0.2441
[CLAP PIPELINE] Sample 0: 93115/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 401/10586 [01:43<43:38,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9535, max=0.9998, mean=-0.0000, std=0.1022
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9526, max=1.0010, mean=-0.0000, std=0.1022
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9526, max=1.0010, mean=-0.0000, std=0.1022
[CLAP PIPELINE] Sample 0: 132260/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 402/10586 [01:43<43:39,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6593, max=0.6695, mean=0.0131, std=0.0694
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6108, max=0.6440, mean=0.0131, std=0.0693
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6108, max=0.6440, mean=0.0131, std=0.0693
[CLAP PIPELINE] Sample 0: 95673/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 403/10586 [01:43<43:37,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8297, max=1.0162, mean=-0.0000, std=0.0370
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Sample 0: 64456/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 404/10586 [01:43<43:37,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9970, max=1.0000, mean=-0.0013, std=0.1053
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=0.9937, mean=-0.0013, std=0.1053
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=0.9937, mean=-0.0013, std=0.1053
[CLAP PIPELINE] Sample 0: 109096/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 405/10586 [01:44<43:37,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0: 150811/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 406/10586 [01:44<43:38,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9876, max=0.9875, mean=-0.0000, std=0.5074
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9966, max=0.9971, mean=-0.0000, std=0.4985
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9966, max=0.9971, mean=-0.0000, std=0.4985
[CLAP PIPELINE] Sample 0: 87997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 407/10586 [01:44<43:36,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=0.8610, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0: 148709/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 408/10586 [01:44<43:37,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.8940, mean=-0.0010, std=0.0770
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9297, max=0.8711, mean=-0.0010, std=0.0750
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9297, max=0.8711, mean=-0.0010, std=0.0750
[CLAP PIPELINE] Sample 0: 98510/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▏                                                                                | 409/10586 [01:45<43:36,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9689, max=0.9981, mean=-0.0001, std=0.1220
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9673, max=0.9932, mean=-0.0001, std=0.1221
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9673, max=0.9932, mean=-0.0001, std=0.1221
[CLAP PIPELINE] Sample 0: 47746/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 410/10586 [01:44<43:24,  3.91it/s, v_num=hdbx, train/loss=nan.0, lr=3.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.7122, mean=0.0001, std=0.0413
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9814, max=0.7056, mean=0.0001, std=0.0413
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9814, max=0.7056, mean=0.0001, std=0.0413
[CLAP PIPELINE] Sample 0: 63882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 411/10586 [01:45<43:26,  3.90it/s, v_num=hdbx, train/loss=nan.0, lr=3.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0061, max=0.9453, mean=-0.0007, std=0.0940
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9258, max=0.9155, mean=-0.0007, std=0.0937
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9258, max=0.9155, mean=-0.0007, std=0.0937
[CLAP PIPELINE] Sample 0: 129626/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 412/10586 [01:45<43:26,  3.90it/s, v_num=hdbx, train/loss=nan.0, lr=3.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9966, max=1.0236, mean=-0.0004, std=0.1075
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8091, max=0.8008, mean=-0.0004, std=0.0959
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8091, max=0.8008, mean=-0.0004, std=0.0959
[CLAP PIPELINE] Sample 0: 68168/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 413/10586 [01:45<43:29,  3.90it/s, v_num=hdbx, train/loss=nan.0, lr=3.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9370, max=1.0003, mean=-0.0000, std=0.0302
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7837, max=0.6333, mean=-0.0000, std=0.0228
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7837, max=0.6333, mean=-0.0000, std=0.0228
[CLAP PIPELINE] Sample 0: 133735/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 414/10586 [01:46<43:31,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8996, max=0.9494, mean=-0.0004, std=0.0613
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6411, max=0.6016, mean=-0.0004, std=0.0600
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6411, max=0.6016, mean=-0.0004, std=0.0600
[CLAP PIPELINE] Sample 0: 33174/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 415/10586 [01:46<43:33,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9216, max=0.9771, mean=-0.0000, std=0.0947
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8647, max=0.9453, mean=-0.0000, std=0.0916
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8647, max=0.9453, mean=-0.0000, std=0.0916
[CLAP PIPELINE] Sample 0: 77930/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 416/10586 [01:46<43:35,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9954, max=0.9578, mean=-0.0013, std=0.0753
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9922, max=0.9604, mean=-0.0013, std=0.0754
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9922, max=0.9604, mean=-0.0013, std=0.0754
[CLAP PIPELINE] Sample 0: 105618/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 417/10586 [01:47<43:35,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8310, max=0.9703, mean=0.0000, std=0.1648
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8350, max=0.9697, mean=0.0000, std=0.1648
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8350, max=0.9697, mean=0.0000, std=0.1648
[CLAP PIPELINE] Sample 0: 1934/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 418/10586 [01:47<43:34,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9802, max=1.0484, mean=0.0006, std=0.2153
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9399, max=0.9727, mean=0.0006, std=0.2085
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9399, max=0.9727, mean=0.0006, std=0.2085
[CLAP PIPELINE] Sample 0: 13341/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 419/10586 [01:47<43:34,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7116, max=0.9945, mean=-0.0000, std=0.0276
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6792, max=0.9692, mean=-0.0000, std=0.0276
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6792, max=0.9692, mean=-0.0000, std=0.0276
[CLAP PIPELINE] Sample 0: 88588/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 420/10586 [01:48<43:35,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9994, mean=-0.0028, std=0.1001
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9595, max=0.9692, mean=-0.0028, std=0.1001
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9595, max=0.9692, mean=-0.0028, std=0.1001
[CLAP PIPELINE] Sample 0: 127126/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 421/10586 [01:48<43:35,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=0.9456, mean=-0.0006, std=0.1208
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9902, max=0.9458, mean=-0.0006, std=0.1208
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9902, max=0.9458, mean=-0.0006, std=0.1208
[CLAP PIPELINE] Sample 0: 122108/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▎                                                                                | 422/10586 [01:48<43:34,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9904, max=0.9272, mean=0.0027, std=0.1281
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9814, max=0.9062, mean=0.0027, std=0.1281
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9814, max=0.9062, mean=0.0027, std=0.1281
[CLAP PIPELINE] Sample 0: 95003/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                 | 423/10586 [01:48<43:32,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0096, max=0.9659, mean=-0.0000, std=0.1041
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9419, max=0.7036, mean=-0.0000, std=0.1002
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9419, max=0.7036, mean=-0.0000, std=0.1002
[CLAP PIPELINE] Sample 0: 64231/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                 | 424/10586 [01:48<43:31,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9950, mean=0.0001, std=0.2440
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9956, mean=0.0001, std=0.2441
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9956, mean=0.0001, std=0.2441
[CLAP PIPELINE] Sample 0: 93115/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                 | 425/10586 [01:49<43:31,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0035, max=1.0585, mean=-0.0000, std=0.0444
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0537, max=0.8989, mean=-0.0000, std=0.0439
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0537, max=0.8989, mean=-0.0000, std=0.0439
[CLAP PIPELINE] Sample 0: 111862/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                 | 426/10586 [01:49<43:30,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9880, max=1.0216, mean=0.0000, std=0.1027
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0283, max=1.0322, mean=0.0000, std=0.1027
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0283, max=1.0322, mean=0.0000, std=0.1027
[CLAP PIPELINE] Sample 0: 110215/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 427/10586 [01:49<43:30,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0461, max=1.0268, mean=-0.0000, std=0.0511
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0977, max=1.1162, mean=-0.0000, std=0.0461
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0977, max=1.1162, mean=-0.0000, std=0.0461
[CLAP PIPELINE] Sample 0: 109767/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 428/10586 [01:49<43:29,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7296, max=1.0020, mean=-0.0000, std=0.1396
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7324, max=0.9863, mean=-0.0000, std=0.1396
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7324, max=0.9863, mean=-0.0000, std=0.1396
[CLAP PIPELINE] Sample 0: 44167/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 429/10586 [01:50<43:27,  3.89it/s, v_num=hdbx, train/loss=nan.0, lr=3.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9860, max=0.6366, mean=-0.0001, std=0.0639
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Sample 0: 91800/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 430/10586 [01:57<46:09,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6951, max=0.9978, mean=0.0000, std=0.1569
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6909, max=0.9673, mean=0.0000, std=0.1570
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6909, max=0.9673, mean=0.0000, std=0.1570
[CLAP PIPELINE] Sample 0: 69125/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 431/10586 [01:57<46:11,  3.66it/s, v_num=hdbx, train/loss=nan.0, lr=3.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8864, max=1.0000, mean=0.0002, std=0.0629
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8867, max=1.0000, mean=0.0002, std=0.0629
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8867, max=1.0000, mean=0.0002, std=0.0629
[CLAP PIPELINE] Sample 0: 112030/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 432/10586 [01:57<46:09,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0326, max=0.9009, mean=-0.0001, std=0.0352
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7246, max=0.7437, mean=-0.0001, std=0.0337
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7246, max=0.7437, mean=-0.0001, std=0.0337
[CLAP PIPELINE] Sample 0: 12072/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 433/10586 [01:58<46:07,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9899, max=0.9946, mean=-0.0022, std=0.1272
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9858, max=0.9927, mean=-0.0022, std=0.1272
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9858, max=0.9927, mean=-0.0022, std=0.1272
[CLAP PIPELINE] Sample 0: 109744/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 434/10586 [01:58<46:06,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1897, max=1.2273, mean=-0.0000, std=0.1331
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9194, max=1.0537, mean=-0.0000, std=0.1292
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9194, max=1.0537, mean=-0.0000, std=0.1292
[CLAP PIPELINE] Sample 0: 65535/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 435/10586 [01:58<46:06,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0051, max=1.0069, mean=-0.0000, std=0.0850
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9321, max=0.9971, mean=-0.0000, std=0.0850
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9321, max=0.9971, mean=-0.0000, std=0.0850
[CLAP PIPELINE] Sample 0: 35042/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 436/10586 [01:58<46:04,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7375, max=0.9280, mean=-0.0000, std=0.0428
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5459, max=0.5586, mean=-0.0000, std=0.0302
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5459, max=0.5586, mean=-0.0000, std=0.0302
[CLAP PIPELINE] Sample 0: 35991/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 437/10586 [01:59<46:05,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9150, max=0.9981, mean=0.0019, std=0.0743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9150, max=0.9985, mean=0.0019, std=0.0743
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=0.9985, mean=0.0019, std=0.0743
[CLAP PIPELINE] Sample 0: 95288/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 438/10586 [01:59<46:05,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9609, max=1.0002, mean=0.0007, std=0.1751
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9438, max=0.9937, mean=0.0007, std=0.1752
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9438, max=0.9937, mean=0.0007, std=0.1752
[CLAP PIPELINE] Sample 0: 99469/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 439/10586 [01:59<46:04,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0018, max=0.9247, mean=-0.0000, std=0.4909
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0059, max=0.9419, mean=-0.0000, std=0.4905
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0059, max=0.9419, mean=-0.0000, std=0.4905
[CLAP PIPELINE] Sample 0: 17289/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 440/10586 [01:59<46:03,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9149, max=0.9999, mean=-0.0012, std=0.2173
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9155, max=0.9966, mean=-0.0012, std=0.2174
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9155, max=0.9966, mean=-0.0012, std=0.2174
[CLAP PIPELINE] Sample 0: 15/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▍                                                                                | 441/10586 [02:00<46:06,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=1.0001, mean=-0.0000, std=0.7321
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=1.0010, mean=-0.0000, std=0.7324
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=1.0010, mean=-0.0000, std=0.7324
[CLAP PIPELINE] Sample 0: 48036/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 442/10586 [02:00<46:05,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=1.0001, mean=-0.0000, std=0.7321
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=1.0010, mean=-0.0000, std=0.7324
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=1.0010, mean=-0.0000, std=0.7324
[CLAP PIPELINE] Sample 0: 48036/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 443/10586 [02:00<46:03,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.7863, mean=-0.0000, std=0.0185
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.7861, mean=-0.0000, std=0.0185
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.7861, mean=-0.0000, std=0.0185
[CLAP PIPELINE] Sample 0: 112731/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 444/10586 [02:00<46:01,  3.67it/s, v_num=hdbx, train/loss=nan.0, lr=3.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0382, max=0.9402, mean=-0.0005, std=0.0289
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9604, max=0.8018, mean=-0.0005, std=0.0267
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9604, max=0.8018, mean=-0.0005, std=0.0267
[CLAP PIPELINE] Sample 0: 139370/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 445/10586 [02:01<45:59,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8420, max=0.7613, mean=0.0001, std=0.0669
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7979, max=0.6338, mean=0.0001, std=0.0299
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7979, max=0.6338, mean=0.0001, std=0.0299
[CLAP PIPELINE] Sample 0: 25482/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 446/10586 [02:01<45:58,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9793, max=0.9997, mean=0.0003, std=0.1813
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9644, max=0.9917, mean=0.0003, std=0.1813
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9644, max=0.9917, mean=0.0003, std=0.1813
[CLAP PIPELINE] Sample 0: 69162/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 447/10586 [02:01<45:58,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9563, max=1.0000, mean=-0.0006, std=0.0685
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9741, max=0.9819, mean=-0.0006, std=0.0685
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9741, max=0.9819, mean=-0.0006, std=0.0685
[CLAP PIPELINE] Sample 0: 133060/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 448/10586 [02:01<45:56,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8687, max=0.9427, mean=-0.0000, std=0.0148
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4741, max=0.4734, mean=-0.0000, std=0.0094
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4741, max=0.4734, mean=-0.0000, std=0.0094
[CLAP PIPELINE] Sample 0: 143717/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 449/10586 [02:02<45:55,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6316, max=0.9994, mean=0.0004, std=0.1151
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6289, max=0.9985, mean=0.0004, std=0.1151
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6289, max=0.9985, mean=0.0004, std=0.1151
[CLAP PIPELINE] Sample 0: 137931/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 450/10586 [02:02<45:53,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.8940, mean=-0.0010, std=0.0770
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9297, max=0.8711, mean=-0.0010, std=0.0750
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9297, max=0.8711, mean=-0.0010, std=0.0750
[CLAP PIPELINE] Sample 0: 98510/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 451/10586 [02:02<45:54,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1091, max=1.1092, mean=-0.0000, std=0.0830
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.1162, max=1.1260, mean=-0.0000, std=0.0690
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.1162, max=1.1260, mean=-0.0000, std=0.0690
[CLAP PIPELINE] Sample 0: 68539/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 452/10586 [02:02<45:54,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9178, max=0.9936, mean=0.1238, std=0.4039
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=0.9927, mean=0.1239, std=0.4033
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=0.9927, mean=0.1239, std=0.4033
[CLAP PIPELINE] Sample 0: 22882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 453/10586 [02:03<45:53,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7767, max=0.9819, mean=-0.0001, std=0.1435
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7612, max=0.9814, mean=-0.0001, std=0.1436
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7612, max=0.9814, mean=-0.0001, std=0.1436
[CLAP PIPELINE] Sample 0: 39910/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 454/10586 [02:03<45:53,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0: 150811/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 455/10586 [02:03<45:52,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9724, max=1.0004, mean=-0.0000, std=0.1376
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9722, max=1.0000, mean=-0.0000, std=0.1376
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9722, max=1.0000, mean=-0.0000, std=0.1376
[CLAP PIPELINE] Sample 0: 67796/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▌                                                                                | 456/10586 [02:04<45:55,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0097, max=0.9844, mean=0.0002, std=0.0780
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0264, max=0.9795, mean=0.0002, std=0.0779
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0264, max=0.9795, mean=0.0002, std=0.0779
[CLAP PIPELINE] Sample 0: 53226/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 457/10586 [02:04<45:52,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9968, max=0.7898, mean=-0.0000, std=0.0482
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.7783, mean=-0.0000, std=0.0482
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.7783, mean=-0.0000, std=0.0482
[CLAP PIPELINE] Sample 0: 96554/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 458/10586 [02:04<45:51,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=0.5959, mean=-0.0000, std=0.0308
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9893, max=0.5952, mean=-0.0000, std=0.0308
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9893, max=0.5952, mean=-0.0000, std=0.0308
[CLAP PIPELINE] Sample 0: 149682/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 459/10586 [02:04<45:49,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8896, max=0.9998, mean=-0.0000, std=0.1505
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8853, max=0.9961, mean=-0.0000, std=0.1505
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8853, max=0.9961, mean=-0.0000, std=0.1505
[CLAP PIPELINE] Sample 0: 86549/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 460/10586 [02:04<45:48,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9937, max=1.0002, mean=0.0001, std=0.1020
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9941, max=1.0000, mean=0.0001, std=0.1021
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9941, max=1.0000, mean=0.0001, std=0.1021
[CLAP PIPELINE] Sample 0: 112296/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 461/10586 [02:05<45:48,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9899, max=0.9946, mean=-0.0022, std=0.1272
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9858, max=0.9927, mean=-0.0022, std=0.1272
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9858, max=0.9927, mean=-0.0022, std=0.1272
[CLAP PIPELINE] Sample 0: 109744/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                 | 462/10586 [02:05<45:47,  3.68it/s, v_num=hdbx, train/loss=nan.0, lr=3.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9095, max=0.8173, mean=-0.1639, std=0.1775
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5010, max=0.5264, mean=-0.1641, std=0.1622
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5010, max=0.5264, mean=-0.1641, std=0.1622
[CLAP PIPELINE] Sample 0: 22674/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                 | 463/10586 [02:05<45:46,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9425, max=0.9968, mean=-0.0004, std=0.2565
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9399, max=0.9966, mean=-0.0004, std=0.2563
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9399, max=0.9966, mean=-0.0004, std=0.2563
[CLAP PIPELINE] Sample 0: 15683/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                 | 464/10586 [02:05<45:45,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0027, max=0.9153, mean=-0.0009, std=0.1298
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9131, mean=-0.0009, std=0.1298
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.9131, mean=-0.0009, std=0.1298
[CLAP PIPELINE] Sample 0: 126186/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 465/10586 [02:06<45:44,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9809, max=0.9995, mean=0.0000, std=0.2168
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9902, max=1.0068, mean=0.0000, std=0.2168
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9902, max=1.0068, mean=0.0000, std=0.2168
[CLAP PIPELINE] Sample 0: 63997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 466/10586 [02:06<45:44,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9216, max=0.9771, mean=-0.0000, std=0.0947
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8647, max=0.9453, mean=-0.0000, std=0.0916
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8647, max=0.9453, mean=-0.0000, std=0.0916
[CLAP PIPELINE] Sample 0: 77930/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 467/10586 [02:06<45:42,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7779, max=1.0014, mean=0.0000, std=0.0140
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7236, max=1.0020, mean=0.0000, std=0.0140
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7236, max=1.0020, mean=0.0000, std=0.0140
[CLAP PIPELINE] Sample 0: 146991/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 468/10586 [02:06<45:42,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.7420, mean=-0.0188, std=0.0789
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9932, max=0.7388, mean=-0.0188, std=0.0789
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9932, max=0.7388, mean=-0.0188, std=0.0789
[CLAP PIPELINE] Sample 0: 116805/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 469/10586 [02:07<45:42,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.9046, mean=0.0002, std=0.2047
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9038, mean=0.0002, std=0.2047
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.9038, mean=0.0002, std=0.2047
[CLAP PIPELINE] Sample 0: 122124/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 470/10586 [02:07<45:41,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.5613, mean=-0.0000, std=0.0136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0: 18388/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 471/10586 [02:07<45:40,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7342, max=0.7987, mean=-0.0000, std=0.0520
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5332, max=0.7290, mean=-0.0000, std=0.0467
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5332, max=0.7290, mean=-0.0000, std=0.0467
[CLAP PIPELINE] Sample 0: 80145/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▋                                                                                | 472/10586 [02:07<45:41,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9341, max=0.9025, mean=-0.0009, std=0.0382
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8545, max=0.8267, mean=-0.0009, std=0.0369
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8545, max=0.8267, mean=-0.0009, std=0.0369
[CLAP PIPELINE] Sample 0: 107183/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▊                                                                                | 473/10586 [02:08<45:38,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9751, max=0.9976, mean=-0.0001, std=0.2598
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Sample 0: 25/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▊                                                                                | 474/10586 [02:08<45:38,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0052, max=0.9255, mean=0.0000, std=0.1698
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0049, max=0.9258, mean=0.0000, std=0.1698
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0049, max=0.9258, mean=0.0000, std=0.1698
[CLAP PIPELINE] Sample 0: 72395/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▊                                                                                | 475/10586 [02:08<45:36,  3.69it/s, v_num=hdbx, train/loss=nan.0, lr=3.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.9986, mean=0.0001, std=0.3426
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9990, mean=0.0001, std=0.3428
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9990, mean=0.0001, std=0.3428
[CLAP PIPELINE] Sample 0: 111848/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   4%|███▊                                                                                | 476/10586 [02:08<45:34,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9166, max=0.9913, mean=0.0000, std=0.1800
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9175, max=0.9834, mean=0.0000, std=0.1801
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9175, max=0.9834, mean=0.0000, std=0.1801
[CLAP PIPELINE] Sample 0: 16240/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 477/10586 [02:08<45:33,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6688, max=0.7300, mean=-0.0000, std=0.0359
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6216, max=0.6060, mean=-0.0000, std=0.0346
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6216, max=0.6060, mean=-0.0000, std=0.0346
[CLAP PIPELINE] Sample 0: 127682/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 478/10586 [02:09<45:33,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0118, max=1.0184, mean=0.0002, std=0.1353
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0156, max=1.0693, mean=0.0002, std=0.1348
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0156, max=1.0693, mean=0.0002, std=0.1348
[CLAP PIPELINE] Sample 0: 30825/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 479/10586 [02:09<45:32,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.9979, mean=-0.0000, std=0.2585
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9927, max=0.9941, mean=-0.0000, std=0.2585
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9927, max=0.9941, mean=-0.0000, std=0.2585
[CLAP PIPELINE] Sample 0: 58451/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 480/10586 [02:09<45:31,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9951, max=0.9816, mean=0.0001, std=0.1136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9790, mean=0.0001, std=0.1136
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9790, mean=0.0001, std=0.1136
[CLAP PIPELINE] Sample 0: 133661/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 481/10586 [02:10<45:31,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9868, max=0.8880, mean=0.0000, std=0.1075
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9766, max=0.8892, mean=0.0000, std=0.1067
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9766, max=0.8892, mean=0.0000, std=0.1067
[CLAP PIPELINE] Sample 0: 733/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 482/10586 [02:10<45:30,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9204, mean=-0.0006, std=0.1068
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9995, max=0.9189, mean=-0.0006, std=0.1069
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9995, max=0.9189, mean=-0.0006, std=0.1069
[CLAP PIPELINE] Sample 0: 16644/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 483/10586 [02:10<45:29,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.5584, max=1.0044, mean=-0.0000, std=0.0272
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5225, max=1.0039, mean=-0.0000, std=0.0212
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5225, max=1.0039, mean=-0.0000, std=0.0212
[CLAP PIPELINE] Sample 0: 94999/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 484/10586 [02:10<45:28,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9809, max=0.9995, mean=0.0000, std=0.2168
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9902, max=1.0068, mean=0.0000, std=0.2168
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9902, max=1.0068, mean=0.0000, std=0.2168
[CLAP PIPELINE] Sample 0: 63997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 485/10586 [02:11<45:29,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.8940, mean=-0.0010, std=0.0770
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9297, max=0.8711, mean=-0.0010, std=0.0750
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9297, max=0.8711, mean=-0.0010, std=0.0750
[CLAP PIPELINE] Sample 0: 98510/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 486/10586 [02:11<45:28,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9991, mean=-0.0000, std=0.2045
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9912, max=0.9946, mean=-0.0000, std=0.2046
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9912, max=0.9946, mean=-0.0000, std=0.2046
[CLAP PIPELINE] Sample 0: 45695/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 487/10586 [02:11<45:29,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.7122, mean=0.0001, std=0.0413
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9814, max=0.7056, mean=0.0001, std=0.0413
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9814, max=0.7056, mean=0.0001, std=0.0413
[CLAP PIPELINE] Sample 0: 63882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▊                                                                                | 488/10586 [02:11<45:30,  3.70it/s, v_num=hdbx, train/loss=nan.0, lr=3.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=1.0032, mean=-0.0030, std=0.1216
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9224, max=0.9990, mean=-0.0030, std=0.1213
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9224, max=0.9990, mean=-0.0030, std=0.1213
[CLAP PIPELINE] Sample 0: 123215/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 489/10586 [02:03<42:31,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=3.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0030, max=1.0159, mean=0.0009, std=0.4255
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0264, max=1.0391, mean=0.0009, std=0.4180
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0264, max=1.0391, mean=0.0009, std=0.4180
[CLAP PIPELINE] Sample 0: 88003/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 490/10586 [02:03<42:32,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=3.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9204, max=0.9984, mean=-0.0000, std=0.0979
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=0.9570, mean=-0.0000, std=0.0979
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=0.9570, mean=-0.0000, std=0.0979
[CLAP PIPELINE] Sample 0: 73735/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 491/10586 [02:04<42:33,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8612, max=1.0035, mean=0.0002, std=0.0919
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8564, max=0.9536, mean=0.0002, std=0.0919
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8564, max=0.9536, mean=0.0002, std=0.0919
[CLAP PIPELINE] Sample 0: 96466/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 492/10586 [02:04<42:33,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0037, mean=-0.0005, std=0.0341
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8706, max=0.9971, mean=-0.0005, std=0.0319
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8706, max=0.9971, mean=-0.0005, std=0.0319
[CLAP PIPELINE] Sample 0: 141093/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 493/10586 [02:04<42:35,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9088, max=1.0000, mean=-0.0016, std=0.1978
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9092, max=1.0010, mean=-0.0016, std=0.1980
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9092, max=1.0010, mean=-0.0016, std=0.1980
[CLAP PIPELINE] Sample 0: 95861/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 494/10586 [02:04<42:32,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8860, max=0.8689, mean=-0.0001, std=0.1662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Sample 0: 111849/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 495/10586 [02:05<42:32,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9317, max=0.9996, mean=0.0001, std=0.2213
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9321, max=0.9990, mean=0.0001, std=0.2214
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9321, max=0.9990, mean=0.0001, std=0.2214
[CLAP PIPELINE] Sample 0: 31515/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 496/10586 [02:05<42:31,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9802, max=1.0484, mean=0.0006, std=0.2153
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9399, max=0.9727, mean=0.0006, std=0.2085
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9399, max=0.9727, mean=0.0006, std=0.2085
[CLAP PIPELINE] Sample 0: 13341/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 497/10586 [02:05<42:31,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1497, max=1.2149, mean=-0.0000, std=0.0694
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8442, max=0.9546, mean=-0.0000, std=0.0503
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8442, max=0.9546, mean=-0.0000, std=0.0503
[CLAP PIPELINE] Sample 0: 87007/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 498/10586 [02:05<42:30,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=3.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9766, max=1.0027, mean=-0.0002, std=0.2051
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0107, max=1.0078, mean=-0.0002, std=0.2051
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0107, max=1.0078, mean=-0.0002, std=0.2051
[CLAP PIPELINE] Sample 0: 114878/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 499/10586 [02:06<42:31,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.4251, max=0.9723, mean=-0.0000, std=0.0265
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.2510, max=0.3074, mean=-0.0000, std=0.0263
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.2510, max=0.3074, mean=-0.0000, std=0.0263
[CLAP PIPELINE] Sample 0: 16/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                 | 500/10586 [02:06<42:31,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.9713, mean=0.0001, std=0.2487
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=0.9683, mean=0.0001, std=0.2487
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=0.9683, mean=0.0001, std=0.2487
[CLAP PIPELINE] Sample 0: 48109/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                 | 501/10586 [02:06<42:31,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9965, mean=-0.0004, std=0.1406
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0: 27046/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                 | 502/10586 [02:07<42:31,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8724, max=0.9966, mean=-0.0000, std=0.0277
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8608, max=0.9785, mean=-0.0000, std=0.0276
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8608, max=0.9785, mean=-0.0000, std=0.0276
[CLAP PIPELINE] Sample 0: 93241/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                 | 503/10586 [02:07<42:33,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0068, max=0.9876, mean=-0.0000, std=0.1221
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0068, max=0.9868, mean=-0.0000, std=0.1221
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0068, max=0.9868, mean=-0.0000, std=0.1221
[CLAP PIPELINE] Sample 0: 115119/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|███▉                                                                                | 504/10586 [02:07<42:34,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9204, mean=-0.0006, std=0.1068
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9995, max=0.9189, mean=-0.0006, std=0.1069
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9995, max=0.9189, mean=-0.0006, std=0.1069
[CLAP PIPELINE] Sample 0: 16644/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 505/10586 [02:07<42:32,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8310, max=0.9703, mean=0.0000, std=0.1648
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8350, max=0.9697, mean=0.0000, std=0.1648
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8350, max=0.9697, mean=0.0000, std=0.1648
[CLAP PIPELINE] Sample 0: 1934/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 506/10586 [02:08<42:31,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0097, max=1.0043, mean=0.0000, std=0.0573
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0127, max=1.0068, mean=0.0000, std=0.0574
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0127, max=1.0068, mean=0.0000, std=0.0574
[CLAP PIPELINE] Sample 0: 49207/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 507/10586 [02:08<42:30,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9983, mean=0.0005, std=0.1294
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Sample 0: 140620/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 508/10586 [02:08<42:30,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9989, max=0.9901, mean=-0.0000, std=0.0872
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9888, max=0.9692, mean=-0.0000, std=0.0872
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9888, max=0.9692, mean=-0.0000, std=0.0872
[CLAP PIPELINE] Sample 0: 154271/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 509/10586 [02:08<42:29,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8758, max=1.0003, mean=-0.0056, std=0.3625
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8740, max=0.9980, mean=-0.0056, std=0.3628
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8740, max=0.9980, mean=-0.0056, std=0.3628
[CLAP PIPELINE] Sample 0: 46433/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 510/10586 [02:09<42:30,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0258, max=0.8365, mean=-0.0000, std=0.1814
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0439, max=0.8457, mean=-0.0000, std=0.1808
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0439, max=0.8457, mean=-0.0000, std=0.1808
[CLAP PIPELINE] Sample 0: 18076/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 511/10586 [02:09<42:29,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9993, max=0.9803, mean=0.0000, std=0.1885
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9785, max=0.9771, mean=0.0000, std=0.1885
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9785, max=0.9771, mean=0.0000, std=0.1885
[CLAP PIPELINE] Sample 0: 45493/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 512/10586 [02:09<42:27,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9959, mean=-0.0000, std=0.0515
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9956, mean=-0.0000, std=0.0515
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.9956, mean=-0.0000, std=0.0515
[CLAP PIPELINE] Sample 0: 158259/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 513/10586 [02:09<42:26,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=3.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9517, max=0.9826, mean=-0.0000, std=0.0694
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7388, max=0.6724, mean=-0.0000, std=0.0670
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7388, max=0.6724, mean=-0.0000, std=0.0670
[CLAP PIPELINE] Sample 0: 47569/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 514/10586 [02:10<42:30,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8500, max=1.0037, mean=-0.0000, std=0.1339
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8159, max=1.0000, mean=-0.0000, std=0.1339
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8159, max=1.0000, mean=-0.0000, std=0.1339
[CLAP PIPELINE] Sample 0: 1/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1829, max=0.1224, mean=-0.0026, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap 1.0000
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 515/10586 [02:17<44:43,  3.75it/s, v_num=hdbx, train/loss=nan.0, lr=3.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9872, mean=0.0001, std=0.5035
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0186, max=1.0156, mean=0.0001, std=0.5015
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0186, max=1.0156, mean=0.0001, std=0.5015
[CLAP PIPELINE] Sample 0: 48080/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 516/10586 [02:17<44:42,  3.75it/s, v_num=hdbx, train/loss=nan.0, lr=3.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0024, max=0.6007, mean=-0.0007, std=0.0930
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0: 133106/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 517/10586 [02:17<44:41,  3.76it/s, v_num=hdbx, train/loss=nan.0, lr=3.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0: 47237/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 518/10586 [02:17<44:39,  3.76it/s, v_num=hdbx, train/loss=nan.0, lr=3.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9926, max=0.9998, mean=0.0003, std=0.1458
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9932, max=1.0010, mean=0.0003, std=0.1459
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9932, max=1.0010, mean=0.0003, std=0.1459
[CLAP PIPELINE] Sample 0: 26860/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████                                                                                | 519/10586 [02:18<44:38,  3.76it/s, v_num=hdbx, train/loss=nan.0, lr=3.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9617, max=0.9935, mean=-0.0000, std=0.0818
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9619, max=0.9893, mean=-0.0000, std=0.0818
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9619, max=0.9893, mean=-0.0000, std=0.0818
[CLAP PIPELINE] Sample 0: 146182/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 520/10586 [02:18<44:37,  3.76it/s, v_num=hdbx, train/loss=nan.0, lr=3.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9966, max=1.0236, mean=-0.0004, std=0.1075
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8091, max=0.8008, mean=-0.0004, std=0.0959
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8091, max=0.8008, mean=-0.0004, std=0.0959
[CLAP PIPELINE] Sample 0: 68168/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 521/10586 [02:18<44:37,  3.76it/s, v_num=hdbx, train/loss=nan.0, lr=3.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8165, max=0.7935, mean=-0.0002, std=0.1749
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6182, max=0.5903, mean=-0.0002, std=0.1547
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6182, max=0.5903, mean=-0.0002, std=0.1547
[CLAP PIPELINE] Sample 0: 47653/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 522/10586 [02:18<44:37,  3.76it/s, v_num=hdbx, train/loss=nan.0, lr=3.85e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7930, max=0.9317, mean=-0.0000, std=0.0726
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7744, max=0.9214, mean=-0.0000, std=0.0726
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7744, max=0.9214, mean=-0.0000, std=0.0726
[CLAP PIPELINE] Sample 0: 110221/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 523/10586 [02:19<44:37,  3.76it/s, v_num=hdbx, train/loss=nan.0, lr=3.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0170, max=0.7277, mean=-0.0000, std=0.0304
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8125, max=0.6289, mean=-0.0000, std=0.0293
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8125, max=0.6289, mean=-0.0000, std=0.0293
[CLAP PIPELINE] Sample 0: 138935/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 524/10586 [02:19<44:38,  3.76it/s, v_num=hdbx, train/loss=nan.0, lr=3.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.5452, max=1.0035, mean=0.0012, std=0.1025
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5444, max=1.0029, mean=0.0012, std=0.1025
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5444, max=1.0029, mean=0.0012, std=0.1025
[CLAP PIPELINE] Sample 0: 119345/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 525/10586 [02:19<44:39,  3.75it/s, v_num=hdbx, train/loss=nan.0, lr=3.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0136, max=0.8102, mean=0.0000, std=0.0563
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9956, max=0.8066, mean=0.0000, std=0.0558
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9956, max=0.8066, mean=0.0000, std=0.0558
[CLAP PIPELINE] Sample 0: 118218/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 526/10586 [02:20<44:38,  3.76it/s, v_num=hdbx, train/loss=nan.0, lr=3.86e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0003, max=0.9558, mean=-0.0000, std=0.0667
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9551, mean=-0.0000, std=0.0668
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9551, mean=-0.0000, std=0.0668
[CLAP PIPELINE] Sample 0: 82199/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 527/10586 [02:20<44:39,  3.75it/s, v_num=hdbx, train/loss=nan.0, lr=3.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9917, max=0.9708, mean=0.0006, std=0.0593
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9722, max=0.9492, mean=0.0006, std=0.0593
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9722, max=0.9492, mean=0.0006, std=0.0593
[CLAP PIPELINE] Sample 0: 144685/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 528/10586 [02:20<44:38,  3.75it/s, v_num=hdbx, train/loss=nan.0, lr=3.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9095, max=0.9891, mean=-0.0008, std=0.2289
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8799, max=0.9995, mean=-0.0008, std=0.2285
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8799, max=0.9995, mean=-0.0008, std=0.2285
[CLAP PIPELINE] Sample 0: 13114/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 529/10586 [02:20<44:40,  3.75it/s, v_num=hdbx, train/loss=nan.0, lr=3.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9832, max=1.0024, mean=-0.0005, std=0.0982
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0049, max=0.9897, mean=-0.0005, std=0.0981
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0049, max=0.9897, mean=-0.0005, std=0.0981
[CLAP PIPELINE] Sample 0: 139602/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 530/10586 [02:21<44:40,  3.75it/s, v_num=hdbx, train/loss=nan.0, lr=3.87e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0012, max=0.9842, mean=0.0002, std=0.0785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9844, mean=0.0002, std=0.0785
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9844, mean=0.0002, std=0.0785
[CLAP PIPELINE] Sample 0: 123144/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 531/10586 [02:21<44:41,  3.75it/s, v_num=hdbx, train/loss=nan.0, lr=3.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8649, max=1.0010, mean=-0.0000, std=0.0203
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8438, max=0.9897, mean=-0.0000, std=0.0203
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8438, max=0.9897, mean=-0.0000, std=0.0203
[CLAP PIPELINE] Sample 0: 152021/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 532/10586 [02:21<44:42,  3.75it/s, v_num=hdbx, train/loss=nan.0, lr=3.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9808, mean=-0.0060, std=0.2939
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=0.9810, mean=-0.0060, std=0.2939
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=0.9810, mean=-0.0060, std=0.2939
[CLAP PIPELINE] Sample 0: 95725/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 533/10586 [02:22<44:44,  3.74it/s, v_num=hdbx, train/loss=nan.0, lr=3.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9624, max=1.0002, mean=-0.0004, std=0.1298
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Sample 0: 144175/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 534/10586 [02:22<44:46,  3.74it/s, v_num=hdbx, train/loss=nan.0, lr=3.88e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9982, max=0.8683, mean=-0.0054, std=0.0859
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.8677, mean=-0.0054, std=0.0859
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.8677, mean=-0.0054, std=0.0859
[CLAP PIPELINE] Sample 0: 127269/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▏                                                                               | 535/10586 [02:23<44:46,  3.74it/s, v_num=hdbx, train/loss=nan.0, lr=3.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0051, max=1.0069, mean=-0.0000, std=0.0850
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9321, max=0.9971, mean=-0.0000, std=0.0850
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9321, max=0.9971, mean=-0.0000, std=0.0850
[CLAP PIPELINE] Sample 0: 35042/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 536/10586 [02:23<44:47,  3.74it/s, v_num=hdbx, train/loss=nan.0, lr=3.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9766, max=1.0027, mean=-0.0002, std=0.2051
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0107, max=1.0078, mean=-0.0002, std=0.2051
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0107, max=1.0078, mean=-0.0002, std=0.2051
[CLAP PIPELINE] Sample 0: 114878/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 537/10586 [02:23<44:46,  3.74it/s, v_num=hdbx, train/loss=nan.0, lr=3.89e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9563, max=1.0000, mean=-0.0002, std=0.0248
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9556, max=0.9941, mean=-0.0002, std=0.0248
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9556, max=0.9941, mean=-0.0002, std=0.0248
[CLAP PIPELINE] Sample 0: 154090/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                                | 538/10586 [02:23<44:47,  3.74it/s, v_num=hdbx, train/loss=nan.0, lr=3.9e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0000, max=0.8992, mean=-0.0015, std=0.0570
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.8818, mean=-0.0015, std=0.0570
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.8818, mean=-0.0015, std=0.0570
[CLAP PIPELINE] Sample 0: 91147/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                                | 539/10586 [02:24<44:50,  3.73it/s, v_num=hdbx, train/loss=nan.0, lr=3.9e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=0.9456, mean=-0.0006, std=0.1208
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9902, max=0.9458, mean=-0.0006, std=0.1208
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9902, max=0.9458, mean=-0.0006, std=0.1208
[CLAP PIPELINE] Sample 0: 122108/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                                | 540/10586 [02:24<44:51,  3.73it/s, v_num=hdbx, train/loss=nan.0, lr=3.9e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9341, max=0.9025, mean=-0.0009, std=0.0382
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8545, max=0.8267, mean=-0.0009, std=0.0369
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8545, max=0.8267, mean=-0.0009, std=0.0369
[CLAP PIPELINE] Sample 0: 107183/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                                | 541/10586 [02:25<44:53,  3.73it/s, v_num=hdbx, train/loss=nan.0, lr=3.9e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9985, max=0.8840, mean=-0.0026, std=0.0598
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=0.8623, mean=-0.0026, std=0.0598
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=0.8623, mean=-0.0026, std=0.0598
[CLAP PIPELINE] Sample 0: 90708/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 542/10586 [02:25<44:54,  3.73it/s, v_num=hdbx, train/loss=nan.0, lr=3.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0003, max=0.8273, mean=-0.0025, std=0.1678
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Sample 0: 24967/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 543/10586 [02:25<44:56,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9280, max=0.9990, mean=-0.0000, std=0.0785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0: 50632/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 544/10586 [02:26<44:57,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9980, max=0.8624, mean=-0.0004, std=0.1807
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.8608, mean=-0.0004, std=0.1807
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.8608, mean=-0.0004, std=0.1807
[CLAP PIPELINE] Sample 0: 90997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 545/10586 [02:26<44:59,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.91e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8996, max=0.9494, mean=-0.0004, std=0.0613
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6411, max=0.6016, mean=-0.0004, std=0.0600
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6411, max=0.6016, mean=-0.0004, std=0.0600
[CLAP PIPELINE] Sample 0: 33174/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 546/10586 [02:26<45:02,  3.71it/s, v_num=hdbx, train/loss=nan.0, lr=3.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9614, max=1.0023, mean=-0.0006, std=0.1827
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9614, max=1.0039, mean=-0.0006, std=0.1826
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9614, max=1.0039, mean=-0.0006, std=0.1826
[CLAP PIPELINE] Sample 0: 106124/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 547/10586 [02:27<45:01,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8810, max=0.9989, mean=-0.0003, std=0.1860
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8804, max=0.9990, mean=-0.0003, std=0.1860
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8804, max=0.9990, mean=-0.0003, std=0.1860
[CLAP PIPELINE] Sample 0: 6/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 548/10586 [02:27<45:01,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9634, mean=0.0000, std=0.0657
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9917, max=0.9473, mean=0.0000, std=0.0657
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9917, max=0.9473, mean=0.0000, std=0.0657
[CLAP PIPELINE] Sample 0: 76982/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 549/10586 [02:27<44:58,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.92e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8162, max=0.9993, mean=-0.0001, std=0.0561
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8091, max=0.9873, mean=-0.0001, std=0.0559
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8091, max=0.9873, mean=-0.0001, std=0.0559
[CLAP PIPELINE] Sample 0: 96048/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 550/10586 [02:27<44:57,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.9650, mean=-0.0000, std=0.1547
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9658, mean=-0.0000, std=0.1548
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9658, mean=-0.0000, std=0.1548
[CLAP PIPELINE] Sample 0: 111893/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▎                                                                               | 551/10586 [02:28<44:57,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9044, mean=-0.0008, std=0.2235
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.8965, mean=-0.0008, std=0.2236
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.8965, mean=-0.0008, std=0.2236
[CLAP PIPELINE] Sample 0: 15391/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 552/10586 [02:28<44:56,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9474, max=1.0005, mean=-0.0014, std=0.0640
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9453, max=0.9980, mean=-0.0014, std=0.0640
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9453, max=0.9980, mean=-0.0014, std=0.0640
[CLAP PIPELINE] Sample 0: 112816/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 553/10586 [02:28<44:55,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.93e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8606, max=1.0119, mean=0.0192, std=0.2174
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8516, max=0.9956, mean=0.0192, std=0.2169
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8516, max=0.9956, mean=0.0192, std=0.2169
[CLAP PIPELINE] Sample 0: 3118/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 554/10586 [02:28<44:55,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9876, max=0.9875, mean=-0.0000, std=0.5074
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9966, max=0.9971, mean=-0.0000, std=0.4985
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9966, max=0.9971, mean=-0.0000, std=0.4985
[CLAP PIPELINE] Sample 0: 87997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 555/10586 [02:29<44:54,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9676, max=1.0002, mean=-0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Sample 0: 25933/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 556/10586 [02:29<44:55,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9603, max=0.9986, mean=-0.0015, std=0.1803
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9575, max=0.9985, mean=-0.0015, std=0.1803
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9575, max=0.9985, mean=-0.0015, std=0.1803
[CLAP PIPELINE] Sample 0: 121818/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 557/10586 [02:29<44:54,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.94e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.7122, mean=0.0001, std=0.0413
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9814, max=0.7056, mean=0.0001, std=0.0413
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9814, max=0.7056, mean=0.0001, std=0.0413
[CLAP PIPELINE] Sample 0: 63882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 558/10586 [02:29<44:53,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0236, max=0.8598, mean=-0.0000, std=0.0502
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7427, max=0.5659, mean=-0.0000, std=0.0434
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7427, max=0.5659, mean=-0.0000, std=0.0434
[CLAP PIPELINE] Sample 0: 47737/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 559/10586 [02:30<44:52,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.9991, mean=-0.0015, std=0.0866
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9985, mean=-0.0015, std=0.0866
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.9985, mean=-0.0015, std=0.0866
[CLAP PIPELINE] Sample 0: 85296/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 560/10586 [02:30<44:51,  3.72it/s, v_num=hdbx, train/loss=nan.0, lr=3.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9280, max=0.9990, mean=-0.0000, std=0.0785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Sample 0: 50632/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 561/10586 [02:30<44:50,  3.73it/s, v_num=hdbx, train/loss=nan.0, lr=3.95e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9844, mean=-0.0005, std=0.0555
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9043, max=0.9102, mean=-0.0005, std=0.0554
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9043, max=0.9102, mean=-0.0005, std=0.0554
[CLAP PIPELINE] Sample 0: 130368/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 562/10586 [02:30<44:49,  3.73it/s, v_num=hdbx, train/loss=nan.0, lr=3.96e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=1.0002, mean=-0.0000, std=0.5685
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=1.0010, mean=-0.0000, std=0.5688
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=1.0010, mean=-0.0000, std=0.5688
[CLAP PIPELINE] Sample 0: 95855/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 563/10586 [02:31<44:49,  3.73it/s, v_num=hdbx, train/loss=nan.0, lr=3.96e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1897, max=1.2273, mean=-0.0000, std=0.1331
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9194, max=1.0537, mean=-0.0000, std=0.1292
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9194, max=1.0537, mean=-0.0000, std=0.1292
[CLAP PIPELINE] Sample 0: 65535/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 564/10586 [02:31<44:48,  3.73it/s, v_num=hdbx, train/loss=nan.0, lr=3.96e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=1.0002, mean=-0.0000, std=0.1336
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=1.0020, mean=-0.0000, std=0.1335
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=1.0020, mean=-0.0000, std=0.1335
[CLAP PIPELINE] Sample 0: 48427/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 565/10586 [02:31<44:48,  3.73it/s, v_num=hdbx, train/loss=nan.0, lr=3.97e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.7777, mean=0.0000, std=0.0404
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.7783, mean=0.0000, std=0.0404
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.7783, mean=0.0000, std=0.0404
[CLAP PIPELINE] Sample 0: 113254/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 566/10586 [02:31<44:47,  3.73it/s, v_num=hdbx, train/loss=nan.0, lr=3.97e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.5584, max=1.0044, mean=-0.0000, std=0.0272
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5225, max=1.0039, mean=-0.0000, std=0.0212
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5225, max=1.0039, mean=-0.0000, std=0.0212
[CLAP PIPELINE] Sample 0: 94999/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▍                                                                               | 567/10586 [02:32<44:46,  3.73it/s, v_num=hdbx, train/loss=nan.0, lr=3.97e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9793, max=0.9997, mean=0.0003, std=0.1813
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9644, max=0.9917, mean=0.0003, std=0.1813
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9644, max=0.9917, mean=0.0003, std=0.1813
[CLAP PIPELINE] Sample 0: 69162/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▌                                                                               | 568/10586 [02:23<42:12,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=3.97e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9989, max=0.9999, mean=0.0000, std=0.0898
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=1.0010, mean=0.0000, std=0.0898
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=1.0010, mean=0.0000, std=0.0898
[CLAP PIPELINE] Sample 0: 148053/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▌                                                                               | 569/10586 [02:23<42:11,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=3.98e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9908, max=0.9915, mean=-0.0007, std=0.0628
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9536, max=0.9741, mean=-0.0007, std=0.0626
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9536, max=0.9741, mean=-0.0007, std=0.0626
[CLAP PIPELINE] Sample 0: 105981/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▌                                                                               | 570/10586 [02:24<42:12,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=3.98e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9481, max=1.0138, mean=0.0001, std=0.0891
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5371, max=0.5664, mean=0.0001, std=0.0422
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5371, max=0.5664, mean=0.0001, std=0.0422
[CLAP PIPELINE] Sample 0: 15251/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▌                                                                               | 571/10586 [02:24<42:12,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.98e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9855, max=0.9864, mean=-0.0023, std=0.1008
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9653, max=0.9868, mean=-0.0023, std=0.1008
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9653, max=0.9868, mean=-0.0023, std=0.1008
[CLAP PIPELINE] Sample 0: 115624/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▌                                                                               | 572/10586 [02:24<42:11,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=3.98e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.7863, mean=-0.0000, std=0.0185
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.7861, mean=-0.0000, std=0.0185
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.7861, mean=-0.0000, std=0.0185
[CLAP PIPELINE] Sample 0: 112731/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▌                                                                               | 573/10586 [02:24<42:12,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.99e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8996, max=0.9494, mean=-0.0004, std=0.0613
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6411, max=0.6016, mean=-0.0004, std=0.0600
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6411, max=0.6016, mean=-0.0004, std=0.0600
[CLAP PIPELINE] Sample 0: 33174/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▌                                                                               | 574/10586 [02:25<42:10,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=3.99e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.8923, mean=-0.0074, std=0.0662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9888, max=0.8657, mean=-0.0074, std=0.0661
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9888, max=0.8657, mean=-0.0074, std=0.0661
[CLAP PIPELINE] Sample 0: 137635/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▌                                                                               | 575/10586 [02:25<42:10,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=3.99e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6732, max=0.9998, mean=-0.0004, std=0.2167
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6733, max=0.9995, mean=-0.0004, std=0.2168
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6733, max=0.9995, mean=-0.0004, std=0.2168
[CLAP PIPELINE] Sample 0: 95719/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▌                                                                               | 576/10586 [02:25<42:11,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=3.99e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0068, max=0.9876, mean=-0.0000, std=0.1221
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0068, max=0.9868, mean=-0.0000, std=0.1221
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0068, max=0.9868, mean=-0.0000, std=0.1221
[CLAP PIPELINE] Sample 0: 115119/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▋                                                                                  | 577/10586 [02:25<42:11,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0012, max=0.9842, mean=0.0002, std=0.0785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9844, mean=0.0002, std=0.0785
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9844, mean=0.0002, std=0.0785
[CLAP PIPELINE] Sample 0: 123144/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▊                                                                                  | 578/10586 [02:26<42:11,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0676, max=1.0493, mean=-0.0000, std=0.1302
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0723, max=1.0635, mean=-0.0000, std=0.1223
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0723, max=1.0635, mean=-0.0000, std=0.1223
[CLAP PIPELINE] Sample 0: 35123/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▊                                                                                  | 579/10586 [02:26<42:12,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9924, max=0.9995, mean=-0.0000, std=0.1191
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9922, max=1.0000, mean=-0.0000, std=0.1191
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9922, max=1.0000, mean=-0.0000, std=0.1191
[CLAP PIPELINE] Sample 0: 33563/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▊                                                                                  | 580/10586 [02:26<42:13,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9910, max=1.1091, mean=0.0000, std=0.0084
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.3403, max=0.3696, mean=0.0000, std=0.0046
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.3403, max=0.3696, mean=0.0000, std=0.0046
[CLAP PIPELINE] Sample 0: 158782/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▌                                                                               | 581/10586 [02:27<42:13,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.01e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9621, max=0.9844, mean=-0.0000, std=0.0176
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8643, max=0.8418, mean=-0.0000, std=0.0169
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8643, max=0.8418, mean=-0.0000, std=0.0169
[CLAP PIPELINE] Sample 0: 148157/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   5%|████▌                                                                               | 582/10586 [02:27<42:15,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.01e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1497, max=1.2149, mean=-0.0000, std=0.0694
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8442, max=0.9546, mean=-0.0000, std=0.0503
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8442, max=0.9546, mean=-0.0000, std=0.0503
[CLAP PIPELINE] Sample 0: 87007/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 583/10586 [02:27<42:17,  3.94it/s, v_num=hdbx, train/loss=nan.0, lr=4.01e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0024, max=0.9556, mean=-0.0000, std=0.1696
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0059, max=0.9409, mean=-0.0000, std=0.1693
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0059, max=0.9409, mean=-0.0000, std=0.1693
[CLAP PIPELINE] Sample 0: 28090/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 584/10586 [02:28<42:17,  3.94it/s, v_num=hdbx, train/loss=nan.0, lr=4.01e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7955, max=0.8804, mean=-0.0000, std=0.0121
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7207, max=0.6548, mean=-0.0000, std=0.0113
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7207, max=0.6548, mean=-0.0000, std=0.0113
[CLAP PIPELINE] Sample 0: 80040/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 585/10586 [02:28<42:19,  3.94it/s, v_num=hdbx, train/loss=nan.0, lr=4.02e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.9046, mean=0.0002, std=0.2047
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9038, mean=0.0002, std=0.2047
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.9038, mean=0.0002, std=0.2047
[CLAP PIPELINE] Sample 0: 122124/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 586/10586 [02:28<42:19,  3.94it/s, v_num=hdbx, train/loss=nan.0, lr=4.02e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9998, max=0.9835, mean=-0.0003, std=0.0390
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0547, max=1.0146, mean=-0.0003, std=0.0382
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0547, max=1.0146, mean=-0.0003, std=0.0382
[CLAP PIPELINE] Sample 0: 134601/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 587/10586 [02:29<42:19,  3.94it/s, v_num=hdbx, train/loss=nan.0, lr=4.02e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.7863, mean=-0.0000, std=0.0185
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.7861, mean=-0.0000, std=0.0185
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.7861, mean=-0.0000, std=0.0185
[CLAP PIPELINE] Sample 0: 112731/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 588/10586 [02:29<42:20,  3.94it/s, v_num=hdbx, train/loss=nan.0, lr=4.02e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0229, max=0.8038, mean=-0.0000, std=0.0180
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6411, max=0.6953, mean=-0.0000, std=0.0130
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6411, max=0.6953, mean=-0.0000, std=0.0130
[CLAP PIPELINE] Sample 0: 148157/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 589/10586 [02:29<42:19,  3.94it/s, v_num=hdbx, train/loss=nan.0, lr=4.03e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9937, max=1.0002, mean=0.0001, std=0.1020
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9941, max=1.0000, mean=0.0001, std=0.1021
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9941, max=1.0000, mean=0.0001, std=0.1021
[CLAP PIPELINE] Sample 0: 112296/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 590/10586 [02:29<42:19,  3.94it/s, v_num=hdbx, train/loss=nan.0, lr=4.03e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0013, max=0.8671, mean=0.0001, std=0.1276
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9946, max=0.8574, mean=0.0001, std=0.1277
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9946, max=0.8574, mean=0.0001, std=0.1277
[CLAP PIPELINE] Sample 0: 116139/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 591/10586 [02:30<42:19,  3.94it/s, v_num=hdbx, train/loss=nan.0, lr=4.03e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9972, max=0.7821, mean=-0.0000, std=0.0092
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9771, max=0.6641, mean=-0.0000, std=0.0087
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9771, max=0.6641, mean=-0.0000, std=0.0087
[CLAP PIPELINE] Sample 0: 151821/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 592/10586 [02:29<42:11,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.04e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9344, max=0.9998, mean=0.0001, std=0.3089
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9346, max=1.0000, mean=0.0001, std=0.3091
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9346, max=1.0000, mean=0.0001, std=0.3091
[CLAP PIPELINE] Sample 0: 48232/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 593/10586 [02:30<42:11,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.04e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7465, max=0.9657, mean=-0.0000, std=0.0711
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7476, max=0.9722, mean=-0.0000, std=0.0708
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7476, max=0.9722, mean=-0.0000, std=0.0708
[CLAP PIPELINE] Sample 0: 65192/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 594/10586 [02:30<42:11,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.04e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.5832, max=0.9461, mean=0.0001, std=0.0399
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5830, max=0.7734, mean=0.0001, std=0.0398
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5830, max=0.7734, mean=0.0001, std=0.0398
[CLAP PIPELINE] Sample 0: 83644/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 595/10586 [02:30<42:10,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.04e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9782, max=0.6285, mean=-0.0000, std=0.0403
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8354, max=0.4902, mean=-0.0000, std=0.0275
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8354, max=0.4902, mean=-0.0000, std=0.0275
[CLAP PIPELINE] Sample 0: 53132/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 596/10586 [02:30<42:09,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.05e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7779, max=0.9992, mean=-0.0001, std=0.0556
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7656, max=0.9976, mean=-0.0001, std=0.0554
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7656, max=0.9976, mean=-0.0001, std=0.0554
[CLAP PIPELINE] Sample 0: 55114/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 597/10586 [02:31<42:09,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.05e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9937, max=1.0002, mean=0.0001, std=0.1020
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9941, max=1.0000, mean=0.0001, std=0.1021
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9941, max=1.0000, mean=0.0001, std=0.1021
[CLAP PIPELINE] Sample 0: 112296/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▋                                                                               | 598/10586 [02:31<42:08,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.05e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7264, max=1.0099, mean=-0.0001, std=0.0313
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7222, max=0.9917, mean=-0.0001, std=0.0312
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7222, max=0.9917, mean=-0.0001, std=0.0312
[CLAP PIPELINE] Sample 0: 149131/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 599/10586 [02:31<42:07,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.05e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8527, max=0.8608, mean=-0.0002, std=0.0387
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7964, max=0.8042, mean=-0.0002, std=0.0381
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7964, max=0.8042, mean=-0.0002, std=0.0381
[CLAP PIPELINE] Sample 0: 25813/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 600/10586 [02:31<42:07,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.06e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0117, max=0.8647, mean=-0.0006, std=0.1469
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0166, max=0.8638, mean=-0.0006, std=0.1465
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0166, max=0.8638, mean=-0.0006, std=0.1465
[CLAP PIPELINE] Sample 0: 80170/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 601/10586 [02:32<42:06,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.06e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9917, max=0.9581, mean=-0.0000, std=0.0719
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0039, max=0.9253, mean=-0.0000, std=0.0718
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0039, max=0.9253, mean=-0.0000, std=0.0718
[CLAP PIPELINE] Sample 0: 99982/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 602/10586 [02:32<42:04,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.06e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.5613, mean=-0.0000, std=0.0136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Sample 0: 18388/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 603/10586 [02:32<42:03,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.06e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9986, max=0.9916, mean=0.0004, std=0.1325
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0: 99635/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 604/10586 [02:32<42:03,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.07e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9095, max=0.9891, mean=-0.0008, std=0.2289
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8799, max=0.9995, mean=-0.0008, std=0.2285
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8799, max=0.9995, mean=-0.0008, std=0.2285
[CLAP PIPELINE] Sample 0: 13114/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 605/10586 [02:32<42:01,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.07e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0084, mean=-0.0001, std=0.1935
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Sample 0: 39111/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 606/10586 [02:33<42:01,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.07e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9956, max=0.9918, mean=-0.0000, std=0.0923
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7827, max=0.8452, mean=-0.0000, std=0.0874
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7827, max=0.8452, mean=-0.0000, std=0.0874
[CLAP PIPELINE] Sample 0: 132224/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 607/10586 [02:33<42:01,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.07e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9931, max=0.9308, mean=-0.0000, std=0.1076
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9946, max=0.9312, mean=-0.0000, std=0.1076
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9946, max=0.9312, mean=-0.0000, std=0.1076
[CLAP PIPELINE] Sample 0: 50682/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 608/10586 [02:33<42:00,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.08e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9481, max=1.0138, mean=0.0001, std=0.0891
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5371, max=0.5664, mean=0.0001, std=0.0422
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5371, max=0.5664, mean=0.0001, std=0.0422
[CLAP PIPELINE] Sample 0: 15251/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 609/10586 [02:33<42:00,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.08e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9965, mean=-0.0004, std=0.1406
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Sample 0: 27046/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 610/10586 [02:34<42:01,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.08e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9357, max=0.9780, mean=-0.0001, std=0.0555
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9526, max=0.9629, mean=-0.0001, std=0.0555
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9526, max=0.9629, mean=-0.0001, std=0.0555
[CLAP PIPELINE] Sample 0: 140025/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 611/10586 [02:34<41:59,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.08e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0003, max=0.9558, mean=-0.0000, std=0.0667
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9551, mean=-0.0000, std=0.0668
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9551, mean=-0.0000, std=0.0668
[CLAP PIPELINE] Sample 0: 82199/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 612/10586 [02:34<42:01,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.09e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9614, max=1.0023, mean=-0.0006, std=0.1827
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9614, max=1.0039, mean=-0.0006, std=0.1826
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9614, max=1.0039, mean=-0.0006, std=0.1826
[CLAP PIPELINE] Sample 0: 106124/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 613/10586 [02:35<42:02,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.09e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9994, mean=-0.0028, std=0.1001
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9595, max=0.9692, mean=-0.0028, std=0.1001
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9595, max=0.9692, mean=-0.0028, std=0.1001
[CLAP PIPELINE] Sample 0: 127126/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▊                                                                               | 614/10586 [02:34<41:55,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.09e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8628, max=0.9054, mean=-0.0000, std=0.0504
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.3704, max=0.3982, mean=-0.0000, std=0.0234
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.3704, max=0.3982, mean=-0.0000, std=0.0234
[CLAP PIPELINE] Sample 0: 20380/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                                | 615/10586 [02:35<41:54,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.1e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9951, max=0.9816, mean=0.0001, std=0.1136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9790, mean=0.0001, std=0.1136
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9790, mean=0.0001, std=0.1136
[CLAP PIPELINE] Sample 0: 133661/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                                | 616/10586 [02:35<41:55,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.1e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9959, max=1.0002, mean=-0.0000, std=0.1500
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9961, max=0.9976, mean=-0.0000, std=0.1500
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9961, max=0.9976, mean=-0.0000, std=0.1500
[CLAP PIPELINE] Sample 0: 69152/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                                | 617/10586 [02:35<41:56,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.1e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9013, max=1.0021, mean=-0.0000, std=0.0761
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8965, max=0.9722, mean=-0.0000, std=0.0760
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8965, max=0.9722, mean=-0.0000, std=0.0760
[CLAP PIPELINE] Sample 0: 142004/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                                | 618/10586 [02:36<41:57,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.1e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0813, max=1.0524, mean=-0.0000, std=0.0316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.1357, max=1.0908, mean=-0.0000, std=0.0309
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.1357, max=1.0908, mean=-0.0000, std=0.0309
[CLAP PIPELINE] Sample 0: 55383/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 619/10586 [02:36<41:58,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.11e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0013, max=0.6589, mean=-0.0000, std=0.0581
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9780, max=0.6538, mean=-0.0000, std=0.0581
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9780, max=0.6538, mean=-0.0000, std=0.0581
[CLAP PIPELINE] Sample 0: 47224/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 620/10586 [02:36<41:59,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.11e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9786, max=0.8970, mean=0.0009, std=0.1491
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9717, max=0.8916, mean=0.0009, std=0.1492
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9717, max=0.8916, mean=0.0009, std=0.1492
[CLAP PIPELINE] Sample 0: 47216/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 621/10586 [02:36<41:58,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.11e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0170, max=0.7277, mean=-0.0000, std=0.0304
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8125, max=0.6289, mean=-0.0000, std=0.0293
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8125, max=0.6289, mean=-0.0000, std=0.0293
[CLAP PIPELINE] Sample 0: 138935/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 622/10586 [02:37<41:59,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.11e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7048, max=1.0000, mean=0.0048, std=0.1503
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6890, max=1.0000, mean=0.0048, std=0.1504
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6890, max=1.0000, mean=0.0048, std=0.1504
[CLAP PIPELINE] Sample 0: 97388/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 623/10586 [02:37<41:59,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.12e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9829, max=1.0046, mean=0.0000, std=0.1512
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9858, max=1.0127, mean=0.0000, std=0.1512
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9858, max=1.0127, mean=0.0000, std=0.1512
[CLAP PIPELINE] Sample 0: 124330/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 624/10586 [02:37<41:59,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.12e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9718, max=0.9713, mean=-0.0001, std=0.4037
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8530, max=0.8540, mean=-0.0001, std=0.3882
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8530, max=0.8540, mean=-0.0001, std=0.3882
[CLAP PIPELINE] Sample 0: 48020/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 625/10586 [02:38<41:58,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.12e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9559, max=0.6156, mean=0.0001, std=0.0153
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0: 150065/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 626/10586 [02:38<42:00,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.12e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9603, max=0.9986, mean=-0.0015, std=0.1803
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9575, max=0.9985, mean=-0.0015, std=0.1803
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9575, max=0.9985, mean=-0.0015, std=0.1803
[CLAP PIPELINE] Sample 0: 121818/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 627/10586 [02:38<41:59,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.13e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7116, max=0.9945, mean=-0.0000, std=0.0276
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6792, max=0.9692, mean=-0.0000, std=0.0276
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6792, max=0.9692, mean=-0.0000, std=0.0276
[CLAP PIPELINE] Sample 0: 88588/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 628/10586 [02:38<42:00,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.13e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9603, max=0.9986, mean=-0.0015, std=0.1803
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9575, max=0.9985, mean=-0.0015, std=0.1803
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9575, max=0.9985, mean=-0.0015, std=0.1803
[CLAP PIPELINE] Sample 0: 121818/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 629/10586 [02:39<42:01,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.13e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9968, max=0.7898, mean=-0.0000, std=0.0482
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.7783, mean=-0.0000, std=0.0482
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.7783, mean=-0.0000, std=0.0482
[CLAP PIPELINE] Sample 0: 96554/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|████▉                                                                               | 630/10586 [02:39<42:02,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.13e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=1.0015, mean=-0.0003, std=0.0648
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9487, max=1.0020, mean=-0.0003, std=0.0648
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9487, max=1.0020, mean=-0.0003, std=0.0648
[CLAP PIPELINE] Sample 0: 142440/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 631/10586 [02:39<42:03,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.14e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9917, max=0.9581, mean=-0.0000, std=0.0719
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0039, max=0.9253, mean=-0.0000, std=0.0718
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0039, max=0.9253, mean=-0.0000, std=0.0718
[CLAP PIPELINE] Sample 0: 99982/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 632/10586 [02:40<42:02,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.14e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9752, max=1.0037, mean=-0.0017, std=0.0746
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9590, max=1.0029, mean=-0.0017, std=0.0746
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9590, max=1.0029, mean=-0.0017, std=0.0746
[CLAP PIPELINE] Sample 0: 104319/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 633/10586 [02:39<41:55,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.14e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.2816, max=1.1737, mean=0.0000, std=0.1008
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.2266, max=1.2549, mean=0.0000, std=0.0986
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.2266, max=1.2549, mean=0.0000, std=0.0986
[CLAP PIPELINE] Sample 0: 32657/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 634/10586 [02:40<41:55,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.14e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=0.9948, mean=0.0001, std=0.0673
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9961, max=0.9951, mean=0.0001, std=0.0673
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9961, max=0.9951, mean=0.0001, std=0.0673
[CLAP PIPELINE] Sample 0: 141545/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 635/10586 [02:40<41:56,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.15e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7779, max=0.9992, mean=-0.0001, std=0.0556
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7656, max=0.9976, mean=-0.0001, std=0.0554
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7656, max=0.9976, mean=-0.0001, std=0.0554
[CLAP PIPELINE] Sample 0: 55114/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 636/10586 [02:40<41:55,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.15e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0000, max=0.9993, mean=-0.0000, std=0.0946
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0244, max=1.0166, mean=-0.0000, std=0.0945
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0244, max=1.0166, mean=-0.0000, std=0.0945
[CLAP PIPELINE] Sample 0: 140796/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 637/10586 [02:41<41:55,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.15e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9317, max=0.9996, mean=0.0001, std=0.2213
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9321, max=0.9990, mean=0.0001, std=0.2214
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9321, max=0.9990, mean=0.0001, std=0.2214
[CLAP PIPELINE] Sample 0: 31515/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 638/10586 [02:41<41:55,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.15e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9998, max=0.9981, mean=0.0033, std=0.3344
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9985, mean=0.0033, std=0.3345
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9985, mean=0.0033, std=0.3345
[CLAP PIPELINE] Sample 0: 95863/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 639/10586 [02:41<41:55,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.16e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7342, max=0.7987, mean=-0.0000, std=0.0520
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5332, max=0.7290, mean=-0.0000, std=0.0467
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5332, max=0.7290, mean=-0.0000, std=0.0467
[CLAP PIPELINE] Sample 0: 80145/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 640/10586 [02:41<41:53,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.16e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0388, max=1.1043, mean=0.0002, std=0.2195
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0625, max=1.0547, mean=0.0002, std=0.2151
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0625, max=1.0547, mean=0.0002, std=0.2151
[CLAP PIPELINE] Sample 0: 31830/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 641/10586 [02:41<41:51,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.16e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9992, max=0.9834, mean=-0.0002, std=0.0612
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9458, max=0.9844, mean=-0.0002, std=0.0611
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9458, max=0.9844, mean=-0.0002, std=0.0611
[CLAP PIPELINE] Sample 0: 129159/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 642/10586 [02:42<41:50,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.17e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8800, max=0.9998, mean=0.0000, std=0.1742
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8799, max=0.9966, mean=0.0000, std=0.1743
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8799, max=0.9966, mean=0.0000, std=0.1743
[CLAP PIPELINE] Sample 0: 70696/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 643/10586 [02:42<41:48,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.17e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8057, max=0.9947, mean=0.0003, std=0.1520
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7964, max=0.9917, mean=0.0003, std=0.1520
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7964, max=0.9917, mean=0.0003, std=0.1520
[CLAP PIPELINE] Sample 0: 14087/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 644/10586 [02:42<41:47,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.17e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0: 47237/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████                                                                               | 645/10586 [02:42<41:46,  3.97it/s, v_num=hdbx, train/loss=nan.0, lr=4.17e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9985, max=0.9984, mean=0.0027, std=0.4850
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=1.0010, mean=0.0027, std=0.4851
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=1.0010, mean=0.0027, std=0.4851
[CLAP PIPELINE] Sample 0: 12894/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 646/10586 [02:43<41:48,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.18e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9295, max=0.9991, mean=-0.0002, std=0.2361
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=1.0010, mean=-0.0002, std=0.2361
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=1.0010, mean=-0.0002, std=0.2361
[CLAP PIPELINE] Sample 0: 63764/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 647/10586 [02:43<41:47,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.18e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9013, max=1.0021, mean=-0.0000, std=0.0761
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8965, max=0.9722, mean=-0.0000, std=0.0760
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8965, max=0.9722, mean=-0.0000, std=0.0760
[CLAP PIPELINE] Sample 0: 142004/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 648/10586 [02:43<41:49,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.18e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9957, max=0.9346, mean=-0.0006, std=0.0270
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9946, max=0.8574, mean=-0.0006, std=0.0257
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9946, max=0.8574, mean=-0.0006, std=0.0257
[CLAP PIPELINE] Sample 0: 127233/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 649/10586 [02:43<41:50,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.18e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9955, max=0.8652, mean=-0.0000, std=0.0358
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9419, max=0.7778, mean=-0.0000, std=0.0352
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9419, max=0.7778, mean=-0.0000, std=0.0352
[CLAP PIPELINE] Sample 0: 130987/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 650/10586 [02:44<41:50,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.19e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9868, max=0.8880, mean=0.0000, std=0.1075
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9766, max=0.8892, mean=0.0000, std=0.1067
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9766, max=0.8892, mean=0.0000, std=0.1067
[CLAP PIPELINE] Sample 0: 733/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 651/10586 [02:44<41:52,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.19e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9967, max=0.8892, mean=-0.0005, std=0.1574
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=0.8481, mean=-0.0005, std=0.1570
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=0.8481, mean=-0.0005, std=0.1570
[CLAP PIPELINE] Sample 0: 12270/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 652/10586 [02:44<41:52,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.19e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0016, max=0.9504, mean=-0.0000, std=0.0334
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9907, max=0.9399, mean=-0.0000, std=0.0334
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9907, max=0.9399, mean=-0.0000, std=0.0334
[CLAP PIPELINE] Sample 0: 150328/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 653/10586 [02:45<41:53,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.19e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9880, max=0.8940, mean=0.0001, std=0.1081
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.8838, mean=0.0001, std=0.1069
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.8838, mean=0.0001, std=0.1069
[CLAP PIPELINE] Sample 0: 80626/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                               | 654/10586 [02:44<41:45,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.2e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9680, max=0.9256, mean=-0.0028, std=0.1273
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7139, max=0.8584, mean=-0.0028, std=0.1180
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7139, max=0.8584, mean=-0.0028, std=0.1180
[CLAP PIPELINE] Sample 0: 14928/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                               | 655/10586 [02:45<41:45,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.2e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7515, max=0.9823, mean=-0.0000, std=0.0562
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5659, max=0.5854, mean=-0.0000, std=0.0477
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5659, max=0.5854, mean=-0.0000, std=0.0477
[CLAP PIPELINE] Sample 0: 62671/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                               | 656/10586 [02:45<41:45,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.2e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9910, max=1.1091, mean=0.0000, std=0.0084
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.3403, max=0.3696, mean=0.0000, std=0.0046
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.3403, max=0.3696, mean=0.0000, std=0.0046
[CLAP PIPELINE] Sample 0: 158782/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                               | 657/10586 [02:45<41:47,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.2e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.9314, mean=-0.0002, std=0.0972
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9316, mean=-0.0002, std=0.0973
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9316, mean=-0.0002, std=0.0973
[CLAP PIPELINE] Sample 0: 79478/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 658/10586 [02:46<41:49,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.21e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.7420, mean=-0.0188, std=0.0789
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9932, max=0.7388, mean=-0.0188, std=0.0789
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9932, max=0.7388, mean=-0.0188, std=0.0789
[CLAP PIPELINE] Sample 0: 116805/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 659/10586 [02:46<41:49,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.21e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0053, max=0.8300, mean=-0.0000, std=0.0187
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9824, max=0.8228, mean=-0.0000, std=0.0188
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9824, max=0.8228, mean=-0.0000, std=0.0188
[CLAP PIPELINE] Sample 0: 134619/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 660/10586 [02:46<41:49,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.21e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0082, max=0.9617, mean=-0.0000, std=0.0235
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6812, max=0.6206, mean=-0.0000, std=0.0112
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6812, max=0.6206, mean=-0.0000, std=0.0112
[CLAP PIPELINE] Sample 0: 655/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▏                                                                              | 661/10586 [02:47<41:49,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.21e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0097, max=0.9844, mean=0.0002, std=0.0780
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0264, max=0.9795, mean=0.0002, std=0.0779
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0264, max=0.9795, mean=0.0002, std=0.0779
[CLAP PIPELINE] Sample 0: 53226/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 662/10586 [02:47<41:49,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.22e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8002, max=1.0032, mean=-0.0002, std=0.0756
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0: 131203/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 663/10586 [02:47<41:50,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.22e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0010, max=0.9703, mean=-0.0000, std=0.0720
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.9653, mean=-0.0000, std=0.0720
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.9653, mean=-0.0000, std=0.0720
[CLAP PIPELINE] Sample 0: 113177/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 664/10586 [02:48<41:50,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.22e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9609, max=0.9900, mean=-0.0000, std=0.1076
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9565, max=0.9468, mean=-0.0000, std=0.1075
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9565, max=0.9468, mean=-0.0000, std=0.1075
[CLAP PIPELINE] Sample 0: 52094/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 665/10586 [02:48<41:51,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.22e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9044, mean=-0.0008, std=0.2235
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.8965, mean=-0.0008, std=0.2236
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.8965, mean=-0.0008, std=0.2236
[CLAP PIPELINE] Sample 0: 15391/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 666/10586 [02:48<41:53,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.23e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.3257, max=0.5664, mean=-0.0000, std=0.0078
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.2211, max=0.3230, mean=-0.0000, std=0.0060
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.2211, max=0.3230, mean=-0.0000, std=0.0060
[CLAP PIPELINE] Sample 0: 138959/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 667/10586 [02:49<41:53,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.23e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9295, max=0.9991, mean=-0.0002, std=0.2361
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=1.0010, mean=-0.0002, std=0.2361
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=1.0010, mean=-0.0002, std=0.2361
[CLAP PIPELINE] Sample 0: 63764/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 668/10586 [02:49<41:53,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.23e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.8334, mean=-0.0000, std=0.1383
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9912, max=0.8223, mean=-0.0000, std=0.1383
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9912, max=0.8223, mean=-0.0000, std=0.1383
[CLAP PIPELINE] Sample 0: 103796/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 669/10586 [02:49<41:53,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.24e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.9017, mean=0.0002, std=0.1192
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9956, max=0.9019, mean=0.0002, std=0.1191
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9956, max=0.9019, mean=0.0002, std=0.1191
[CLAP PIPELINE] Sample 0: 131309/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 670/10586 [02:49<41:55,  3.94it/s, v_num=hdbx, train/loss=nan.0, lr=4.24e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9908, max=1.0015, mean=-0.0000, std=0.0681
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9399, max=0.9409, mean=-0.0000, std=0.0680
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9399, max=0.9409, mean=-0.0000, std=0.0680
[CLAP PIPELINE] Sample 0: 109221/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 671/10586 [02:50<41:54,  3.94it/s, v_num=hdbx, train/loss=nan.0, lr=4.24e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7351, max=0.7890, mean=-0.0000, std=0.0251
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.3665, max=0.3438, mean=-0.0000, std=0.0220
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.3665, max=0.3438, mean=-0.0000, std=0.0220
[CLAP PIPELINE] Sample 0: 17026/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 672/10586 [02:49<41:46,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.24e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9839, max=0.7538, mean=-0.0006, std=0.0165
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0742, max=0.7061, mean=-0.0006, std=0.0162
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0742, max=0.7061, mean=-0.0006, std=0.0162
[CLAP PIPELINE] Sample 0: 130485/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 673/10586 [02:50<41:46,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.25e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0031, max=0.9248, mean=-0.0000, std=0.1113
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9839, max=0.8633, mean=-0.0000, std=0.1111
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9839, max=0.8633, mean=-0.0000, std=0.1111
[CLAP PIPELINE] Sample 0: 20438/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 674/10586 [02:50<41:45,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.25e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8999, max=1.0054, mean=-0.0001, std=0.0841
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8789, max=0.7700, mean=-0.0001, std=0.0834
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8789, max=0.7700, mean=-0.0001, std=0.0834
[CLAP PIPELINE] Sample 0: 51329/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 675/10586 [02:50<41:45,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.25e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9617, max=0.9935, mean=-0.0000, std=0.0818
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9619, max=0.9893, mean=-0.0000, std=0.0818
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9619, max=0.9893, mean=-0.0000, std=0.0818
[CLAP PIPELINE] Sample 0: 146182/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 676/10586 [02:50<41:44,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.25e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.5832, max=0.9461, mean=0.0001, std=0.0399
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5830, max=0.7734, mean=0.0001, std=0.0398
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5830, max=0.7734, mean=0.0001, std=0.0398
[CLAP PIPELINE] Sample 0: 83644/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▎                                                                              | 677/10586 [02:51<41:43,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.26e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9986, max=0.9916, mean=0.0004, std=0.1325
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0: 99635/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▍                                                                              | 678/10586 [02:51<41:43,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.26e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.9657, mean=0.0000, std=0.2225
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9663, mean=0.0000, std=0.2227
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9663, mean=0.0000, std=0.2227
[CLAP PIPELINE] Sample 0: 41608/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▍                                                                              | 679/10586 [02:51<41:43,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.26e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9766, max=1.0027, mean=-0.0002, std=0.2051
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0107, max=1.0078, mean=-0.0002, std=0.2051
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0107, max=1.0078, mean=-0.0002, std=0.2051
[CLAP PIPELINE] Sample 0: 114878/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▍                                                                              | 680/10586 [02:51<41:42,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.26e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7048, max=1.0000, mean=0.0048, std=0.1503
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6890, max=1.0000, mean=0.0048, std=0.1504
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6890, max=1.0000, mean=0.0048, std=0.1504
[CLAP PIPELINE] Sample 0: 97388/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▍                                                                              | 681/10586 [02:52<41:42,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.27e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8002, max=1.0032, mean=-0.0002, std=0.0756
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0: 131203/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▍                                                                              | 682/10586 [02:52<41:42,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.27e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9934, max=0.7545, mean=-0.0000, std=0.0495
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9917, max=0.7549, mean=-0.0000, std=0.0495
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9917, max=0.7549, mean=-0.0000, std=0.0495
[CLAP PIPELINE] Sample 0: 121158/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▍                                                                              | 683/10586 [02:52<41:42,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.27e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9985, max=0.9924, mean=-0.0011, std=0.0672
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.9897, mean=-0.0011, std=0.0671
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.9897, mean=-0.0011, std=0.0671
[CLAP PIPELINE] Sample 0: 114670/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▍                                                                              | 684/10586 [02:52<41:42,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.27e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8672, max=0.9987, mean=-0.0006, std=0.1397
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8564, max=1.0049, mean=-0.0006, std=0.1396
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8564, max=1.0049, mean=-0.0006, std=0.1396
[CLAP PIPELINE] Sample 0: 77077/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▍                                                                              | 685/10586 [02:53<41:42,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.28e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=0.9759, mean=-0.0001, std=0.0569
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9688, max=0.9858, mean=-0.0001, std=0.0569
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9688, max=0.9858, mean=-0.0001, std=0.0569
[CLAP PIPELINE] Sample 0: 138091/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▍                                                                              | 686/10586 [02:53<41:42,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.28e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Sample 0: 150811/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▍                                                                              | 687/10586 [02:53<41:43,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.28e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=0.5959, mean=-0.0000, std=0.0308
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9893, max=0.5952, mean=-0.0000, std=0.0308
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9893, max=0.5952, mean=-0.0000, std=0.0308
[CLAP PIPELINE] Sample 0: 149682/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   6%|█████▍                                                                              | 688/10586 [02:53<41:42,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.28e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8866, max=0.9992, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7402, max=0.7954, mean=0.0000, std=0.0335
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7402, max=0.7954, mean=0.0000, std=0.0335
[CLAP PIPELINE] Sample 0: 104330/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▍                                                                              | 689/10586 [02:54<41:43,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.29e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8091, mean=-0.0016, std=0.1717
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9570, max=0.8037, mean=-0.0016, std=0.1718
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9570, max=0.8037, mean=-0.0016, std=0.1718
[CLAP PIPELINE] Sample 0: 84771/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▍                                                                              | 690/10586 [02:54<41:41,  3.96it/s, v_num=hdbx, train/loss=nan.0, lr=4.29e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9952, max=0.9755, mean=-0.0015, std=0.1295
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9917, max=0.9536, mean=-0.0015, std=0.1295
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9917, max=0.9536, mean=-0.0015, std=0.1295
[CLAP PIPELINE] Sample 0: 94005/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▍                                                                              | 691/10586 [02:54<41:44,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.29e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9922, max=0.9009, mean=-0.0003, std=0.0580
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9233, max=0.8716, mean=-0.0003, std=0.0569
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9233, max=0.8716, mean=-0.0003, std=0.0569
[CLAP PIPELINE] Sample 0: 141883/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                               | 692/10586 [02:55<41:44,  3.95it/s, v_num=hdbx, train/loss=nan.0, lr=4.3e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.9991, mean=-0.0015, std=0.0866
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9985, mean=-0.0015, std=0.0866
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.9985, mean=-0.0015, std=0.0866
[CLAP PIPELINE] Sample 0: 85296/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                               | 693/10586 [03:02<43:21,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.3e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7665, max=0.9953, mean=-0.0000, std=0.0514
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7573, max=1.0137, mean=-0.0000, std=0.0514
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7573, max=1.0137, mean=-0.0000, std=0.0514
[CLAP PIPELINE] Sample 0: 153241/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                               | 694/10586 [03:02<43:20,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.3e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=0.8569, mean=0.0008, std=0.2146
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.8496, mean=0.0008, std=0.2146
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.8496, mean=0.0008, std=0.2146
[CLAP PIPELINE] Sample 0: 96009/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                               | 695/10586 [03:02<43:19,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.3e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=0.9760, mean=-0.0000, std=0.2755
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9741, mean=-0.0000, std=0.2756
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.9741, mean=-0.0000, std=0.2756
[CLAP PIPELINE] Sample 0: 6/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1511, max=0.1292, mean=-0.0003, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap 1.0000
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 696/10586 [03:02<43:19,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.31e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0136, max=0.8102, mean=0.0000, std=0.0563
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9956, max=0.8066, mean=0.0000, std=0.0558
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9956, max=0.8066, mean=0.0000, std=0.0558
[CLAP PIPELINE] Sample 0: 118218/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 697/10586 [03:03<43:18,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.31e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0000, max=0.9631, mean=-0.0008, std=0.0653
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9619, mean=-0.0008, std=0.0653
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9619, mean=-0.0008, std=0.0653
[CLAP PIPELINE] Sample 0: 111981/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 698/10586 [03:03<43:18,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.31e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8091, mean=-0.0016, std=0.1717
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9570, max=0.8037, mean=-0.0016, std=0.1718
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9570, max=0.8037, mean=-0.0016, std=0.1718
[CLAP PIPELINE] Sample 0: 84771/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 699/10586 [03:03<43:18,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.31e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9028, max=0.9999, mean=-0.0007, std=0.0467
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8931, max=0.9795, mean=-0.0007, std=0.0466
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8931, max=0.9795, mean=-0.0007, std=0.0466
[CLAP PIPELINE] Sample 0: 142859/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 700/10586 [03:03<43:18,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.32e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7767, max=0.9819, mean=-0.0001, std=0.1435
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7612, max=0.9814, mean=-0.0001, std=0.1436
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7612, max=0.9814, mean=-0.0001, std=0.1436
[CLAP PIPELINE] Sample 0: 39910/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 701/10586 [03:04<43:17,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.32e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9028, max=0.9999, mean=-0.0007, std=0.0467
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8931, max=0.9795, mean=-0.0007, std=0.0466
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8931, max=0.9795, mean=-0.0007, std=0.0466
[CLAP PIPELINE] Sample 0: 142859/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 702/10586 [03:04<43:16,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.32e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8585, max=0.9812, mean=-0.0000, std=0.0313
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8525, max=0.9839, mean=-0.0000, std=0.0312
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8525, max=0.9839, mean=-0.0000, std=0.0312
[CLAP PIPELINE] Sample 0: 65896/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 703/10586 [03:04<43:15,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.32e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8860, max=0.8689, mean=-0.0001, std=0.1662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Sample 0: 111849/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 704/10586 [03:04<43:16,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.33e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9393, max=0.9817, mean=-0.0007, std=0.0737
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9214, max=0.9331, mean=-0.0007, std=0.0737
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9214, max=0.9331, mean=-0.0007, std=0.0737
[CLAP PIPELINE] Sample 0: 121490/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 705/10586 [03:05<43:15,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.33e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0885, max=1.0489, mean=0.0000, std=0.0271
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8237, max=0.7852, mean=0.0000, std=0.0258
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8237, max=0.7852, mean=0.0000, std=0.0258
[CLAP PIPELINE] Sample 0: 66436/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 706/10586 [03:05<43:14,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.33e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=0.8843, mean=-0.0004, std=0.0544
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0039, max=0.8853, mean=-0.0004, std=0.0544
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0039, max=0.8853, mean=-0.0004, std=0.0544
[CLAP PIPELINE] Sample 0: 142838/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 707/10586 [03:05<43:13,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.33e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9133, max=0.9307, mean=0.0004, std=0.0840
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7100, max=0.7236, mean=0.0004, std=0.0832
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7100, max=0.7236, mean=0.0004, std=0.0832
[CLAP PIPELINE] Sample 0: 113946/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▌                                                                              | 708/10586 [03:05<43:12,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.34e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9729, max=0.8797, mean=-0.0026, std=0.1368
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9287, max=0.8765, mean=-0.0026, std=0.1364
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9287, max=0.8765, mean=-0.0026, std=0.1364
[CLAP PIPELINE] Sample 0: 100571/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 709/10586 [03:06<43:12,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.34e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8141, max=0.9992, mean=-0.0000, std=0.0608
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8115, max=1.0000, mean=-0.0000, std=0.0608
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8115, max=1.0000, mean=-0.0000, std=0.0608
[CLAP PIPELINE] Sample 0: 6/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1828, max=0.1266, mean=-0.0028, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap 1.0000
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 710/10586 [03:06<43:11,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.34e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8131, max=0.8253, mean=0.0002, std=0.0249
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6875, max=0.6328, mean=0.0002, std=0.0214
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6875, max=0.6328, mean=0.0002, std=0.0214
[CLAP PIPELINE] Sample 0: 125452/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 711/10586 [03:06<43:11,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.34e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9930, max=0.9276, mean=0.0007, std=0.1144
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9556, max=0.9409, mean=0.0007, std=0.1144
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9556, max=0.9409, mean=0.0007, std=0.1144
[CLAP PIPELINE] Sample 0: 145418/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 712/10586 [03:06<43:10,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.35e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9938, max=0.9878, mean=-0.0000, std=0.1524
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9902, mean=-0.0000, std=0.1533
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9902, mean=-0.0000, std=0.1533
[CLAP PIPELINE] Sample 0: 72551/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 713/10586 [03:07<43:10,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.35e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0570, max=1.0064, mean=0.0063, std=0.1807
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9360, max=0.9819, mean=0.0063, std=0.1780
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9360, max=0.9819, mean=0.0063, std=0.1780
[CLAP PIPELINE] Sample 0: 6/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 714/10586 [03:07<43:09,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.35e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8940, max=1.0011, mean=-0.0000, std=0.0529
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9102, max=1.0449, mean=-0.0000, std=0.0526
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9102, max=1.0449, mean=-0.0000, std=0.0526
[CLAP PIPELINE] Sample 0: 152560/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 715/10586 [03:07<43:09,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.35e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8297, max=1.0162, mean=-0.0000, std=0.0370
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Sample 0: 64456/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 716/10586 [03:07<43:08,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.36e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.9380, mean=-0.0001, std=0.0279
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9917, max=0.9292, mean=-0.0001, std=0.0279
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9917, max=0.9292, mean=-0.0001, std=0.0279
[CLAP PIPELINE] Sample 0: 143526/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 717/10586 [03:08<43:08,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.36e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9028, max=0.9999, mean=-0.0007, std=0.0467
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8931, max=0.9795, mean=-0.0007, std=0.0466
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8931, max=0.9795, mean=-0.0007, std=0.0466
[CLAP PIPELINE] Sample 0: 142859/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 718/10586 [03:08<43:07,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.36e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9867, max=0.9724, mean=0.0001, std=0.1165
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9902, max=0.9727, mean=0.0001, std=0.1165
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9902, max=0.9727, mean=0.0001, std=0.1165
[CLAP PIPELINE] Sample 0: 62695/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 719/10586 [03:08<43:07,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.37e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0027, max=0.9153, mean=-0.0009, std=0.1298
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9131, mean=-0.0009, std=0.1298
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.9131, mean=-0.0009, std=0.1298
[CLAP PIPELINE] Sample 0: 126186/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 720/10586 [03:08<43:07,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.37e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0068, max=1.0006, mean=0.0000, std=0.0540
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0283, max=0.9473, mean=0.0000, std=0.0527
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0283, max=0.9473, mean=0.0000, std=0.0527
[CLAP PIPELINE] Sample 0: 66311/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 721/10586 [03:08<43:05,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.37e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7930, max=0.9317, mean=-0.0000, std=0.0726
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7744, max=0.9214, mean=-0.0000, std=0.0726
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7744, max=0.9214, mean=-0.0000, std=0.0726
[CLAP PIPELINE] Sample 0: 110221/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 722/10586 [03:09<43:04,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.37e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9427, max=1.0070, mean=-0.0001, std=0.0416
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7642, max=0.6187, mean=-0.0001, std=0.0379
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7642, max=0.6187, mean=-0.0001, std=0.0379
[CLAP PIPELINE] Sample 0: 46123/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 723/10586 [03:09<43:04,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.38e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0031, max=0.9248, mean=-0.0000, std=0.1113
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9839, max=0.8633, mean=-0.0000, std=0.1111
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9839, max=0.8633, mean=-0.0000, std=0.1111
[CLAP PIPELINE] Sample 0: 20438/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▋                                                                              | 724/10586 [03:09<43:04,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.38e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8166, max=0.8773, mean=0.0499, std=0.3801
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5996, max=0.7314, mean=0.0499, std=0.3506
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5996, max=0.7314, mean=0.0499, std=0.3506
[CLAP PIPELINE] Sample 0: 1413/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 725/10586 [03:10<43:05,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.38e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9954, max=0.9578, mean=-0.0013, std=0.0753
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9922, max=0.9604, mean=-0.0013, std=0.0754
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9922, max=0.9604, mean=-0.0013, std=0.0754
[CLAP PIPELINE] Sample 0: 105618/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 726/10586 [03:10<43:05,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.38e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8880, max=1.0012, mean=-0.0001, std=0.2365
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8770, max=0.9639, mean=-0.0001, std=0.2361
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8770, max=0.9639, mean=-0.0001, std=0.2361
[CLAP PIPELINE] Sample 0: 1988/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 727/10586 [03:10<43:04,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.39e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8940, max=1.0011, mean=-0.0000, std=0.0529
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9102, max=1.0449, mean=-0.0000, std=0.0526
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9102, max=1.0449, mean=-0.0000, std=0.0526
[CLAP PIPELINE] Sample 0: 152560/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 728/10586 [03:10<43:05,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.39e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9793, max=0.6877, mean=-0.0000, std=0.0244
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8970, max=0.6709, mean=-0.0000, std=0.0239
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8970, max=0.6709, mean=-0.0000, std=0.0239
[CLAP PIPELINE] Sample 0: 142943/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 729/10586 [03:11<43:05,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.39e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8896, max=0.9998, mean=-0.0000, std=0.1505
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8853, max=0.9961, mean=-0.0000, std=0.1505
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8853, max=0.9961, mean=-0.0000, std=0.1505
[CLAP PIPELINE] Sample 0: 86549/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 730/10586 [03:11<43:05,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.39e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9306, mean=-0.0000, std=0.0315
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9951, max=0.9326, mean=-0.0000, std=0.0315
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9951, max=0.9326, mean=-0.0000, std=0.0315
[CLAP PIPELINE] Sample 0: 133459/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                               | 731/10586 [03:11<43:06,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0011, max=0.9936, mean=0.0003, std=0.1463
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0117, max=0.9497, mean=0.0003, std=0.1462
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0117, max=0.9497, mean=0.0003, std=0.1462
[CLAP PIPELINE] Sample 0: 107501/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                               | 732/10586 [03:12<43:06,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8800, max=0.9998, mean=0.0000, std=0.1742
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8799, max=0.9966, mean=0.0000, std=0.1743
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8799, max=0.9966, mean=0.0000, std=0.1743
[CLAP PIPELINE] Sample 0: 70696/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                               | 733/10586 [03:12<43:05,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8896, max=0.9998, mean=-0.0000, std=0.1505
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8853, max=0.9961, mean=-0.0000, std=0.1505
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8853, max=0.9961, mean=-0.0000, std=0.1505
[CLAP PIPELINE] Sample 0: 86549/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                               | 734/10586 [03:12<43:06,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.4e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9956, max=0.9918, mean=-0.0000, std=0.0923
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7827, max=0.8452, mean=-0.0000, std=0.0874
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7827, max=0.8452, mean=-0.0000, std=0.0874
[CLAP PIPELINE] Sample 0: 132224/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 735/10586 [03:12<43:05,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.41e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0013, max=0.9574, mean=-0.0000, std=0.0927
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9795, max=0.9463, mean=-0.0000, std=0.0925
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9795, max=0.9463, mean=-0.0000, std=0.0925
[CLAP PIPELINE] Sample 0: 90037/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 736/10586 [03:13<43:05,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.41e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0061, max=0.9453, mean=-0.0007, std=0.0940
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9258, max=0.9155, mean=-0.0007, std=0.0937
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9258, max=0.9155, mean=-0.0007, std=0.0937
[CLAP PIPELINE] Sample 0: 129626/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 737/10586 [03:13<43:06,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.41e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9985, max=0.9924, mean=-0.0011, std=0.0672
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.9897, mean=-0.0011, std=0.0671
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.9897, mean=-0.0011, std=0.0671
[CLAP PIPELINE] Sample 0: 114670/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 738/10586 [03:13<43:04,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.41e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0071, max=1.0056, mean=-0.0011, std=0.3486
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0244, max=1.0195, mean=-0.0011, std=0.3486
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0244, max=1.0195, mean=-0.0011, std=0.3486
[CLAP PIPELINE] Sample 0: 41639/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 739/10586 [03:14<43:05,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.42e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.9545, mean=-0.0035, std=0.0987
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0068, max=0.9443, mean=-0.0035, std=0.0986
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0068, max=0.9443, mean=-0.0035, std=0.0986
[CLAP PIPELINE] Sample 0: 97326/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▊                                                                              | 740/10586 [03:14<43:05,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.42e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9044, mean=-0.0008, std=0.2235
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.8965, mean=-0.0008, std=0.2236
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.8965, mean=-0.0008, std=0.2236
[CLAP PIPELINE] Sample 0: 15391/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 741/10586 [03:14<43:04,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.42e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6732, max=0.9998, mean=-0.0004, std=0.2167
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6733, max=0.9995, mean=-0.0004, std=0.2168
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6733, max=0.9995, mean=-0.0004, std=0.2168
[CLAP PIPELINE] Sample 0: 95719/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 742/10586 [03:14<43:04,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.42e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.9650, mean=-0.0000, std=0.1547
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9658, mean=-0.0000, std=0.1548
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9658, mean=-0.0000, std=0.1548
[CLAP PIPELINE] Sample 0: 111893/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 743/10586 [03:14<43:03,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.43e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1372, max=0.8907, mean=-0.0007, std=0.0374
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8027, max=0.6812, mean=-0.0007, std=0.0320
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8027, max=0.6812, mean=-0.0007, std=0.0320
[CLAP PIPELINE] Sample 0: 14937/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 744/10586 [03:15<43:02,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.43e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8681, max=0.9757, mean=0.0000, std=0.1035
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7969, max=0.9199, mean=0.0000, std=0.0999
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7969, max=0.9199, mean=0.0000, std=0.0999
[CLAP PIPELINE] Sample 0: 93552/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 745/10586 [03:15<43:01,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.43e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0000, max=0.8992, mean=-0.0015, std=0.0570
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.8818, mean=-0.0015, std=0.0570
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.8818, mean=-0.0015, std=0.0570
[CLAP PIPELINE] Sample 0: 91147/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 746/10586 [03:15<42:59,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.44e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9782, max=0.6285, mean=-0.0000, std=0.0403
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8354, max=0.4902, mean=-0.0000, std=0.0275
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8354, max=0.4902, mean=-0.0000, std=0.0275
[CLAP PIPELINE] Sample 0: 53132/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 747/10586 [03:15<42:58,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.44e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9751, max=0.9976, mean=-0.0001, std=0.2598
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Sample 0: 25/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 748/10586 [03:16<42:58,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.44e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9860, max=0.6366, mean=-0.0001, std=0.0639
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Sample 0: 91800/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 749/10586 [03:16<42:57,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.44e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9994, max=0.9898, mean=-0.0013, std=0.0614
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9946, max=0.9980, mean=-0.0013, std=0.0609
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9946, max=0.9980, mean=-0.0013, std=0.0609
[CLAP PIPELINE] Sample 0: 108172/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 750/10586 [03:16<42:56,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.45e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9938, max=0.9895, mean=-0.0000, std=0.1246
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.9810, mean=-0.0000, std=0.1244
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.9810, mean=-0.0000, std=0.1244
[CLAP PIPELINE] Sample 0: 49102/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 751/10586 [03:16<42:55,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.45e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9640, max=0.5638, mean=0.0001, std=0.0611
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9619, max=0.5381, mean=0.0001, std=0.0578
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9619, max=0.5381, mean=0.0001, std=0.0578
[CLAP PIPELINE] Sample 0: 64499/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 752/10586 [03:16<42:55,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.45e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8810, max=0.9989, mean=-0.0003, std=0.1860
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8804, max=0.9990, mean=-0.0003, std=0.1860
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8804, max=0.9990, mean=-0.0003, std=0.1860
[CLAP PIPELINE] Sample 0: 6/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 753/10586 [03:17<42:58,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.45e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.8496, mean=-0.0000, std=0.0451
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0: 101891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 754/10586 [03:17<42:59,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.46e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9563, max=1.0000, mean=-0.0002, std=0.0248
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9556, max=0.9941, mean=-0.0002, std=0.0248
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9556, max=0.9941, mean=-0.0002, std=0.0248
[CLAP PIPELINE] Sample 0: 154090/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 755/10586 [03:18<42:59,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.46e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9729, max=0.8797, mean=-0.0026, std=0.1368
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9287, max=0.8765, mean=-0.0026, std=0.1364
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9287, max=0.8765, mean=-0.0026, std=0.1364
[CLAP PIPELINE] Sample 0: 100571/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|█████▉                                                                              | 756/10586 [03:18<42:59,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.46e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9969, max=0.9633, mean=-0.0029, std=0.0779
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9653, max=0.9424, mean=-0.0029, std=0.0772
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9653, max=0.9424, mean=-0.0029, std=0.0772
[CLAP PIPELINE] Sample 0: 124514/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 757/10586 [03:18<42:59,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.46e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9922, max=0.9009, mean=-0.0003, std=0.0580
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9233, max=0.8716, mean=-0.0003, std=0.0569
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9233, max=0.8716, mean=-0.0003, std=0.0569
[CLAP PIPELINE] Sample 0: 141883/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 758/10586 [03:18<42:58,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.47e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9998, mean=-0.0000, std=0.1118
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=1.0000, mean=-0.0000, std=0.1118
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=1.0000, mean=-0.0000, std=0.1118
[CLAP PIPELINE] Sample 0: 151882/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 759/10586 [03:19<42:58,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.47e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0148, max=1.0138, mean=0.0065, std=0.2044
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0078, max=1.0098, mean=0.0065, std=0.2039
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0078, max=1.0098, mean=0.0065, std=0.2039
[CLAP PIPELINE] Sample 0: 81317/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 760/10586 [03:19<42:58,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.47e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9986, max=0.9916, mean=0.0004, std=0.1325
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Sample 0: 99635/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 761/10586 [03:19<42:58,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.47e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1091, max=1.1092, mean=-0.0000, std=0.0830
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.1162, max=1.1260, mean=-0.0000, std=0.0690
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.1162, max=1.1260, mean=-0.0000, std=0.0690
[CLAP PIPELINE] Sample 0: 68539/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 762/10586 [03:19<42:57,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.48e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9985, max=0.9984, mean=0.0027, std=0.4850
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=1.0010, mean=0.0027, std=0.4851
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=1.0010, mean=0.0027, std=0.4851
[CLAP PIPELINE] Sample 0: 12894/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 763/10586 [03:20<42:57,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.48e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6610, max=0.9999, mean=0.0007, std=0.1842
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6611, max=0.9971, mean=0.0007, std=0.1842
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6611, max=0.9971, mean=0.0007, std=0.1842
[CLAP PIPELINE] Sample 0: 118464/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 764/10586 [03:20<42:57,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.48e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0075, max=1.0070, mean=-0.0004, std=0.4867
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0107, max=1.0117, mean=-0.0004, std=0.4866
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0107, max=1.0117, mean=-0.0004, std=0.4866
[CLAP PIPELINE] Sample 0: 95870/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 765/10586 [03:20<42:57,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.48e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9559, max=0.6156, mean=0.0001, std=0.0153
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Sample 0: 150065/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 766/10586 [03:21<42:57,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.49e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0097, max=1.0043, mean=0.0000, std=0.0573
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0127, max=1.0068, mean=0.0000, std=0.0574
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0127, max=1.0068, mean=0.0000, std=0.0574
[CLAP PIPELINE] Sample 0: 49207/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 767/10586 [03:21<42:56,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.49e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9988, max=0.9306, mean=-0.0000, std=0.0315
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9951, max=0.9326, mean=-0.0000, std=0.0315
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9951, max=0.9326, mean=-0.0000, std=0.0315
[CLAP PIPELINE] Sample 0: 133459/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████                                                                              | 768/10586 [03:21<42:55,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.49e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9992, max=0.9834, mean=-0.0002, std=0.0612
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9458, max=0.9844, mean=-0.0002, std=0.0611
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9458, max=0.9844, mean=-0.0002, std=0.0611
[CLAP PIPELINE] Sample 0: 129159/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                              | 769/10586 [03:21<42:55,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9724, max=1.0004, mean=-0.0000, std=0.1376
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9722, max=1.0000, mean=-0.0000, std=0.1376
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9722, max=1.0000, mean=-0.0000, std=0.1376
[CLAP PIPELINE] Sample 0: 67796/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                              | 770/10586 [03:21<42:54,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9206, max=0.9938, mean=-0.0031, std=0.1233
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8921, max=0.9375, mean=-0.0031, std=0.1224
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8921, max=0.9375, mean=-0.0031, std=0.1224
[CLAP PIPELINE] Sample 0: 2/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1832, max=0.1233, mean=-0.0028, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap 1.0000
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                              | 771/10586 [03:22<42:53,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9968, max=0.7898, mean=-0.0000, std=0.0482
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.7783, mean=-0.0000, std=0.0482
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.7783, mean=-0.0000, std=0.0482
[CLAP PIPELINE] Sample 0: 96554/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                              | 772/10586 [03:22<42:54,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9953, max=0.9269, mean=-0.0007, std=0.0796
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Sample 0: 119923/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 773/10586 [03:22<42:54,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0024, max=0.6007, mean=-0.0007, std=0.0930
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Sample 0: 133106/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 774/10586 [03:23<42:53,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9787, max=0.9668, mean=-0.0001, std=0.0864
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9907, max=0.9541, mean=-0.0001, std=0.0863
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9907, max=0.9541, mean=-0.0001, std=0.0863
[CLAP PIPELINE] Sample 0: 123586/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 775/10586 [03:23<42:53,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9991, max=0.9868, mean=-0.0023, std=0.0904
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0039, max=0.9937, mean=-0.0023, std=0.0904
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0039, max=0.9937, mean=-0.0023, std=0.0904
[CLAP PIPELINE] Sample 0: 136496/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 776/10586 [03:23<42:53,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.9657, mean=0.0000, std=0.2225
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9663, mean=0.0000, std=0.2227
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9663, mean=0.0000, std=0.2227
[CLAP PIPELINE] Sample 0: 41608/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 777/10586 [03:23<42:52,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.4047, max=1.3210, mean=-0.0000, std=0.0546
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.2100, max=1.2090, mean=-0.0000, std=0.0390
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.2100, max=1.2090, mean=-0.0000, std=0.0390
[CLAP PIPELINE] Sample 0: 149699/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 778/10586 [03:24<42:52,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9968, max=0.7898, mean=-0.0000, std=0.0482
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.7783, mean=-0.0000, std=0.0482
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.7783, mean=-0.0000, std=0.0482
[CLAP PIPELINE] Sample 0: 96554/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 779/10586 [03:24<42:52,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9011, max=1.0054, mean=-0.0162, std=0.2223
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9009, max=1.0068, mean=-0.0162, std=0.2223
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9009, max=1.0068, mean=-0.0162, std=0.2223
[CLAP PIPELINE] Sample 0: 111860/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 780/10586 [03:24<42:51,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9330, max=0.9979, mean=-0.0002, std=0.1570
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9292, max=0.9971, mean=-0.0002, std=0.1570
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9292, max=0.9971, mean=-0.0002, std=0.1570
[CLAP PIPELINE] Sample 0: 16027/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 781/10586 [03:24<42:51,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=1.0036, mean=-0.0002, std=0.5019
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0859, max=1.0859, mean=-0.0002, std=0.5000
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0859, max=1.0859, mean=-0.0002, std=0.5000
[CLAP PIPELINE] Sample 0: 87995/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 782/10586 [03:24<42:49,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0043, max=0.8394, mean=-0.0000, std=0.0324
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5513, max=0.6128, mean=-0.0000, std=0.0288
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5513, max=0.6128, mean=-0.0000, std=0.0288
[CLAP PIPELINE] Sample 0: 152754/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 783/10586 [03:25<42:48,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9809, max=0.9995, mean=0.0000, std=0.2168
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9902, max=1.0068, mean=0.0000, std=0.2168
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9902, max=1.0068, mean=0.0000, std=0.2168
[CLAP PIPELINE] Sample 0: 63997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 784/10586 [03:25<42:48,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6379, max=0.6203, mean=-0.0099, std=0.0409
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5181, max=0.5645, mean=-0.0099, std=0.0389
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5181, max=0.5645, mean=-0.0099, std=0.0389
[CLAP PIPELINE] Sample 0: 14834/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 785/10586 [03:25<42:48,  3.82it/s, v_num=hdbx, train/loss=nan.0, lr=4.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8866, max=0.9992, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7402, max=0.7954, mean=0.0000, std=0.0335
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7402, max=0.7954, mean=0.0000, std=0.0335
[CLAP PIPELINE] Sample 0: 104330/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 786/10586 [03:26<42:49,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0406, max=0.9617, mean=0.0002, std=0.0605
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0859, max=0.9653, mean=0.0002, std=0.0601
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0859, max=0.9653, mean=0.0002, std=0.0601
[CLAP PIPELINE] Sample 0: 142540/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▏                                                                             | 787/10586 [03:26<42:49,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9998, max=0.9981, mean=0.0033, std=0.3344
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9985, mean=0.0033, std=0.3345
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9985, mean=0.0033, std=0.3345
[CLAP PIPELINE] Sample 0: 95863/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▎                                                                             | 788/10586 [03:26<42:49,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1091, max=1.1092, mean=-0.0000, std=0.0830
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.1162, max=1.1260, mean=-0.0000, std=0.0690
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.1162, max=1.1260, mean=-0.0000, std=0.0690
[CLAP PIPELINE] Sample 0: 68539/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▎                                                                             | 789/10586 [03:26<42:48,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9904, max=0.9976, mean=-0.0010, std=0.1395
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0020, max=0.9868, mean=-0.0010, std=0.1395
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0020, max=0.9868, mean=-0.0010, std=0.1395
[CLAP PIPELINE] Sample 0: 117096/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▎                                                                             | 790/10586 [03:27<42:49,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0013, max=0.6589, mean=-0.0000, std=0.0581
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9780, max=0.6538, mean=-0.0000, std=0.0581
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9780, max=0.6538, mean=-0.0000, std=0.0581
[CLAP PIPELINE] Sample 0: 47224/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▎                                                                             | 791/10586 [03:27<42:48,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6593, max=0.6695, mean=0.0131, std=0.0694
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6108, max=0.6440, mean=0.0131, std=0.0693
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6108, max=0.6440, mean=0.0131, std=0.0693
[CLAP PIPELINE] Sample 0: 95673/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▎                                                                             | 792/10586 [03:27<42:48,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9904, max=0.9976, mean=-0.0010, std=0.1395
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0020, max=0.9868, mean=-0.0010, std=0.1395
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0020, max=0.9868, mean=-0.0010, std=0.1395
[CLAP PIPELINE] Sample 0: 117096/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   7%|██████▎                                                                             | 793/10586 [03:28<42:49,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9991, mean=-0.0000, std=0.2045
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9912, max=0.9946, mean=-0.0000, std=0.2046
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9912, max=0.9946, mean=-0.0000, std=0.2046
[CLAP PIPELINE] Sample 0: 45695/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▎                                                                             | 794/10586 [03:28<42:48,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9782, max=0.6285, mean=-0.0000, std=0.0403
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8354, max=0.4902, mean=-0.0000, std=0.0275
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8354, max=0.4902, mean=-0.0000, std=0.0275
[CLAP PIPELINE] Sample 0: 53132/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▎                                                                             | 795/10586 [03:28<42:49,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9983, max=0.9597, mean=-0.0000, std=0.1789
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9829, max=0.9595, mean=-0.0000, std=0.1788
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9829, max=0.9595, mean=-0.0000, std=0.1788
[CLAP PIPELINE] Sample 0: 72691/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▎                                                                             | 796/10586 [03:28<42:49,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0037, mean=-0.0005, std=0.0341
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8706, max=0.9971, mean=-0.0005, std=0.0319
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8706, max=0.9971, mean=-0.0005, std=0.0319
[CLAP PIPELINE] Sample 0: 141093/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▎                                                                             | 797/10586 [03:29<42:49,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Sample 0: 47237/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▎                                                                             | 798/10586 [03:29<42:49,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8612, max=1.0035, mean=0.0002, std=0.0919
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8564, max=0.9536, mean=0.0002, std=0.0919
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8564, max=0.9536, mean=0.0002, std=0.0919
[CLAP PIPELINE] Sample 0: 96466/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▎                                                                             | 799/10586 [03:29<42:50,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9474, max=1.0005, mean=-0.0014, std=0.0640
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9453, max=0.9980, mean=-0.0014, std=0.0640
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9453, max=0.9980, mean=-0.0014, std=0.0640
[CLAP PIPELINE] Sample 0: 112816/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▎                                                                             | 800/10586 [03:30<42:50,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=1.0100, mean=-0.0020, std=0.3134
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0137, max=1.0205, mean=-0.0020, std=0.3135
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0137, max=1.0205, mean=-0.0020, std=0.3135
[CLAP PIPELINE] Sample 0: 93495/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▎                                                                             | 801/10586 [03:30<42:51,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9766, max=1.0027, mean=-0.0002, std=0.2051
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0107, max=1.0078, mean=-0.0002, std=0.2051
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0107, max=1.0078, mean=-0.0002, std=0.2051
[CLAP PIPELINE] Sample 0: 114878/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▎                                                                             | 802/10586 [03:30<42:50,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9874, max=0.9607, mean=-0.0000, std=0.0566
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.9512, mean=-0.0000, std=0.0565
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.9512, mean=-0.0000, std=0.0565
[CLAP PIPELINE] Sample 0: 106891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▎                                                                             | 803/10586 [03:30<42:50,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9998, max=0.9981, mean=0.0033, std=0.3344
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9985, mean=0.0033, std=0.3345
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9985, mean=0.0033, std=0.3345
[CLAP PIPELINE] Sample 0: 95863/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 804/10586 [03:31<42:50,  3.81it/s, v_num=hdbx, train/loss=nan.0, lr=4.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7753, max=0.9996, mean=0.0000, std=0.0789
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7759, max=1.0000, mean=0.0000, std=0.0790
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7759, max=1.0000, mean=0.0000, std=0.0790
[CLAP PIPELINE] Sample 0: 128887/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 805/10586 [03:31<42:50,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0004, max=0.9938, mean=-0.0000, std=0.0471
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9932, mean=-0.0000, std=0.0471
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9932, mean=-0.0000, std=0.0471
[CLAP PIPELINE] Sample 0: 96561/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 806/10586 [03:31<42:50,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9876, max=1.0462, mean=-0.0003, std=0.0745
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0: 92412/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 807/10586 [03:32<42:50,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9088, max=1.0000, mean=-0.0016, std=0.1978
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9092, max=1.0010, mean=-0.0016, std=0.1980
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9092, max=1.0010, mean=-0.0016, std=0.1980
[CLAP PIPELINE] Sample 0: 95861/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                              | 808/10586 [03:32<42:50,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.2033, max=1.2404, mean=-0.0000, std=0.1116
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5557, max=0.5879, mean=-0.0000, std=0.0728
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.5557, max=0.5879, mean=-0.0000, std=0.0728
[CLAP PIPELINE] Sample 0: 74077/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                              | 809/10586 [03:32<42:51,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9972, max=0.7821, mean=-0.0000, std=0.0092
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9771, max=0.6641, mean=-0.0000, std=0.0087
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9771, max=0.6641, mean=-0.0000, std=0.0087
[CLAP PIPELINE] Sample 0: 151821/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                              | 810/10586 [03:33<42:50,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9851, mean=-0.0003, std=0.0838
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9951, max=0.9873, mean=-0.0003, std=0.0838
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9951, max=0.9873, mean=-0.0003, std=0.0838
[CLAP PIPELINE] Sample 0: 128808/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                              | 811/10586 [03:33<42:51,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9998, max=0.9835, mean=-0.0003, std=0.0390
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0547, max=1.0146, mean=-0.0003, std=0.0382
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0547, max=1.0146, mean=-0.0003, std=0.0382
[CLAP PIPELINE] Sample 0: 134601/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 812/10586 [03:33<42:51,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0258, max=0.8365, mean=-0.0000, std=0.1814
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0439, max=0.8457, mean=-0.0000, std=0.1808
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0439, max=0.8457, mean=-0.0000, std=0.1808
[CLAP PIPELINE] Sample 0: 18076/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 813/10586 [03:33<42:52,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9968, max=0.7898, mean=-0.0000, std=0.0482
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.7783, mean=-0.0000, std=0.0482
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.7783, mean=-0.0000, std=0.0482
[CLAP PIPELINE] Sample 0: 96554/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 814/10586 [03:34<42:53,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=0.7863, mean=-0.0062, std=0.1600
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.7866, mean=-0.0062, std=0.1600
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.7866, mean=-0.0062, std=0.1600
[CLAP PIPELINE] Sample 0: 75095/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 815/10586 [03:34<42:53,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9680, max=0.9256, mean=-0.0028, std=0.1273
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7139, max=0.8584, mean=-0.0028, std=0.1180
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7139, max=0.8584, mean=-0.0028, std=0.1180
[CLAP PIPELINE] Sample 0: 14928/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 816/10586 [03:34<42:52,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9638, max=0.9958, mean=-0.0016, std=0.1701
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9717, max=0.9648, mean=-0.0016, std=0.1698
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9717, max=0.9648, mean=-0.0016, std=0.1698
[CLAP PIPELINE] Sample 0: 82212/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 817/10586 [03:35<42:52,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1685, max=1.2134, mean=0.0000, std=0.0425
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.2793, max=1.2266, mean=0.0000, std=0.0379
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.2793, max=1.2266, mean=0.0000, std=0.0379
[CLAP PIPELINE] Sample 0: 155547/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 818/10586 [03:35<42:52,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1897, max=1.2273, mean=-0.0000, std=0.1331
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9194, max=1.0537, mean=-0.0000, std=0.1292
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9194, max=1.0537, mean=-0.0000, std=0.1292
[CLAP PIPELINE] Sample 0: 65535/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▍                                                                             | 819/10586 [03:35<42:53,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9023, max=1.0001, mean=-0.0002, std=0.0496
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8970, max=0.9990, mean=-0.0002, std=0.0496
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8970, max=0.9990, mean=-0.0002, std=0.0496
[CLAP PIPELINE] Sample 0: 155840/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 820/10586 [03:36<42:52,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0137, max=1.0162, mean=-0.0000, std=0.1196
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9604, max=0.9634, mean=-0.0000, std=0.1135
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9604, max=0.9634, mean=-0.0000, std=0.1135
[CLAP PIPELINE] Sample 0: 151829/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 821/10586 [03:36<42:52,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9991, max=0.9980, mean=0.0001, std=0.3865
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9951, max=0.9985, mean=0.0001, std=0.3867
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9951, max=0.9985, mean=0.0001, std=0.3867
[CLAP PIPELINE] Sample 0: 93390/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 822/10586 [03:36<42:52,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9341, max=0.9025, mean=-0.0009, std=0.0382
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8545, max=0.8267, mean=-0.0009, std=0.0369
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8545, max=0.8267, mean=-0.0009, std=0.0369
[CLAP PIPELINE] Sample 0: 107183/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 823/10586 [03:36<42:51,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0746, max=0.9658, mean=-0.0000, std=0.0424
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6411, max=0.5557, mean=-0.0000, std=0.0312
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6411, max=0.5557, mean=-0.0000, std=0.0312
[CLAP PIPELINE] Sample 0: 126859/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 824/10586 [03:37<42:51,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9880, max=1.0216, mean=0.0000, std=0.1027
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0283, max=1.0322, mean=0.0000, std=0.1027
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0283, max=1.0322, mean=0.0000, std=0.1027
[CLAP PIPELINE] Sample 0: 110215/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 825/10586 [03:37<42:51,  3.80it/s, v_num=hdbx, train/loss=nan.0, lr=4.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0475, max=1.0644, mean=0.0058, std=0.1233
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.1221, max=1.1006, mean=0.0058, std=0.1232
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.1221, max=1.1006, mean=0.0058, std=0.1232
[CLAP PIPELINE] Sample 0: 83194/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 826/10586 [03:27<40:46,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9966, max=1.0181, mean=-0.0001, std=0.0736
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=1.0098, mean=-0.0001, std=0.0735
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=1.0098, mean=-0.0001, std=0.0735
[CLAP PIPELINE] Sample 0: 103091/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 827/10586 [03:27<40:46,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0172, max=0.8247, mean=-0.0000, std=0.0534
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8066, max=0.8003, mean=-0.0000, std=0.0496
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8066, max=0.8003, mean=-0.0000, std=0.0496
[CLAP PIPELINE] Sample 0: 137544/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 828/10586 [03:27<40:46,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.4508, max=0.9956, mean=-0.0000, std=0.0981
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4429, max=0.9536, mean=-0.0000, std=0.0980
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4429, max=0.9536, mean=-0.0000, std=0.0980
[CLAP PIPELINE] Sample 0: 98921/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 829/10586 [03:27<40:45,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9204, mean=-0.0006, std=0.1068
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9995, max=0.9189, mean=-0.0006, std=0.1069
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9995, max=0.9189, mean=-0.0006, std=0.1069
[CLAP PIPELINE] Sample 0: 16644/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 830/10586 [03:28<40:45,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8002, max=1.0032, mean=-0.0002, std=0.0756
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Sample 0: 131203/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 831/10586 [03:28<40:45,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9232, max=1.0015, mean=-0.0001, std=0.1578
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9556, mean=-0.0001, std=0.1578
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9556, mean=-0.0001, std=0.1578
[CLAP PIPELINE] Sample 0: 40567/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 832/10586 [03:28<40:44,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8864, max=1.0000, mean=0.0002, std=0.0629
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8867, max=1.0000, mean=0.0002, std=0.0629
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8867, max=1.0000, mean=0.0002, std=0.0629
[CLAP PIPELINE] Sample 0: 112030/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 833/10586 [03:28<40:43,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9985, max=0.9924, mean=-0.0011, std=0.0672
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.9897, mean=-0.0011, std=0.0671
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.9897, mean=-0.0011, std=0.0671
[CLAP PIPELINE] Sample 0: 114670/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▌                                                                             | 834/10586 [03:29<40:44,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9835, max=0.8843, mean=-0.0000, std=0.2600
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9609, max=0.8481, mean=-0.0000, std=0.2581
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9609, max=0.8481, mean=-0.0000, std=0.2581
[CLAP PIPELINE] Sample 0: 26255/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 835/10586 [03:29<40:43,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=0.8610, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Sample 0: 148709/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 836/10586 [03:29<40:43,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8718, max=0.9969, mean=-0.0000, std=0.2954
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8662, max=0.9927, mean=-0.0000, std=0.2954
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8662, max=0.9927, mean=-0.0000, std=0.2954
[CLAP PIPELINE] Sample 0: 80235/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 837/10586 [03:29<40:43,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0011, max=0.9936, mean=0.0003, std=0.1463
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0117, max=0.9497, mean=0.0003, std=0.1462
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0117, max=0.9497, mean=0.0003, std=0.1462
[CLAP PIPELINE] Sample 0: 107501/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 838/10586 [03:29<40:42,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9835, max=0.8843, mean=-0.0000, std=0.2600
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9609, max=0.8481, mean=-0.0000, std=0.2581
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9609, max=0.8481, mean=-0.0000, std=0.2581
[CLAP PIPELINE] Sample 0: 26255/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 839/10586 [03:30<40:41,  3.99it/s, v_num=hdbx, train/loss=nan.0, lr=4.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1497, max=1.2149, mean=-0.0000, std=0.0694
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8442, max=0.9546, mean=-0.0000, std=0.0503
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8442, max=0.9546, mean=-0.0000, std=0.0503
[CLAP PIPELINE] Sample 0: 87007/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 840/10586 [03:30<40:36,  4.00it/s, v_num=hdbx, train/loss=nan.0, lr=4.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0019, max=0.7700, mean=0.0000, std=0.0997
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0020, max=0.7661, mean=0.0000, std=0.0997
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0020, max=0.7661, mean=0.0000, std=0.0997
[CLAP PIPELINE] Sample 0: 56540/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 841/10586 [03:30<40:35,  4.00it/s, v_num=hdbx, train/loss=nan.0, lr=4.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0070, max=0.7997, mean=-0.0002, std=0.0171
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0059, max=0.7822, mean=-0.0002, std=0.0169
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0059, max=0.7822, mean=-0.0002, std=0.0169
[CLAP PIPELINE] Sample 0: 42218/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 842/10586 [03:30<40:34,  4.00it/s, v_num=hdbx, train/loss=nan.0, lr=4.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9295, max=0.9991, mean=-0.0002, std=0.2361
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=1.0010, mean=-0.0002, std=0.2361
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=1.0010, mean=-0.0002, std=0.2361
[CLAP PIPELINE] Sample 0: 63764/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 843/10586 [03:30<40:32,  4.00it/s, v_num=hdbx, train/loss=nan.0, lr=4.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0308, max=1.0208, mean=0.0000, std=0.2349
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0732, max=1.0781, mean=0.0000, std=0.2347
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0732, max=1.0781, mean=0.0000, std=0.2347
[CLAP PIPELINE] Sample 0: 110848/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 844/10586 [03:30<40:32,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1497, max=1.2149, mean=-0.0000, std=0.0694
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8442, max=0.9546, mean=-0.0000, std=0.0503
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8442, max=0.9546, mean=-0.0000, std=0.0503
[CLAP PIPELINE] Sample 0: 87007/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 845/10586 [03:30<40:31,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0008, max=0.6007, mean=0.0002, std=0.0658
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=0.5972, mean=0.0002, std=0.0658
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=0.5972, mean=0.0002, std=0.0658
[CLAP PIPELINE] Sample 0: 92790/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                              | 846/10586 [03:31<40:29,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0013, max=0.6589, mean=-0.0000, std=0.0581
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9780, max=0.6538, mean=-0.0000, std=0.0581
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9780, max=0.6538, mean=-0.0000, std=0.0581
[CLAP PIPELINE] Sample 0: 47224/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                              | 847/10586 [03:31<40:28,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.3156, max=1.3559, mean=0.0001, std=0.0703
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.2871, max=1.2734, mean=0.0001, std=0.0539
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.2871, max=1.2734, mean=0.0001, std=0.0539
[CLAP PIPELINE] Sample 0: 133276/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                              | 848/10586 [03:31<40:28,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0533, max=1.0435, mean=0.0000, std=0.0677
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0068, max=1.0635, mean=0.0000, std=0.0674
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0068, max=1.0635, mean=0.0000, std=0.0674
[CLAP PIPELINE] Sample 0: 26197/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                              | 849/10586 [03:31<40:27,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9536, max=0.9943, mean=-0.0000, std=0.1444
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9395, max=0.9805, mean=-0.0000, std=0.1434
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9805, mean=-0.0000, std=0.1434
[CLAP PIPELINE] Sample 0: 12/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▋                                                                             | 850/10586 [03:31<40:26,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9876, max=1.0462, mean=-0.0003, std=0.0745
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Sample 0: 92412/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 851/10586 [03:32<40:25,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6316, max=0.9994, mean=0.0004, std=0.1151
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6289, max=0.9985, mean=0.0004, std=0.1151
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6289, max=0.9985, mean=0.0004, std=0.1151
[CLAP PIPELINE] Sample 0: 137931/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 852/10586 [03:32<40:25,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0475, max=1.0644, mean=0.0058, std=0.1233
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.1221, max=1.1006, mean=0.0058, std=0.1232
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.1221, max=1.1006, mean=0.0058, std=0.1232
[CLAP PIPELINE] Sample 0: 83194/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 853/10586 [03:32<40:25,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7937, max=1.0061, mean=-0.0000, std=0.1177
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7939, max=0.9849, mean=-0.0000, std=0.1177
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7939, max=0.9849, mean=-0.0000, std=0.1177
[CLAP PIPELINE] Sample 0: 39820/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 854/10586 [03:32<40:25,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0461, max=1.0268, mean=-0.0000, std=0.0511
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0977, max=1.1162, mean=-0.0000, std=0.0461
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0977, max=1.1162, mean=-0.0000, std=0.0461
[CLAP PIPELINE] Sample 0: 109767/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 855/10586 [03:33<40:25,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9433, max=0.9782, mean=-0.0005, std=0.0523
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8989, max=0.8970, mean=-0.0005, std=0.0514
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8989, max=0.8970, mean=-0.0005, std=0.0514
[CLAP PIPELINE] Sample 0: 129997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 856/10586 [03:33<40:25,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9603, max=0.9986, mean=-0.0015, std=0.1803
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9575, max=0.9985, mean=-0.0015, std=0.1803
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9575, max=0.9985, mean=-0.0015, std=0.1803
[CLAP PIPELINE] Sample 0: 121818/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 857/10586 [03:33<40:25,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9985, max=0.9924, mean=-0.0011, std=0.0672
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.9897, mean=-0.0011, std=0.0671
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.9897, mean=-0.0011, std=0.0671
[CLAP PIPELINE] Sample 0: 114670/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 858/10586 [03:33<40:24,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8943, max=0.9992, mean=0.0039, std=0.1649
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8853, max=0.9937, mean=0.0039, std=0.1647
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8853, max=0.9937, mean=0.0039, std=0.1647
[CLAP PIPELINE] Sample 0: 0/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1704, max=0.1110, mean=-0.0024, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap 1.0000
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 859/10586 [03:34<40:24,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6432, max=0.9897, mean=-0.0002, std=0.1123
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6323, max=0.9502, mean=-0.0002, std=0.1122
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6323, max=0.9502, mean=-0.0002, std=0.1122
[CLAP PIPELINE] Sample 0: 75744/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 860/10586 [03:34<40:23,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9161, max=0.8852, mean=-0.0002, std=0.0118
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8423, max=0.8545, mean=-0.0002, std=0.0112
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8423, max=0.8545, mean=-0.0002, std=0.0112
[CLAP PIPELINE] Sample 0: 152915/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 861/10586 [03:34<40:22,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9638, max=0.9958, mean=-0.0016, std=0.1701
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9717, max=0.9648, mean=-0.0016, std=0.1698
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9717, max=0.9648, mean=-0.0016, std=0.1698
[CLAP PIPELINE] Sample 0: 82212/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 862/10586 [03:34<40:22,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0280, max=1.1183, mean=0.0004, std=0.0506
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8228, max=0.7773, mean=0.0004, std=0.0494
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8228, max=0.7773, mean=0.0004, std=0.0494
[CLAP PIPELINE] Sample 0: 48179/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 863/10586 [03:34<40:21,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6610, max=0.9999, mean=0.0007, std=0.1842
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6611, max=0.9971, mean=0.0007, std=0.1842
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6611, max=0.9971, mean=0.0007, std=0.1842
[CLAP PIPELINE] Sample 0: 118464/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 864/10586 [03:35<40:21,  4.01it/s, v_num=hdbx, train/loss=nan.0, lr=4.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9964, max=0.9869, mean=-0.0000, std=0.1666
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9966, max=0.9766, mean=-0.0000, std=0.1665
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9966, max=0.9766, mean=-0.0000, std=0.1665
[CLAP PIPELINE] Sample 0: 33631/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 865/10586 [03:35<40:17,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.9380, mean=-0.0001, std=0.0279
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9917, max=0.9292, mean=-0.0001, std=0.0279
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9917, max=0.9292, mean=-0.0001, std=0.0279
[CLAP PIPELINE] Sample 0: 143526/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▊                                                                             | 866/10586 [03:35<40:16,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.8496, mean=-0.0000, std=0.0451
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Sample 0: 101891/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 867/10586 [03:35<40:15,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8927, max=1.0000, mean=0.0000, std=0.0786
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8281, max=0.7896, mean=0.0000, std=0.0775
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8281, max=0.7896, mean=0.0000, std=0.0775
[CLAP PIPELINE] Sample 0: 150521/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 868/10586 [03:35<40:14,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9057, max=0.9394, mean=0.0000, std=0.0707
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Sample 0: 51621/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 869/10586 [03:35<40:14,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9232, max=1.0015, mean=-0.0001, std=0.1578
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9556, mean=-0.0001, std=0.1578
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9556, mean=-0.0001, std=0.1578
[CLAP PIPELINE] Sample 0: 40567/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 870/10586 [03:36<40:14,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8497, max=0.9994, mean=-0.0007, std=0.2589
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8501, max=1.0000, mean=-0.0008, std=0.2590
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=1.0000, mean=-0.0008, std=0.2590
[CLAP PIPELINE] Sample 0: 86957/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 871/10586 [03:36<40:14,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0001, max=0.9844, mean=-0.0005, std=0.0555
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9043, max=0.9102, mean=-0.0005, std=0.0554
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9043, max=0.9102, mean=-0.0005, std=0.0554
[CLAP PIPELINE] Sample 0: 130368/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 872/10586 [03:36<40:14,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8612, max=1.0035, mean=0.0002, std=0.0919
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8564, max=0.9536, mean=0.0002, std=0.0919
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8564, max=0.9536, mean=0.0002, std=0.0919
[CLAP PIPELINE] Sample 0: 96466/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 873/10586 [03:36<40:14,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9433, max=0.9782, mean=-0.0005, std=0.0523
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8989, max=0.8970, mean=-0.0005, std=0.0514
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8989, max=0.8970, mean=-0.0005, std=0.0514
[CLAP PIPELINE] Sample 0: 129997/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 874/10586 [03:37<40:14,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9465, max=0.9757, mean=-0.0003, std=0.2705
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0479, max=1.0293, mean=-0.0003, std=0.2649
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0479, max=1.0293, mean=-0.0003, std=0.2649
[CLAP PIPELINE] Sample 0: 95846/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 875/10586 [03:37<40:13,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9699, max=0.7890, mean=-0.0001, std=0.0202
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9023, max=0.7676, mean=-0.0001, std=0.0188
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9023, max=0.7676, mean=-0.0001, std=0.0188
[CLAP PIPELINE] Sample 0: 117554/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 876/10586 [03:37<40:13,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0940, max=1.0025, mean=0.0008, std=0.0431
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0020, max=0.9111, mean=0.0008, std=0.0389
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0020, max=0.9111, mean=0.0008, std=0.0389
[CLAP PIPELINE] Sample 0: 19021/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 877/10586 [03:37<40:12,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8254, max=0.9992, mean=0.0000, std=0.1543
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8223, max=0.9795, mean=0.0000, std=0.1544
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8223, max=0.9795, mean=0.0000, std=0.1544
[CLAP PIPELINE] Sample 0: 45016/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 878/10586 [03:38<40:12,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1485, max=1.0776, mean=0.0001, std=0.1704
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0498, max=1.0098, mean=0.0001, std=0.1689
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0498, max=1.0098, mean=0.0001, std=0.1689
[CLAP PIPELINE] Sample 0: 42190/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 879/10586 [03:38<40:12,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9487, max=0.9851, mean=0.0053, std=0.2636
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9243, max=0.9644, mean=0.0053, std=0.2637
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9243, max=0.9644, mean=0.0053, std=0.2637
[CLAP PIPELINE] Sample 0: 124477/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 880/10586 [03:38<40:13,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9993, max=0.8699, mean=-0.0019, std=0.1727
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.8672, mean=-0.0019, std=0.1727
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.8672, mean=-0.0019, std=0.1727
[CLAP PIPELINE] Sample 0: 94052/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 881/10586 [03:39<40:13,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9986, mean=-0.0003, std=0.0470
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8184, max=0.8174, mean=-0.0003, std=0.0450
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8184, max=0.8174, mean=-0.0003, std=0.0450
[CLAP PIPELINE] Sample 0: 146392/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|██████▉                                                                             | 882/10586 [03:39<40:12,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8524, max=1.0104, mean=0.0031, std=0.1023
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8569, max=0.9546, mean=0.0031, std=0.1024
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8569, max=0.9546, mean=0.0031, std=0.1024
[CLAP PIPELINE] Sample 0: 129716/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|███████                                                                             | 883/10586 [03:39<40:12,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9624, max=1.0002, mean=-0.0004, std=0.1298
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Sample 0: 144175/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|███████                                                                             | 884/10586 [03:39<40:12,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9427, max=1.0070, mean=-0.0001, std=0.0416
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7642, max=0.6187, mean=-0.0001, std=0.0379
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7642, max=0.6187, mean=-0.0001, std=0.0379
[CLAP PIPELINE] Sample 0: 46123/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|███████                                                                              | 885/10586 [03:40<40:12,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9532, max=0.9568, mean=0.0000, std=0.0080
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8169, max=0.9629, mean=0.0000, std=0.0076
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.8169, max=0.9629, mean=0.0000, std=0.0076
[CLAP PIPELINE] Sample 0: 158047/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   8%|███████                                                                              | 886/10586 [03:40<40:12,  4.02it/s, v_num=hdbx, train/loss=nan.0, lr=4.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9035, mean=-0.0000, std=0.1785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Sample 0: 78920/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im nan, y_ref nan, y_clap nan
[SIMILARITY] Temps: tau nan, cross_temp nan
[SIMILARITY] WARNING: y_im has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_ref has 512/512 NaN and 0/512 Inf
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=nan, max=nan, mean=nan
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_qvim_log has 1/1 NaN values
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values

Detected KeyboardInterrupt, attempting graceful shutdown ...
