Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs

----- Training -----
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name              | Type                               | Params | Mode
---------------------------------------------------------------------------------
0 | mel               | AugmentMelSTFT                     | 0      | train
1 | imitation_encoder | MobileNetV3                        | 2.7 M  | train
2 | reference_encoder | MobileNetV3                        | 2.7 M  | train
3 | clap_model        | CLAPAudioEmbeddingClassifierFreev2 | 158 M  | eval
  | other params      | n/a                                | 2      | n/a
---------------------------------------------------------------------------------
5.4 M     Trainable params
158 M     Non-trainable params
163 M     Total params
655.085   Total estimated model params size (MB)
419       Modules in train mode
465       Modules in eval mode
Sanity Checking DataLoader 0:   0%|                                                                                                                       | 0/2 [00:00<?, ?it/s]
[VALIDATION] Batch 0 - Processing validation data
[VALIDATION] Imitation shape: torch.Size([1, 320000]), Reference shape: torch.Size([1, 320000])
[VALIDATION] Imitation: min=-0.8460, max=1.0018
[VALIDATION] Reference: min=-0.9751, max=0.9976
[CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9751, max=0.9976, mean=-0.0001, std=0.2598
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Removed 0 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Sample 0: 25/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[VALIDATION] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[VALIDATION] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[VALIDATION] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[VALIDATION] WARNING: NaN found in log_softmax outputs:
[VALIDATION]   - C_qvim_log: 0/1 NaN values
[VALIDATION]   - C_ref_clap_log: 1/1 NaN values
[VALIDATION]   - C_im_clap_log: 1/1 NaN values
Sanity Checking DataLoader 0:  50%|███████████████████████████████████████████████████████▌                                                       | 1/2 [00:01<00:01,  0.61it/s]
[VALIDATION] Batch 1 - Processing validation data
[VALIDATION] Imitation shape: torch.Size([1, 320000]), Reference shape: torch.Size([1, 320000])
[VALIDATION] Imitation: min=-0.9942, max=0.9894
[VALIDATION] Reference: min=-0.9515, max=1.0022
[CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Removed 150802 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([9198])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0003, std=0.0709
[CLAP PIPELINE] Sample 0: 11/9198 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[VALIDATION] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[VALIDATION] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[VALIDATION] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[VALIDATION] WARNING: NaN found in log_softmax outputs:
[VALIDATION]   - C_qvim_log: 0/1 NaN values
[VALIDATION]   - C_ref_clap_log: 1/1 NaN values
[VALIDATION]   - C_im_clap_log: 1/1 NaN values
Epoch 0:   0%|                                                                                                                                        | 0/10586 [00:00<?, ?it/s][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9448, max=0.7345, mean=0.0000, std=0.0662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8896, max=0.5645, mean=0.0000, std=0.0489
[CLAP PIPELINE] Removed 75421 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([84579])
[CLAP PIPELINE] Sample 0 stats: min=-0.8896, max=0.5645, mean=0.0001, std=0.0673
[CLAP PIPELINE] Sample 0: 0/84579 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                     | 1/10586 [00:02<8:06:38,  0.36it/s, v_num=n7n6, train/loss=nan.0, lr=2.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8902, max=1.0016, mean=-0.0000, std=0.1195
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8936, max=1.0020, mean=-0.0000, std=0.1195
[CLAP PIPELINE] Removed 117084 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([42916])
[CLAP PIPELINE] Sample 0 stats: min=-0.8936, max=1.0020, mean=-0.0002, std=0.2308
[CLAP PIPELINE] Sample 0: 7/42916 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                     | 2/10586 [00:03<4:36:55,  0.64it/s, v_num=n7n6, train/loss=nan.0, lr=2.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9285, max=0.9992, mean=-0.0009, std=0.0743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Removed 129717 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([30283])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=1.0010, mean=-0.0048, std=0.1708
[CLAP PIPELINE] Sample 0: 2/30283 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 3/10586 [00:03<3:20:01,  0.88it/s, v_num=n7n6, train/loss=nan.0, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9876, max=1.0462, mean=-0.0003, std=0.0745
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Removed 92399 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([67601])
[CLAP PIPELINE] Sample 0 stats: min=-0.9731, max=0.9883, mean=-0.0008, std=0.1135
[CLAP PIPELINE] Sample 0: 13/67601 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 4/10586 [00:03<2:45:06,  1.07it/s, v_num=n7n6, train/loss=nan.0, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0043, max=0.8394, mean=-0.0000, std=0.0324
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5513, max=0.6128, mean=-0.0000, std=0.0288
[CLAP PIPELINE] Removed 152753 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([7247])
[CLAP PIPELINE] Sample 0 stats: min=-0.5513, max=0.6128, mean=-0.0000, std=0.1353
[CLAP PIPELINE] Sample 0: 2/7247 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 5/10586 [00:04<2:25:14,  1.21it/s, v_num=n7n6, train/loss=nan.0, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8219, max=0.8306, mean=-0.0000, std=0.0854
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7202, max=0.7139, mean=-0.0000, std=0.0742
[CLAP PIPELINE] Removed 0 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7202, max=0.7139, mean=-0.0000, std=0.0742
[CLAP PIPELINE] Sample 0: 7/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1771, max=0.1289, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.9661, max=-0.9661, mean=-0.9661
[SIMILARITY] C_im_clap: min=-0.9330, max=-0.9330, mean=-0.9330
Epoch 0:   0%|                                                                                    | 6/10586 [00:04<2:14:50,  1.31it/s, v_num=n7n6, train/loss=-0.00, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9915, max=0.7667, mean=-0.0008, std=0.0252
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Removed 104632 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([55368])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.6553, mean=-0.0024, std=0.0429
[CLAP PIPELINE] Sample 0: 7/55368 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 7/10586 [00:05<2:06:16,  1.40it/s, v_num=n7n6, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.9017, mean=0.0002, std=0.1192
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9956, max=0.9019, mean=0.0002, std=0.1191
[CLAP PIPELINE] Removed 131310 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([28690])
[CLAP PIPELINE] Sample 0 stats: min=-0.9956, max=0.9019, mean=0.0008, std=0.2815
[CLAP PIPELINE] Sample 0: 2/28690 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 8/10586 [00:05<1:55:40,  1.52it/s, v_num=n7n6, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9986, max=0.9916, mean=0.0004, std=0.1325
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Removed 99636 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([60364])
[CLAP PIPELINE] Sample 0 stats: min=-0.9878, max=0.9771, mean=0.0012, std=0.2157
[CLAP PIPELINE] Sample 0: 0/60364 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 9/10586 [00:05<1:48:54,  1.62it/s, v_num=n7n6, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9860, max=0.6366, mean=-0.0001, std=0.0639
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Removed 91799 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([68201])
[CLAP PIPELINE] Sample 0 stats: min=-0.8730, max=0.5918, mean=-0.0002, std=0.0954
[CLAP PIPELINE] Sample 0: 1/68201 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 10/10586 [00:05<1:43:34,  1.70it/s, v_num=n7n6, train/loss=nan.0, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9877, max=0.9983, mean=0.0004, std=0.0661
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Removed 87726 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([72274])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9971, mean=0.0009, std=0.0984
[CLAP PIPELINE] Sample 0: 1/72274 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 11/10586 [00:06<1:40:09,  1.76it/s, v_num=n7n6, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8575, mean=-0.0000, std=0.0995
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Removed 66889 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([93111])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.1300
[CLAP PIPELINE] Sample 0: 4/93111 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 12/10586 [00:06<1:35:50,  1.84it/s, v_num=n7n6, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8927, max=1.0000, mean=0.0000, std=0.0786
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8281, max=0.7896, mean=0.0000, std=0.0775
[CLAP PIPELINE] Removed 150522 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([9478])
[CLAP PIPELINE] Sample 0 stats: min=-0.8281, max=0.7896, mean=0.0000, std=0.3184
[CLAP PIPELINE] Sample 0: 0/9478 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 13/10586 [00:06<1:33:21,  1.89it/s, v_num=n7n6, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=1.0032, mean=-0.0030, std=0.1216
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9224, max=0.9990, mean=-0.0030, std=0.1213
[CLAP PIPELINE] Removed 123214 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([36786])
[CLAP PIPELINE] Sample 0 stats: min=-0.9224, max=0.9990, mean=-0.0131, std=0.2529
[CLAP PIPELINE] Sample 0: 1/36786 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 14/10586 [00:07<1:30:08,  1.95it/s, v_num=n7n6, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6593, max=0.6695, mean=0.0131, std=0.0694
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6108, max=0.6440, mean=0.0131, std=0.0693
[CLAP PIPELINE] Removed 95672 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([64328])
[CLAP PIPELINE] Sample 0 stats: min=-0.6108, max=0.6440, mean=0.0327, std=0.1064
[CLAP PIPELINE] Sample 0: 3/64328 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                   | 15/10586 [00:07<1:27:50,  2.01it/s, v_num=n7n6, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8606, max=1.0119, mean=0.0192, std=0.2174
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8516, max=0.9956, mean=0.0192, std=0.2169
[CLAP PIPELINE] Removed 3117 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([156883])
[CLAP PIPELINE] Sample 0 stats: min=-0.8516, max=0.9956, mean=0.0196, std=0.2191
[CLAP PIPELINE] Sample 0: 1/156883 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2160, max=14.2160, mean=14.2160
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 16/10586 [00:07<1:26:04,  2.05it/s, v_num=n7n6, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9965, mean=-0.0004, std=0.1406
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Removed 27044 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([132956])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1542
[CLAP PIPELINE] Sample 0: 3/132956 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 17/10586 [00:08<1:24:18,  2.09it/s, v_num=n7n6, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9966, max=1.0181, mean=-0.0001, std=0.0736
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=1.0098, mean=-0.0001, std=0.0735
[CLAP PIPELINE] Removed 103089 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([56911])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=1.0098, mean=-0.0002, std=0.1234
[CLAP PIPELINE] Sample 0: 3/56911 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 18/10586 [00:08<1:23:37,  2.11it/s, v_num=n7n6, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8297, max=1.0162, mean=-0.0000, std=0.0370
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Removed 64230 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([95770])
[CLAP PIPELINE] Sample 0 stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0478
[CLAP PIPELINE] Sample 0: 228/95770 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 19/10586 [00:08<1:21:46,  2.15it/s, v_num=n7n6, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9877, max=0.9983, mean=0.0004, std=0.0661
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Removed 87726 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([72274])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9971, mean=0.0009, std=0.0984
[CLAP PIPELINE] Sample 0: 1/72274 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 20/10586 [00:09<1:19:38,  2.21it/s, v_num=n7n6, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9285, max=0.9992, mean=-0.0009, std=0.0743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Removed 129717 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([30283])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=1.0010, mean=-0.0048, std=0.1708
[CLAP PIPELINE] Sample 0: 2/30283 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 21/10586 [00:09<1:18:18,  2.25it/s, v_num=n7n6, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9624, max=1.0002, mean=-0.0004, std=0.1298
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Removed 144171 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([15829])
[CLAP PIPELINE] Sample 0 stats: min=-0.9771, max=0.9927, mean=-0.0045, std=0.4124
[CLAP PIPELINE] Sample 0: 7/15829 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 22/10586 [00:09<1:16:58,  2.29it/s, v_num=n7n6, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0033, max=1.0005, mean=-0.0001, std=0.0460
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9839, max=0.9712, mean=-0.0001, std=0.0460
[CLAP PIPELINE] Removed 154783 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([5217])
[CLAP PIPELINE] Sample 0 stats: min=-0.9839, max=0.9712, mean=-0.0043, std=0.2546
[CLAP PIPELINE] Sample 0: 0/5217 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1592, max=0.1164, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.9312, max=-0.9312, mean=-0.9312
[SIMILARITY] C_im_clap: min=-0.9242, max=-0.9242, mean=-0.9242
Epoch 0:   0%|▏                                                                                  | 23/10586 [00:09<1:15:44,  2.32it/s, v_num=n7n6, train/loss=-0.00, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9982, max=0.8683, mean=-0.0054, std=0.0859
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.8677, mean=-0.0054, std=0.0859
[CLAP PIPELINE] Removed 127115 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([32885])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.8677, mean=-0.0261, std=0.1880
[CLAP PIPELINE] Sample 0: 155/32885 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 24/10586 [00:10<1:15:37,  2.33it/s, v_num=n7n6, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0084, mean=-0.0001, std=0.1935
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Removed 39111 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([120889])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.2036
[CLAP PIPELINE] Sample 0: 1/120889 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 25/10586 [00:10<1:14:52,  2.35it/s, v_num=n7n6, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0007, max=0.9729, mean=-0.0113, std=0.4417
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.9702, mean=-0.0113, std=0.4414
[CLAP PIPELINE] Removed 20050 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([139950])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.9702, mean=-0.0129, std=0.4719
[CLAP PIPELINE] Sample 0: 0/139950 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 26/10586 [00:10<1:13:41,  2.39it/s, v_num=n7n6, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9992, max=0.9896, mean=-0.0005, std=0.0853
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9897, mean=-0.0005, std=0.0853
[CLAP PIPELINE] Removed 137707 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([22293])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9897, mean=-0.0034, std=0.2285
[CLAP PIPELINE] Sample 0: 122/22293 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 27/10586 [00:11<1:12:58,  2.41it/s, v_num=n7n6, train/loss=nan.0, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9676, max=1.0002, mean=-0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Removed 25925 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([134075])
[CLAP PIPELINE] Sample 0 stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1438
[CLAP PIPELINE] Sample 0: 8/134075 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 28/10586 [00:11<1:11:53,  2.45it/s, v_num=n7n6, train/loss=nan.0, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9931, max=0.9308, mean=-0.0000, std=0.1076
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9946, max=0.9312, mean=-0.0000, std=0.1076
[CLAP PIPELINE] Removed 50682 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([109318])
[CLAP PIPELINE] Sample 0 stats: min=-0.9946, max=0.9312, mean=-0.0000, std=0.1302
[CLAP PIPELINE] Sample 0: 0/109318 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 29/10586 [00:11<1:11:01,  2.48it/s, v_num=n7n6, train/loss=nan.0, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7830, max=0.7911, mean=-0.0000, std=0.0254
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.2476, max=0.2393, mean=-0.0000, std=0.0161
[CLAP PIPELINE] Removed 63026 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([96974])
[CLAP PIPELINE] Sample 0 stats: min=-0.2476, max=0.2393, mean=-0.0000, std=0.0207
[CLAP PIPELINE] Sample 0: 101/96974 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 30/10586 [00:12<1:10:58,  2.48it/s, v_num=n7n6, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8381, max=1.0082, mean=0.0006, std=0.1664
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8252, max=0.9849, mean=0.0006, std=0.1655
[CLAP PIPELINE] Removed 88664 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([71336])
[CLAP PIPELINE] Sample 0 stats: min=-0.8252, max=0.9849, mean=0.0013, std=0.2479
[CLAP PIPELINE] Sample 0: 2/71336 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                  | 31/10586 [00:12<1:11:01,  2.48it/s, v_num=n7n6, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8002, max=1.0032, mean=-0.0002, std=0.0756
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Removed 131203 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([28797])
[CLAP PIPELINE] Sample 0 stats: min=-0.7935, max=1.0176, mean=-0.0013, std=0.1779
[CLAP PIPELINE] Sample 0: 0/28797 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 32/10586 [00:12<1:10:22,  2.50it/s, v_num=n7n6, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8584, max=0.9898, mean=-0.0015, std=0.2105
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8594, max=0.8374, mean=-0.0015, std=0.2020
[CLAP PIPELINE] Removed 104008 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([55992])
[CLAP PIPELINE] Sample 0 stats: min=-0.8594, max=0.8374, mean=-0.0043, std=0.3416
[CLAP PIPELINE] Sample 0: 1/55992 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 33/10586 [00:13<1:09:37,  2.53it/s, v_num=n7n6, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8739, max=0.9986, mean=-0.0000, std=0.0380
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Removed 59395 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([100605])
[CLAP PIPELINE] Sample 0 stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0475
[CLAP PIPELINE] Sample 0: 2090/100605 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 34/10586 [00:13<1:09:43,  2.52it/s, v_num=n7n6, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8882, max=0.8452, mean=-0.0001, std=0.0777
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5093, max=0.4824, mean=-0.0001, std=0.0479
[CLAP PIPELINE] Removed 63203 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([96797])
[CLAP PIPELINE] Sample 0 stats: min=-0.5093, max=0.4824, mean=-0.0002, std=0.0616
[CLAP PIPELINE] Sample 0: 1/96797 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 35/10586 [00:13<1:08:55,  2.55it/s, v_num=n7n6, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9034, max=1.0000, mean=-0.0000, std=0.1119
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Removed 63472 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([96528])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9868, mean=-0.0001, std=0.1442
[CLAP PIPELINE] Sample 0: 2/96528 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 36/10586 [00:14<1:08:27,  2.57it/s, v_num=n7n6, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1079, max=1.1765, mean=-0.0007, std=0.1651
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.1914, max=1.1514, mean=-0.0007, std=0.1642
[CLAP PIPELINE] Removed 6092 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([153908])
[CLAP PIPELINE] Sample 0 stats: min=-1.1914, max=1.1514, mean=-0.0007, std=0.1675
[CLAP PIPELINE] Sample 0: 5/153908 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 37/10586 [00:14<1:07:37,  2.60it/s, v_num=n7n6, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9734, max=0.9692, mean=-0.0008, std=0.4332
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=1.0049, mean=-0.0008, std=0.4280
[CLAP PIPELINE] Removed 88005 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([71995])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=1.0049, mean=-0.0017, std=0.6377
[CLAP PIPELINE] Sample 0: 0/71995 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                   | 38/10586 [00:14<1:07:08,  2.62it/s, v_num=n7n6, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Removed 150802 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([9198])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0003, std=0.0709
[CLAP PIPELINE] Sample 0: 11/9198 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2229, max=14.2229, mean=14.2229
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                   | 39/10586 [00:14<1:06:34,  2.64it/s, v_num=n7n6, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.7777, mean=0.0000, std=0.0404
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.7783, mean=0.0000, std=0.0404
[CLAP PIPELINE] Removed 112894 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([47106])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.7783, mean=0.0001, std=0.0745
[CLAP PIPELINE] Sample 0: 362/47106 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                   | 40/10586 [00:15<1:06:13,  2.65it/s, v_num=n7n6, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0082, max=0.9617, mean=-0.0000, std=0.0235
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6812, max=0.6206, mean=-0.0000, std=0.0112
[CLAP PIPELINE] Removed 0 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6812, max=0.6206, mean=-0.0000, std=0.0112
[CLAP PIPELINE] Sample 0: 655/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                   | 41/10586 [00:15<1:06:04,  2.66it/s, v_num=n7n6, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9410, max=0.9987, mean=0.0008, std=0.1114
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8623, max=0.8574, mean=0.0008, std=0.1111
[CLAP PIPELINE] Removed 129901 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([30099])
[CLAP PIPELINE] Sample 0 stats: min=-0.8623, max=0.8574, mean=0.0042, std=0.2561
[CLAP PIPELINE] Sample 0: 0/30099 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 42/10586 [00:15<1:05:32,  2.68it/s, v_num=n7n6, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8091, mean=-0.0016, std=0.1717
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9570, max=0.8037, mean=-0.0016, std=0.1718
[CLAP PIPELINE] Removed 84577 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([75423])
[CLAP PIPELINE] Sample 0 stats: min=-0.9570, max=0.8037, mean=-0.0034, std=0.2500
[CLAP PIPELINE] Sample 0: 194/75423 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 43/10586 [00:16<1:05:27,  2.68it/s, v_num=n7n6, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8915, max=0.9999, mean=-0.0001, std=0.0858
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8911, max=1.0000, mean=-0.0001, std=0.0858
[CLAP PIPELINE] Removed 134118 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([25882])
[CLAP PIPELINE] Sample 0 stats: min=-0.8911, max=1.0000, mean=-0.0003, std=0.2134
[CLAP PIPELINE] Sample 0: 1/25882 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 44/10586 [00:16<1:05:25,  2.69it/s, v_num=n7n6, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8860, max=0.8689, mean=-0.0001, std=0.1662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Removed 111848 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([48152])
[CLAP PIPELINE] Sample 0 stats: min=-0.9492, max=0.9482, mean=-0.0003, std=0.2668
[CLAP PIPELINE] Sample 0: 3/48152 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 45/10586 [00:16<1:05:14,  2.69it/s, v_num=n7n6, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.9639, mean=0.0002, std=0.0911
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9595, max=0.9629, mean=0.0002, std=0.0911
[CLAP PIPELINE] Removed 101675 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([58325])
[CLAP PIPELINE] Sample 0 stats: min=-0.9595, max=0.9629, mean=0.0005, std=0.1508
[CLAP PIPELINE] Sample 0: 171/58325 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 46/10586 [00:17<1:05:30,  2.68it/s, v_num=n7n6, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9609, max=0.9900, mean=-0.0000, std=0.1076
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9565, max=0.9468, mean=-0.0000, std=0.1075
[CLAP PIPELINE] Removed 51816 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([108184])
[CLAP PIPELINE] Sample 0 stats: min=-0.9565, max=0.9468, mean=-0.0000, std=0.1307
[CLAP PIPELINE] Sample 0: 280/108184 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                  | 47/10586 [00:17<1:05:48,  2.67it/s, v_num=n7n6, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9793, max=0.9997, mean=0.0003, std=0.1813
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9644, max=0.9917, mean=0.0003, std=0.1813
[CLAP PIPELINE] Removed 69162 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([90838])
[CLAP PIPELINE] Sample 0 stats: min=-0.9644, max=0.9917, mean=0.0005, std=0.2406
[CLAP PIPELINE] Sample 0: 0/90838 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                  | 48/10586 [00:18<1:06:11,  2.65it/s, v_num=n7n6, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.5452, max=1.0035, mean=0.0012, std=0.1025
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5444, max=1.0029, mean=0.0012, std=0.1025
[CLAP PIPELINE] Removed 119300 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([40700])
[CLAP PIPELINE] Sample 0 stats: min=-0.5444, max=1.0029, mean=0.0049, std=0.2032
[CLAP PIPELINE] Sample 0: 45/40700 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                  | 49/10586 [00:18<1:06:25,  2.64it/s, v_num=n7n6, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9915, max=0.7667, mean=-0.0008, std=0.0252
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Removed 104632 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([55368])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.6553, mean=-0.0024, std=0.0429
[CLAP PIPELINE] Sample 0: 7/55368 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                  | 50/10586 [00:18<1:06:05,  2.66it/s, v_num=n7n6, train/loss=nan.0, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0745, max=1.0750, mean=0.0004, std=0.1761
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0283, max=1.0527, mean=0.0004, std=0.1761
[CLAP PIPELINE] Removed 35332 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([124668])
[CLAP PIPELINE] Sample 0 stats: min=-1.0283, max=1.0527, mean=0.0005, std=0.1996
[CLAP PIPELINE] Sample 0: 3618/124668 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                  | 51/10586 [00:19<1:06:15,  2.65it/s, v_num=n7n6, train/loss=nan.0, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=0.8610, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Removed 148710 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([11290])
[CLAP PIPELINE] Sample 0 stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.1481
[CLAP PIPELINE] Sample 0: 0/11290 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                  | 52/10586 [00:19<1:05:35,  2.68it/s, v_num=n7n6, train/loss=nan.0, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8210, max=1.0005, mean=0.0000, std=0.0306
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8125, max=0.9976, mean=0.0000, std=0.0306
[CLAP PIPELINE] Removed 139914 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([20086])
[CLAP PIPELINE] Sample 0 stats: min=-0.8125, max=0.9976, mean=0.0000, std=0.0864
[CLAP PIPELINE] Sample 0: 0/20086 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                  | 53/10586 [00:19<1:05:12,  2.69it/s, v_num=n7n6, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9396, mean=-0.0023, std=0.0912
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0127, max=0.9453, mean=-0.0023, std=0.0912
[CLAP PIPELINE] Removed 113648 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([46352])
[CLAP PIPELINE] Sample 0 stats: min=-1.0127, max=0.9453, mean=-0.0079, std=0.1693
[CLAP PIPELINE] Sample 0: 6/46352 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                  | 54/10586 [00:19<1:04:47,  2.71it/s, v_num=n7n6, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.5613, mean=-0.0000, std=0.0136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Removed 18175 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform after trimming: shape=torch.Size([141825])
[CLAP PIPELINE] Sample 0 stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0142
[CLAP PIPELINE] Sample 0: 216/141825 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 audio dicts
[CLAP PIPELINE] Sample 0 processed waveform shape: torch.Size([480000])
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan

Detected KeyboardInterrupt, attempting graceful shutdown ...
