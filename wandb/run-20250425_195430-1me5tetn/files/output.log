Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs

----- Training -----
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name              | Type                               | Params | Mode
---------------------------------------------------------------------------------
0 | mel               | AugmentMelSTFT                     | 0      | train
1 | imitation_encoder | MobileNetV3                        | 2.7 M  | train
2 | reference_encoder | MobileNetV3                        | 2.7 M  | train
3 | clap_model        | CLAPAudioEmbeddingClassifierFreev2 | 158 M  | eval
  | other params      | n/a                                | 2      | n/a
---------------------------------------------------------------------------------
5.4 M     Trainable params
158 M     Non-trainable params
163 M     Total params
655.085   Total estimated model params size (MB)
419       Modules in train mode
465       Modules in eval mode
Sanity Checking DataLoader 0:   0%|                                                                                                                       | 0/2 [00:00<?, ?it/s]
[VALIDATION] Batch 0 - Processing validation data
[VALIDATION] Imitation shape: torch.Size([1, 320000]), Reference shape: torch.Size([1, 320000])
[VALIDATION] Imitation: min=-0.8460, max=1.0018
[VALIDATION] Reference: min=-0.9751, max=0.9976
[CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9751, max=0.9976, mean=-0.0001, std=0.2598
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Removed 0 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.9692, max=0.9917, mean=-0.0001, std=0.2598
[CLAP PIPELINE] Sample 0: 25/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[VALIDATION] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[VALIDATION] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[VALIDATION] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[VALIDATION] WARNING: NaN found in log_softmax outputs:
[VALIDATION]   - C_qvim_log: 0/1 NaN values
[VALIDATION]   - C_ref_clap_log: 1/1 NaN values
[VALIDATION]   - C_im_clap_log: 1/1 NaN values
Sanity Checking DataLoader 0:  50%|███████████████████████████████████████████████████████▌                                                       | 1/2 [00:01<00:01,  0.51it/s]
[VALIDATION] Batch 1 - Processing validation data
[VALIDATION] Imitation shape: torch.Size([1, 320000]), Reference shape: torch.Size([1, 320000])
[VALIDATION] Imitation: min=-0.9942, max=0.9894
[VALIDATION] Reference: min=-0.9515, max=1.0022
[CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Removed 150802 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([9198])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0003, std=0.0709
[CLAP PIPELINE] Sample 0: 11/9198 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[VALIDATION] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[VALIDATION] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[VALIDATION] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[VALIDATION] WARNING: NaN found in log_softmax outputs:
[VALIDATION]   - C_qvim_log: 0/1 NaN values
[VALIDATION]   - C_ref_clap_log: 1/1 NaN values
[VALIDATION]   - C_im_clap_log: 1/1 NaN values
Epoch 0:   0%|                                                                                                                                        | 0/10586 [00:00<?, ?it/s][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9448, max=0.7345, mean=0.0000, std=0.0662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8896, max=0.5645, mean=0.0000, std=0.0489
[CLAP PIPELINE] Removed 75421 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([84579])
[CLAP PIPELINE] Sample 0 stats: min=-0.8896, max=0.5645, mean=0.0001, std=0.0673
[CLAP PIPELINE] Sample 0: 0/84579 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1815, max=0.1260, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.9016, max=-0.9016, mean=-0.9016
[SIMILARITY] C_im_clap: min=-0.9644, max=-0.9644, mean=-0.9644
Epoch 0:   0%|                                                                                     | 1/10586 [00:02<8:47:52,  0.33it/s, v_num=tetn, train/loss=-0.00, lr=2.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8902, max=1.0016, mean=-0.0000, std=0.1195
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8936, max=1.0020, mean=-0.0000, std=0.1195
[CLAP PIPELINE] Removed 117084 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([42916])
[CLAP PIPELINE] Sample 0 stats: min=-0.8936, max=1.0020, mean=-0.0002, std=0.2308
[CLAP PIPELINE] Sample 0: 7/42916 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                     | 2/10586 [00:03<5:16:58,  0.56it/s, v_num=tetn, train/loss=nan.0, lr=2.5e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9285, max=0.9992, mean=-0.0009, std=0.0743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Removed 129717 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([30283])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=1.0010, mean=-0.0048, std=0.1708
[CLAP PIPELINE] Sample 0: 2/30283 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1986, max=0.1242, mean=-0.0025, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.8148, max=-0.8148, mean=-0.8148
[SIMILARITY] C_im_clap: min=-0.8323, max=-0.8323, mean=-0.8323
Epoch 0:   0%|                                                                                    | 3/10586 [00:03<3:50:56,  0.76it/s, v_num=tetn, train/loss=-0.00, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9876, max=1.0462, mean=-0.0003, std=0.0745
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Removed 92399 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([67601])
[CLAP PIPELINE] Sample 0 stats: min=-0.9731, max=0.9883, mean=-0.0008, std=0.1135
[CLAP PIPELINE] Sample 0: 13/67601 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                                    | 4/10586 [00:04<3:02:17,  0.97it/s, v_num=tetn, train/loss=nan.0, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0043, max=0.8394, mean=-0.0000, std=0.0324
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5513, max=0.6128, mean=-0.0000, std=0.0288
[CLAP PIPELINE] Removed 152753 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([7247])
[CLAP PIPELINE] Sample 0 stats: min=-0.5513, max=0.6128, mean=-0.0000, std=0.1353
[CLAP PIPELINE] Sample 0: 2/7247 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1853, max=0.1210, mean=-0.0025, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.8192, max=-0.8192, mean=-0.8192
[SIMILARITY] C_im_clap: min=-0.8078, max=-0.8078, mean=-0.8078
Epoch 0:   0%|                                                                                    | 5/10586 [00:04<2:38:15,  1.11it/s, v_num=tetn, train/loss=-0.00, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8219, max=0.8306, mean=-0.0000, std=0.0854
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7202, max=0.7139, mean=-0.0000, std=0.0742
[CLAP PIPELINE] Removed 0 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7202, max=0.7139, mean=-0.0000, std=0.0742
[CLAP PIPELINE] Sample 0: 7/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1782, max=0.1287, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.9382, max=-0.9382, mean=-0.9382
[SIMILARITY] C_im_clap: min=-0.9033, max=-0.9033, mean=-0.9033
Epoch 0:   0%|                                                                                    | 6/10586 [00:04<2:20:32,  1.25it/s, v_num=tetn, train/loss=-0.00, lr=2.51e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9915, max=0.7667, mean=-0.0008, std=0.0252
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Removed 104632 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([55368])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.6553, mean=-0.0024, std=0.0429
[CLAP PIPELINE] Sample 0: 7/55368 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1826, max=0.1258, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=-0.9051, max=-0.9051, mean=-0.9051
[SIMILARITY] C_im_clap: min=-0.8811, max=-0.8811, mean=-0.8811
Epoch 0:   0%|                                                                                | 7/10586 [-1:59:57<-2:28:26, -1.93it/s, v_num=tetn, train/loss=-0.00, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.9017, mean=0.0002, std=0.1192
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9956, max=0.9019, mean=0.0002, std=0.1191
[CLAP PIPELINE] Removed 131310 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([28690])
[CLAP PIPELINE] Sample 0 stats: min=-0.9956, max=0.9019, mean=0.0008, std=0.2815
[CLAP PIPELINE] Sample 0: 2/28690 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1839, max=0.1340, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.9033, max=-0.9033, mean=-0.9033
[SIMILARITY] C_im_clap: min=-0.9164, max=-0.9164, mean=-0.9164
Epoch 0:   0%|                                                                                | 8/10586 [-1:59:57<-2:46:56, -2.41it/s, v_num=tetn, train/loss=-0.00, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9986, max=0.9916, mean=0.0004, std=0.1325
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9878, max=0.9771, mean=0.0004, std=0.1324
[CLAP PIPELINE] Removed 99636 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([60364])
[CLAP PIPELINE] Sample 0 stats: min=-0.9878, max=0.9771, mean=0.0012, std=0.2157
[CLAP PIPELINE] Sample 0: 0/60364 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1763, max=0.1256, mean=-0.0028, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.8257, max=-0.8257, mean=-0.8257
[SIMILARITY] C_im_clap: min=-0.8929, max=-0.8929, mean=-0.8929
Epoch 0:   0%|                                                                                | 9/10586 [-1:59:58<-1:02:02, -3.04it/s, v_num=tetn, train/loss=-0.00, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9860, max=0.6366, mean=-0.0001, std=0.0639
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8730, max=0.5918, mean=-0.0001, std=0.0623
[CLAP PIPELINE] Removed 91799 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([68201])
[CLAP PIPELINE] Sample 0 stats: min=-0.8730, max=0.5918, mean=-0.0002, std=0.0954
[CLAP PIPELINE] Sample 0: 1/68201 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1787, max=0.1260, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=-0.9269, max=-0.9269, mean=-0.9269
[SIMILARITY] C_im_clap: min=-0.9103, max=-0.9103, mean=-0.9103
Epoch 0:   0%|                                                                               | 10/10586 [-1:59:58<-1:13:16, -3.77it/s, v_num=tetn, train/loss=-0.00, lr=2.52e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9877, max=0.9983, mean=0.0004, std=0.0661
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Removed 87726 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([72274])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9971, mean=0.0009, std=0.0984
[CLAP PIPELINE] Sample 0: 1/72274 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                               | 11/10586 [-1:59:58<-1:20:11, -4.43it/s, v_num=tetn, train/loss=nan.0, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8575, mean=-0.0000, std=0.0995
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.0992
[CLAP PIPELINE] Removed 66889 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([93111])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.8506, mean=-0.0000, std=0.1300
[CLAP PIPELINE] Sample 0: 4/93111 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1832, max=0.1332, mean=-0.0027, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.8497, max=-0.8497, mean=-0.8497
[SIMILARITY] C_im_clap: min=-0.8471, max=-0.8471, mean=-0.8471
Epoch 0:   0%|                                                                               | 12/10586 [-1:59:58<-1:28:51, -5.65it/s, v_num=tetn, train/loss=-0.00, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8927, max=1.0000, mean=0.0000, std=0.0786
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8281, max=0.7896, mean=0.0000, std=0.0775
[CLAP PIPELINE] Removed 150522 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([9478])
[CLAP PIPELINE] Sample 0 stats: min=-0.8281, max=0.7896, mean=0.0000, std=0.3184
[CLAP PIPELINE] Sample 0: 0/9478 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1959, max=0.1455, mean=-0.0024, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=-0.8139, max=-0.8139, mean=-0.8139
[SIMILARITY] C_im_clap: min=-0.7093, max=-0.7093, mean=-0.7093
Epoch 0:   0%|                                                                               | 13/10586 [-1:59:59<-1:34:54, -7.02it/s, v_num=tetn, train/loss=-0.00, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=1.0032, mean=-0.0030, std=0.1216
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9224, max=0.9990, mean=-0.0030, std=0.1213
[CLAP PIPELINE] Removed 123214 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([36786])
[CLAP PIPELINE] Sample 0 stats: min=-0.9224, max=0.9990, mean=-0.0131, std=0.2529
[CLAP PIPELINE] Sample 0: 1/36786 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1677, max=0.1233, mean=-0.0025, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.7882, max=-0.7882, mean=-0.7882
[SIMILARITY] C_im_clap: min=-0.8244, max=-0.8244, mean=-0.8244
Epoch 0:   0%|                                                                               | 14/10586 [-1:59:59<-1:40:03, -8.83it/s, v_num=tetn, train/loss=-0.00, lr=2.53e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6593, max=0.6695, mean=0.0131, std=0.0694
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6108, max=0.6440, mean=0.0131, std=0.0693
[CLAP PIPELINE] Removed 95672 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([64328])
[CLAP PIPELINE] Sample 0 stats: min=-0.6108, max=0.6440, mean=0.0327, std=0.1064
[CLAP PIPELINE] Sample 0: 3/64328 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                              | 15/10586 [-1:59:59<-1:45:02, -11.76it/s, v_num=tetn, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8606, max=1.0119, mean=0.0192, std=0.2174
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8516, max=0.9956, mean=0.0192, std=0.2169
[CLAP PIPELINE] Removed 3117 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([156883])
[CLAP PIPELINE] Sample 0 stats: min=-0.8516, max=0.9956, mean=0.0196, std=0.2191
[CLAP PIPELINE] Sample 0: 1/156883 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2160, max=14.2160, mean=14.2160
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|                                                                              | 16/10586 [-1:59:59<-1:48:40, -15.54it/s, v_num=tetn, train/loss=nan.0, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9391, max=0.9965, mean=-0.0004, std=0.1406
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1406
[CLAP PIPELINE] Removed 27044 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([132956])
[CLAP PIPELINE] Sample 0 stats: min=-0.9395, max=0.9966, mean=-0.0004, std=0.1542
[CLAP PIPELINE] Sample 0: 3/132956 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1690, max=0.1435, mean=-0.0024, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.7329, max=-0.7329, mean=-0.7329
[SIMILARITY] C_im_clap: min=-0.7860, max=-0.7860, mean=-0.7860
Epoch 0:   0%|▏                                                                                | 17/10586 [00:00<-1:52:38, -23.87it/s, v_num=tetn, train/loss=-0.00, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9966, max=1.0181, mean=-0.0001, std=0.0736
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=1.0098, mean=-0.0001, std=0.0735
[CLAP PIPELINE] Removed 103089 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([56911])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=1.0098, mean=-0.0002, std=0.1234
[CLAP PIPELINE] Sample 0: 3/56911 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1769, max=0.1243, mean=-0.0030, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.9399, max=-0.9399, mean=-0.9399
[SIMILARITY] C_im_clap: min=-0.9434, max=-0.9434, mean=-0.9434
Epoch 0:   0%|▏                                                                                | 18/10586 [00:00<-1:56:13, -46.52it/s, v_num=tetn, train/loss=-0.00, lr=2.54e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8297, max=1.0162, mean=-0.0000, std=0.0370
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0370
[CLAP PIPELINE] Removed 64230 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([95770])
[CLAP PIPELINE] Sample 0 stats: min=-0.8179, max=0.9873, mean=-0.0000, std=0.0478
[CLAP PIPELINE] Sample 0: 228/95770 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                               | 19/10586 [00:00<-1:58:48, -145.21it/s, v_num=tetn, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9877, max=0.9983, mean=0.0004, std=0.0661
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9863, max=0.9971, mean=0.0004, std=0.0662
[CLAP PIPELINE] Removed 87726 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([72274])
[CLAP PIPELINE] Sample 0 stats: min=-0.9863, max=0.9971, mean=0.0009, std=0.0984
[CLAP PIPELINE] Sample 0: 1/72274 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 20/10586 [00:00<01:52, 94.34it/s, v_num=tetn, train/loss=nan.0, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9285, max=0.9992, mean=-0.0009, std=0.0743
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9150, max=1.0010, mean=-0.0009, std=0.0743
[CLAP PIPELINE] Removed 129717 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([30283])
[CLAP PIPELINE] Sample 0 stats: min=-0.9150, max=1.0010, mean=-0.0048, std=0.1708
[CLAP PIPELINE] Sample 0: 2/30283 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1986, max=0.1242, mean=-0.0025, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.7429, max=-0.7429, mean=-0.7429
[SIMILARITY] C_im_clap: min=-0.7677, max=-0.7677, mean=-0.7677
Epoch 0:   0%|▏                                                                                    | 21/10586 [00:00<05:15, 33.54it/s, v_num=tetn, train/loss=-0.00, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9624, max=1.0002, mean=-0.0004, std=0.1298
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9771, max=0.9927, mean=-0.0004, std=0.1298
[CLAP PIPELINE] Removed 144171 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([15829])
[CLAP PIPELINE] Sample 0 stats: min=-0.9771, max=0.9927, mean=-0.0045, std=0.4124
[CLAP PIPELINE] Sample 0: 7/15829 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1857, max=0.1279, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.9330, max=-0.9330, mean=-0.9330
[SIMILARITY] C_im_clap: min=-0.9173, max=-0.9173, mean=-0.9173
Epoch 0:   0%|▏                                                                                    | 22/10586 [00:00<07:38, 23.07it/s, v_num=tetn, train/loss=-0.00, lr=2.55e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0033, max=1.0005, mean=-0.0001, std=0.0460
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9839, max=0.9712, mean=-0.0001, std=0.0460
[CLAP PIPELINE] Removed 154783 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([5217])
[CLAP PIPELINE] Sample 0 stats: min=-0.9839, max=0.9712, mean=-0.0043, std=0.2546
[CLAP PIPELINE] Sample 0: 0/5217 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1476, max=0.1210, mean=-0.0024, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.7738, max=-0.7738, mean=-0.7738
[SIMILARITY] C_im_clap: min=-0.7821, max=-0.7821, mean=-0.7821
Epoch 0:   0%|▏                                                                                    | 23/10586 [00:01<09:58, 17.64it/s, v_num=tetn, train/loss=-0.00, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9982, max=0.8683, mean=-0.0054, std=0.0859
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9937, max=0.8677, mean=-0.0054, std=0.0859
[CLAP PIPELINE] Removed 127115 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([32885])
[CLAP PIPELINE] Sample 0 stats: min=-0.9937, max=0.8677, mean=-0.0261, std=0.1880
[CLAP PIPELINE] Sample 0: 155/32885 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1780, max=0.1313, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.9042, max=-0.9042, mean=-0.9042
[SIMILARITY] C_im_clap: min=-0.8802, max=-0.8802, mean=-0.8802
Epoch 0:   0%|▏                                                                                    | 24/10586 [00:01<11:23, 15.46it/s, v_num=tetn, train/loss=-0.00, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0084, mean=-0.0001, std=0.1935
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Removed 39111 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([120889])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.2036
[CLAP PIPELINE] Sample 0: 1/120889 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 25/10586 [00:01<13:02, 13.50it/s, v_num=tetn, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0007, max=0.9729, mean=-0.0113, std=0.4417
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.9702, mean=-0.0113, std=0.4414
[CLAP PIPELINE] Removed 20050 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([139950])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.9702, mean=-0.0129, std=0.4719
[CLAP PIPELINE] Sample 0: 0/139950 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 26/10586 [00:02<14:47, 11.90it/s, v_num=tetn, train/loss=nan.0, lr=2.56e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9992, max=0.9896, mean=-0.0005, std=0.0853
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9897, mean=-0.0005, std=0.0853
[CLAP PIPELINE] Removed 137707 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([22293])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9897, mean=-0.0034, std=0.2285
[CLAP PIPELINE] Sample 0: 122/22293 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1925, max=0.1389, mean=-0.0031, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.9801, max=-0.9801, mean=-0.9801
[SIMILARITY] C_im_clap: min=-0.9600, max=-0.9600, mean=-0.9600
Epoch 0:   0%|▏                                                                                    | 27/10586 [00:02<15:53, 11.08it/s, v_num=tetn, train/loss=-0.00, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9676, max=1.0002, mean=-0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1316
[CLAP PIPELINE] Removed 25925 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([134075])
[CLAP PIPELINE] Sample 0 stats: min=-0.9658, max=0.9980, mean=-0.0000, std=0.1438
[CLAP PIPELINE] Sample 0: 8/134075 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 28/10586 [00:02<17:18, 10.17it/s, v_num=tetn, train/loss=nan.0, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9931, max=0.9308, mean=-0.0000, std=0.1076
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9946, max=0.9312, mean=-0.0000, std=0.1076
[CLAP PIPELINE] Removed 50682 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([109318])
[CLAP PIPELINE] Sample 0 stats: min=-0.9946, max=0.9312, mean=-0.0000, std=0.1302
[CLAP PIPELINE] Sample 0: 0/109318 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1738, max=0.1337, mean=-0.0021, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.6430, max=-0.6430, mean=-0.6430
[SIMILARITY] C_im_clap: min=-0.6880, max=-0.6880, mean=-0.6880
Epoch 0:   0%|▏                                                                                    | 29/10586 [00:02<15:12, 11.57it/s, v_num=tetn, train/loss=-0.00, lr=2.57e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7830, max=0.7911, mean=-0.0000, std=0.0254
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.2476, max=0.2393, mean=-0.0000, std=0.0161
[CLAP PIPELINE] Removed 63026 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([96974])
[CLAP PIPELINE] Sample 0 stats: min=-0.2476, max=0.2393, mean=-0.0000, std=0.0207
[CLAP PIPELINE] Sample 0: 101/96974 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▏                                                                                    | 30/10586 [00:02<16:46, 10.49it/s, v_num=tetn, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8381, max=1.0082, mean=0.0006, std=0.1664
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8252, max=0.9849, mean=0.0006, std=0.1655
[CLAP PIPELINE] Removed 88664 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([71336])
[CLAP PIPELINE] Sample 0 stats: min=-0.8252, max=0.9849, mean=0.0013, std=0.2479
[CLAP PIPELINE] Sample 0: 2/71336 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1798, max=0.1201, mean=-0.0030, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.9914, max=-0.9914, mean=-0.9914
[SIMILARITY] C_im_clap: min=-0.9626, max=-0.9626, mean=-0.9626
Epoch 0:   0%|▏                                                                                    | 31/10586 [00:03<18:37,  9.44it/s, v_num=tetn, train/loss=-0.00, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8002, max=1.0032, mean=-0.0002, std=0.0756
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7935, max=1.0176, mean=-0.0002, std=0.0755
[CLAP PIPELINE] Removed 131203 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([28797])
[CLAP PIPELINE] Sample 0 stats: min=-0.7935, max=1.0176, mean=-0.0013, std=0.1779
[CLAP PIPELINE] Sample 0: 0/28797 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1850, max=0.1335, mean=-0.0023, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=-0.7368, max=-0.7368, mean=-0.7368
[SIMILARITY] C_im_clap: min=-0.7429, max=-0.7429, mean=-0.7429
Epoch 0:   0%|▎                                                                                    | 32/10586 [00:03<19:55,  8.83it/s, v_num=tetn, train/loss=-0.00, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8584, max=0.9898, mean=-0.0015, std=0.2105
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8594, max=0.8374, mean=-0.0015, std=0.2020
[CLAP PIPELINE] Removed 104008 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([55992])
[CLAP PIPELINE] Sample 0 stats: min=-0.8594, max=0.8374, mean=-0.0043, std=0.3416
[CLAP PIPELINE] Sample 0: 1/55992 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 33/10586 [00:03<21:06,  8.33it/s, v_num=tetn, train/loss=nan.0, lr=2.58e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8739, max=0.9986, mean=-0.0000, std=0.0380
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Removed 59395 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([100605])
[CLAP PIPELINE] Sample 0 stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0475
[CLAP PIPELINE] Sample 0: 2090/100605 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 34/10586 [00:04<22:41,  7.75it/s, v_num=tetn, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8882, max=0.8452, mean=-0.0001, std=0.0777
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5093, max=0.4824, mean=-0.0001, std=0.0479
[CLAP PIPELINE] Removed 63203 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([96797])
[CLAP PIPELINE] Sample 0 stats: min=-0.5093, max=0.4824, mean=-0.0002, std=0.0616
[CLAP PIPELINE] Sample 0: 1/96797 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 35/10586 [00:04<24:13,  7.26it/s, v_num=tetn, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9034, max=1.0000, mean=-0.0000, std=0.1119
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Removed 63472 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([96528])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9868, mean=-0.0001, std=0.1442
[CLAP PIPELINE] Sample 0: 2/96528 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 36/10586 [00:05<24:32,  7.16it/s, v_num=tetn, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.1079, max=1.1765, mean=-0.0007, std=0.1651
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.1914, max=1.1514, mean=-0.0007, std=0.1642
[CLAP PIPELINE] Removed 6092 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([153908])
[CLAP PIPELINE] Sample 0 stats: min=-1.1914, max=1.1514, mean=-0.0007, std=0.1675
[CLAP PIPELINE] Sample 0: 5/153908 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 37/10586 [00:05<25:19,  6.94it/s, v_num=tetn, train/loss=nan.0, lr=2.59e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9734, max=0.9692, mean=-0.0008, std=0.4332
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9985, max=1.0049, mean=-0.0008, std=0.4280
[CLAP PIPELINE] Removed 88005 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([71995])
[CLAP PIPELINE] Sample 0 stats: min=-0.9985, max=1.0049, mean=-0.0017, std=0.6377
[CLAP PIPELINE] Sample 0: 0/71995 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1194, max=0.1176, mean=-0.0011, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.2710, max=-0.2710, mean=-0.2710
[SIMILARITY] C_im_clap: min=-0.3416, max=-0.3416, mean=-0.3416
Epoch 0:   0%|▎                                                                                     | 38/10586 [00:05<25:28,  6.90it/s, v_num=tetn, train/loss=-0.00, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9515, max=1.0022, mean=-0.0000, std=0.0170
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9478, max=1.0039, mean=-0.0000, std=0.0170
[CLAP PIPELINE] Removed 150802 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([9198])
[CLAP PIPELINE] Sample 0 stats: min=-0.9478, max=1.0039, mean=-0.0003, std=0.0709
[CLAP PIPELINE] Sample 0: 11/9198 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2229, max=14.2229, mean=14.2229
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                     | 39/10586 [00:05<25:58,  6.77it/s, v_num=tetn, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9999, max=0.7777, mean=0.0000, std=0.0404
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.7783, mean=0.0000, std=0.0404
[CLAP PIPELINE] Removed 112894 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([47106])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.7783, mean=0.0001, std=0.0745
[CLAP PIPELINE] Sample 0: 362/47106 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                     | 40/10586 [00:05<26:10,  6.71it/s, v_num=tetn, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0082, max=0.9617, mean=-0.0000, std=0.0235
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6812, max=0.6206, mean=-0.0000, std=0.0112
[CLAP PIPELINE] Removed 0 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.6812, max=0.6206, mean=-0.0000, std=0.0112
[CLAP PIPELINE] Sample 0: 655/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                     | 41/10586 [00:06<26:40,  6.59it/s, v_num=tetn, train/loss=nan.0, lr=2.6e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9410, max=0.9987, mean=0.0008, std=0.1114
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8623, max=0.8574, mean=0.0008, std=0.1111
[CLAP PIPELINE] Removed 129901 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([30099])
[CLAP PIPELINE] Sample 0 stats: min=-0.8623, max=0.8574, mean=0.0042, std=0.2561
[CLAP PIPELINE] Sample 0: 0/30099 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1681, max=0.1267, mean=-0.0010, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.3108, max=-0.3108, mean=-0.3108
[SIMILARITY] C_im_clap: min=-0.2572, max=-0.2572, mean=-0.2572
Epoch 0:   0%|▎                                                                                    | 42/10586 [00:06<27:11,  6.46it/s, v_num=tetn, train/loss=-0.00, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0014, max=0.8091, mean=-0.0016, std=0.1717
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9570, max=0.8037, mean=-0.0016, std=0.1718
[CLAP PIPELINE] Removed 84577 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([75423])
[CLAP PIPELINE] Sample 0 stats: min=-0.9570, max=0.8037, mean=-0.0034, std=0.2500
[CLAP PIPELINE] Sample 0: 194/75423 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1817, max=0.1256, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.9260, max=-0.9260, mean=-0.9260
[SIMILARITY] C_im_clap: min=-0.8741, max=-0.8741, mean=-0.8741
Epoch 0:   0%|▎                                                                                    | 43/10586 [00:06<27:34,  6.37it/s, v_num=tetn, train/loss=-0.00, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8915, max=0.9999, mean=-0.0001, std=0.0858
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8911, max=1.0000, mean=-0.0001, std=0.0858
[CLAP PIPELINE] Removed 134118 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([25882])
[CLAP PIPELINE] Sample 0 stats: min=-0.8911, max=1.0000, mean=-0.0003, std=0.2134
[CLAP PIPELINE] Sample 0: 1/25882 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▎                                                                                    | 44/10586 [00:07<28:07,  6.25it/s, v_num=tetn, train/loss=nan.0, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8860, max=0.8689, mean=-0.0001, std=0.1662
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9492, max=0.9482, mean=-0.0001, std=0.1464
[CLAP PIPELINE] Removed 111848 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([48152])
[CLAP PIPELINE] Sample 0 stats: min=-0.9492, max=0.9482, mean=-0.0003, std=0.2668
[CLAP PIPELINE] Sample 0: 3/48152 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1948, max=0.1279, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.8628, max=-0.8628, mean=-0.8628
[SIMILARITY] C_im_clap: min=-0.9155, max=-0.9155, mean=-0.9155
Epoch 0:   0%|▎                                                                                    | 45/10586 [00:07<28:34,  6.15it/s, v_num=tetn, train/loss=-0.00, lr=2.61e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.9639, mean=0.0002, std=0.0911
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9595, max=0.9629, mean=0.0002, std=0.0911
[CLAP PIPELINE] Removed 101675 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([58325])
[CLAP PIPELINE] Sample 0 stats: min=-0.9595, max=0.9629, mean=0.0005, std=0.1508
[CLAP PIPELINE] Sample 0: 171/58325 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1794, max=0.1240, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.9129, max=-0.9129, mean=-0.9129
[SIMILARITY] C_im_clap: min=-0.9242, max=-0.9242, mean=-0.9242
Epoch 0:   0%|▎                                                                                    | 46/10586 [00:07<28:45,  6.11it/s, v_num=tetn, train/loss=-0.00, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9609, max=0.9900, mean=-0.0000, std=0.1076
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9565, max=0.9468, mean=-0.0000, std=0.1075
[CLAP PIPELINE] Removed 51816 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([108184])
[CLAP PIPELINE] Sample 0 stats: min=-0.9565, max=0.9468, mean=-0.0000, std=0.1307
[CLAP PIPELINE] Sample 0: 280/108184 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 47/10586 [00:07<29:17,  6.00it/s, v_num=tetn, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9793, max=0.9997, mean=0.0003, std=0.1813
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9644, max=0.9917, mean=0.0003, std=0.1813
[CLAP PIPELINE] Removed 69162 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([90838])
[CLAP PIPELINE] Sample 0 stats: min=-0.9644, max=0.9917, mean=0.0005, std=0.2406
[CLAP PIPELINE] Sample 0: 0/90838 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1351, max=0.1216, mean=-0.0025, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.8061, max=-0.8061, mean=-0.8061
[SIMILARITY] C_im_clap: min=-0.7926, max=-0.7926, mean=-0.7926
Epoch 0:   0%|▍                                                                                    | 48/10586 [00:07<27:46,  6.32it/s, v_num=tetn, train/loss=-0.00, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.5452, max=1.0035, mean=0.0012, std=0.1025
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5444, max=1.0029, mean=0.0012, std=0.1025
[CLAP PIPELINE] Removed 119300 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([40700])
[CLAP PIPELINE] Sample 0 stats: min=-0.5444, max=1.0029, mean=0.0049, std=0.2032
[CLAP PIPELINE] Sample 0: 45/40700 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 49/10586 [00:07<28:06,  6.25it/s, v_num=tetn, train/loss=nan.0, lr=2.62e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9915, max=0.7667, mean=-0.0008, std=0.0252
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9834, max=0.6553, mean=-0.0008, std=0.0253
[CLAP PIPELINE] Removed 104632 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([55368])
[CLAP PIPELINE] Sample 0 stats: min=-0.9834, max=0.6553, mean=-0.0024, std=0.0429
[CLAP PIPELINE] Sample 0: 7/55368 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1826, max=0.1258, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.9051, max=-0.9051, mean=-0.9051
[SIMILARITY] C_im_clap: min=-0.9077, max=-0.9077, mean=-0.9077
Epoch 0:   0%|▍                                                                                    | 50/10586 [00:08<28:31,  6.16it/s, v_num=tetn, train/loss=-0.00, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0745, max=1.0750, mean=0.0004, std=0.1761
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0283, max=1.0527, mean=0.0004, std=0.1761
[CLAP PIPELINE] Removed 35332 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([124668])
[CLAP PIPELINE] Sample 0 stats: min=-1.0283, max=1.0527, mean=0.0005, std=0.1996
[CLAP PIPELINE] Sample 0: 3618/124668 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   0%|▍                                                                                    | 51/10586 [00:08<28:53,  6.08it/s, v_num=tetn, train/loss=nan.0, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=0.8610, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Removed 148710 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([11290])
[CLAP PIPELINE] Sample 0 stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.1481
[CLAP PIPELINE] Sample 0: 0/11290 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1774, max=0.1362, mean=-0.0027, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.8833, max=-0.8833, mean=-0.8833
[SIMILARITY] C_im_clap: min=-0.8715, max=-0.8715, mean=-0.8715
Epoch 0:   0%|▍                                                                                    | 52/10586 [00:08<28:58,  6.06it/s, v_num=tetn, train/loss=-0.00, lr=2.63e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8210, max=1.0005, mean=0.0000, std=0.0306
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8125, max=0.9976, mean=0.0000, std=0.0306
[CLAP PIPELINE] Removed 139914 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([20086])
[CLAP PIPELINE] Sample 0 stats: min=-0.8125, max=0.9976, mean=0.0000, std=0.0864
[CLAP PIPELINE] Sample 0: 0/20086 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 53/10586 [00:08<29:10,  6.02it/s, v_num=tetn, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9396, mean=-0.0023, std=0.0912
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0127, max=0.9453, mean=-0.0023, std=0.0912
[CLAP PIPELINE] Removed 113648 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([46352])
[CLAP PIPELINE] Sample 0 stats: min=-1.0127, max=0.9453, mean=-0.0079, std=0.1693
[CLAP PIPELINE] Sample 0: 6/46352 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 54/10586 [00:09<29:31,  5.95it/s, v_num=tetn, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.5613, mean=-0.0000, std=0.0136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Removed 18175 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([141825])
[CLAP PIPELINE] Sample 0 stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0142
[CLAP PIPELINE] Sample 0: 216/141825 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 55/10586 [00:09<29:42,  5.91it/s, v_num=tetn, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0767, max=1.0207, mean=-0.0001, std=0.2233
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8193, max=0.8135, mean=-0.0001, std=0.2155
[CLAP PIPELINE] Removed 35158 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([124842])
[CLAP PIPELINE] Sample 0 stats: min=-0.8193, max=0.8135, mean=-0.0001, std=0.2439
[CLAP PIPELINE] Sample 0: 1/124842 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 56/10586 [00:09<30:03,  5.84it/s, v_num=tetn, train/loss=nan.0, lr=2.64e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9968, max=0.9985, mean=0.0000, std=0.1316
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0166, max=1.0215, mean=0.0000, std=0.1312
[CLAP PIPELINE] Removed 86794 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([73206])
[CLAP PIPELINE] Sample 0 stats: min=-1.0166, max=1.0215, mean=0.0000, std=0.1941
[CLAP PIPELINE] Sample 0: 11/73206 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 57/10586 [00:09<30:23,  5.78it/s, v_num=tetn, train/loss=nan.0, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9992, max=0.9896, mean=-0.0005, std=0.0853
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9897, mean=-0.0005, std=0.0853
[CLAP PIPELINE] Removed 137707 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([22293])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9897, mean=-0.0034, std=0.2285
[CLAP PIPELINE] Sample 0: 122/22293 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1925, max=0.1389, mean=-0.0031, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-1.0332, max=-1.0332, mean=-1.0332
[SIMILARITY] C_im_clap: min=-1.0210, max=-1.0210, mean=-1.0210
Epoch 0:   1%|▍                                                                                    | 58/10586 [00:10<30:44,  5.71it/s, v_num=tetn, train/loss=-0.00, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8806, max=0.7257, mean=-0.0001, std=0.0129
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6597, max=0.4893, mean=-0.0001, std=0.0100
[CLAP PIPELINE] Removed 47899 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([112101])
[CLAP PIPELINE] Sample 0 stats: min=-0.6597, max=0.4893, mean=-0.0001, std=0.0119
[CLAP PIPELINE] Sample 0: 14/112101 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1843, max=0.1236, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=-0.9077, max=-0.9077, mean=-0.9077
[SIMILARITY] C_im_clap: min=-0.8955, max=-0.8955, mean=-0.8955
Epoch 0:   1%|▍                                                                                    | 59/10586 [00:10<31:00,  5.66it/s, v_num=tetn, train/loss=-0.00, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9983, mean=0.0005, std=0.1294
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Removed 134275 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([25725])
[CLAP PIPELINE] Sample 0 stats: min=-0.9941, max=0.9985, mean=0.0033, std=0.3228
[CLAP PIPELINE] Sample 0: 6346/25725 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▍                                                                                    | 60/10586 [00:10<31:21,  5.59it/s, v_num=tetn, train/loss=nan.0, lr=2.65e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0024, max=0.6007, mean=-0.0007, std=0.0930
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0029, max=0.5977, mean=-0.0007, std=0.0930
[CLAP PIPELINE] Removed 133108 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([26892])
[CLAP PIPELINE] Sample 0 stats: min=-1.0029, max=0.5977, mean=-0.0042, std=0.2268
[CLAP PIPELINE] Sample 0: 0/26892 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1810, max=0.1111, mean=-0.0030, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-1.0036, max=-1.0036, mean=-1.0036
[SIMILARITY] C_im_clap: min=-0.9539, max=-0.9539, mean=-0.9539
Epoch 0:   1%|▍                                                                                    | 61/10586 [00:11<31:43,  5.53it/s, v_num=tetn, train/loss=-0.00, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9958, max=0.9515, mean=-0.0001, std=0.0717
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9600, max=0.8765, mean=-0.0001, std=0.0699
[CLAP PIPELINE] Removed 36782 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([123218])
[CLAP PIPELINE] Sample 0 stats: min=-0.9600, max=0.8765, mean=-0.0001, std=0.0797
[CLAP PIPELINE] Sample 0: 9/123218 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1825, max=0.1249, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=-0.8863, max=-0.8863, mean=-0.8863
[SIMILARITY] C_im_clap: min=-0.9173, max=-0.9173, mean=-0.9173
Epoch 0:   1%|▍                                                                                    | 62/10586 [00:11<32:10,  5.45it/s, v_num=tetn, train/loss=-0.00, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9835, max=0.8843, mean=-0.0000, std=0.2600
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9609, max=0.8481, mean=-0.0000, std=0.2581
[CLAP PIPELINE] Removed 26255 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([133745])
[CLAP PIPELINE] Sample 0 stats: min=-0.9609, max=0.8481, mean=-0.0000, std=0.2822
[CLAP PIPELINE] Sample 0: 0/133745 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1733, max=0.1221, mean=-0.0022, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.6548, max=-0.6548, mean=-0.6548
[SIMILARITY] C_im_clap: min=-0.6849, max=-0.6849, mean=-0.6849
Epoch 0:   1%|▌                                                                                    | 63/10586 [00:11<32:36,  5.38it/s, v_num=tetn, train/loss=-0.00, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9953, max=0.9269, mean=-0.0007, std=0.0796
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Removed 119925 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([40075])
[CLAP PIPELINE] Sample 0 stats: min=-0.9307, max=0.9238, mean=-0.0027, std=0.1591
[CLAP PIPELINE] Sample 0: 1/40075 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1888, max=0.1265, mean=-0.0025, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.8292, max=-0.8292, mean=-0.8292
[SIMILARITY] C_im_clap: min=-0.7978, max=-0.7978, mean=-0.7978
Epoch 0:   1%|▌                                                                                    | 64/10586 [00:11<32:49,  5.34it/s, v_num=tetn, train/loss=-0.00, lr=2.66e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9969, max=0.9446, mean=-0.0009, std=0.0720
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9927, max=0.9448, mean=-0.0009, std=0.0720
[CLAP PIPELINE] Removed 129530 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([30470])
[CLAP PIPELINE] Sample 0 stats: min=-0.9927, max=0.9448, mean=-0.0048, std=0.1648
[CLAP PIPELINE] Sample 0: 1/30470 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1678, max=0.1340, mean=-0.0027, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.8392, max=-0.8392, mean=-0.8392
[SIMILARITY] C_im_clap: min=-0.8589, max=-0.8589, mean=-0.8589
Epoch 0:   1%|▌                                                                                    | 65/10586 [00:12<33:01,  5.31it/s, v_num=tetn, train/loss=-0.00, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7351, max=0.7890, mean=-0.0000, std=0.0251
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.3665, max=0.3438, mean=-0.0000, std=0.0220
[CLAP PIPELINE] Removed 17016 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([142984])
[CLAP PIPELINE] Sample 0 stats: min=-0.3665, max=0.3438, mean=-0.0000, std=0.0233
[CLAP PIPELINE] Sample 0: 11/142984 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1843, max=0.1242, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.9460, max=-0.9460, mean=-0.9460
[SIMILARITY] C_im_clap: min=-0.9208, max=-0.9208, mean=-0.9208
Epoch 0:   1%|▌                                                                                    | 66/10586 [00:12<33:20,  5.26it/s, v_num=tetn, train/loss=-0.00, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9456, max=0.9481, mean=-0.0003, std=0.0849
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8320, max=0.8291, mean=-0.0003, std=0.0840
[CLAP PIPELINE] Removed 143687 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([16313])
[CLAP PIPELINE] Sample 0 stats: min=-0.8320, max=0.8291, mean=-0.0030, std=0.2632
[CLAP PIPELINE] Sample 0: 407/16313 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1843, max=0.1401, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.9434, max=-0.9434, mean=-0.9434
[SIMILARITY] C_im_clap: min=-0.9129, max=-0.9129, mean=-0.9129
Epoch 0:   1%|▌                                                                                    | 67/10586 [00:12<33:35,  5.22it/s, v_num=tetn, train/loss=-0.00, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9955, max=0.8652, mean=-0.0000, std=0.0358
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9419, max=0.7778, mean=-0.0000, std=0.0352
[CLAP PIPELINE] Removed 130986 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([29014])
[CLAP PIPELINE] Sample 0 stats: min=-0.9419, max=0.7778, mean=-0.0001, std=0.0826
[CLAP PIPELINE] Sample 0: 3/29014 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 68/10586 [00:12<32:38,  5.37it/s, v_num=tetn, train/loss=nan.0, lr=2.67e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9680, max=0.9256, mean=-0.0028, std=0.1273
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7139, max=0.8584, mean=-0.0028, std=0.1180
[CLAP PIPELINE] Removed 14922 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([145078])
[CLAP PIPELINE] Sample 0 stats: min=-0.7139, max=0.8584, mean=-0.0031, std=0.1240
[CLAP PIPELINE] Sample 0: 8/145078 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 69/10586 [00:12<32:53,  5.33it/s, v_num=tetn, train/loss=nan.0, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9638, max=0.9958, mean=-0.0016, std=0.1701
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9717, max=0.9648, mean=-0.0016, std=0.1698
[CLAP PIPELINE] Removed 82181 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([77819])
[CLAP PIPELINE] Sample 0 stats: min=-0.9717, max=0.9648, mean=-0.0032, std=0.2435
[CLAP PIPELINE] Sample 0: 34/77819 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1779, max=0.1172, mean=-0.0024, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.7708, max=-0.7708, mean=-0.7708
[SIMILARITY] C_im_clap: min=-0.7904, max=-0.7904, mean=-0.7904
Epoch 0:   1%|▌                                                                                    | 70/10586 [00:13<33:07,  5.29it/s, v_num=tetn, train/loss=-0.00, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8680, max=0.9994, mean=0.0003, std=0.0444
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8667, max=0.9995, mean=0.0003, std=0.0444
[CLAP PIPELINE] Removed 153870 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([6130])
[CLAP PIPELINE] Sample 0 stats: min=-0.8667, max=0.9995, mean=0.0075, std=0.2267
[CLAP PIPELINE] Sample 0: 15/6130 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 71/10586 [00:13<33:18,  5.26it/s, v_num=tetn, train/loss=nan.0, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0068, max=0.9876, mean=-0.0000, std=0.1221
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0068, max=0.9868, mean=-0.0000, std=0.1221
[CLAP PIPELINE] Removed 115119 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([44881])
[CLAP PIPELINE] Sample 0 stats: min=-1.0068, max=0.9868, mean=-0.0001, std=0.2306
[CLAP PIPELINE] Sample 0: 1/44881 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1908, max=0.1268, mean=-0.0026, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.8523, max=-0.8523, mean=-0.8523
[SIMILARITY] C_im_clap: min=-0.8680, max=-0.8680, mean=-0.8680
Epoch 0:   1%|▌                                                                                    | 72/10586 [00:13<33:29,  5.23it/s, v_num=tetn, train/loss=-0.00, lr=2.68e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8416, max=1.0099, mean=-0.0003, std=0.0124
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8896, max=0.8569, mean=-0.0003, std=0.0116
[CLAP PIPELINE] Removed 150684 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([9316])
[CLAP PIPELINE] Sample 0 stats: min=-0.8896, max=0.8569, mean=-0.0051, std=0.0476
[CLAP PIPELINE] Sample 0: 0/9316 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1810, max=0.1255, mean=-0.0027, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.9077, max=-0.9077, mean=-0.9077
[SIMILARITY] C_im_clap: min=-0.8554, max=-0.8554, mean=-0.8554
Epoch 0:   1%|▌                                                                                    | 73/10586 [00:14<33:47,  5.18it/s, v_num=tetn, train/loss=-0.00, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9876, max=1.0462, mean=-0.0003, std=0.0745
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9731, max=0.9883, mean=-0.0003, std=0.0737
[CLAP PIPELINE] Removed 92399 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([67601])
[CLAP PIPELINE] Sample 0 stats: min=-0.9731, max=0.9883, mean=-0.0008, std=0.1135
[CLAP PIPELINE] Sample 0: 13/67601 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 74/10586 [00:14<34:04,  5.14it/s, v_num=tetn, train/loss=nan.0, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9973, max=0.5613, mean=-0.0000, std=0.0136
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0133
[CLAP PIPELINE] Removed 18175 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([141825])
[CLAP PIPELINE] Sample 0 stats: min=-1.0234, max=0.5122, mean=-0.0000, std=0.0142
[CLAP PIPELINE] Sample 0: 216/141825 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▌                                                                                    | 75/10586 [00:14<34:20,  5.10it/s, v_num=tetn, train/loss=nan.0, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0118, max=1.0184, mean=0.0002, std=0.1353
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0156, max=1.0693, mean=0.0002, std=0.1348
[CLAP PIPELINE] Removed 30825 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([129175])
[CLAP PIPELINE] Sample 0 stats: min=-1.0156, max=1.0693, mean=0.0002, std=0.1500
[CLAP PIPELINE] Sample 0: 0/129175 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1805, max=0.1240, mean=-0.0030, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.9513, max=-0.9513, mean=-0.9513
[SIMILARITY] C_im_clap: min=-0.9155, max=-0.9155, mean=-0.9155
Epoch 0:   1%|▌                                                                                    | 76/10586 [00:15<34:40,  5.05it/s, v_num=tetn, train/loss=-0.00, lr=2.69e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9280, max=0.9990, mean=-0.0000, std=0.0785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0786
[CLAP PIPELINE] Removed 50628 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([109372])
[CLAP PIPELINE] Sample 0 stats: min=-0.9189, max=0.9595, mean=-0.0000, std=0.0950
[CLAP PIPELINE] Sample 0: 7/109372 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                     | 77/10586 [00:15<34:51,  5.02it/s, v_num=tetn, train/loss=nan.0, lr=2.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9035, mean=-0.0000, std=0.1785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Removed 78921 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([81079])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.2510
[CLAP PIPELINE] Sample 0: 0/81079 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                     | 78/10586 [00:15<35:06,  4.99it/s, v_num=tetn, train/loss=nan.0, lr=2.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9512, max=1.0018, mean=-0.0001, std=0.0323
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8413, max=0.8599, mean=-0.0001, std=0.0254
[CLAP PIPELINE] Removed 156967 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([3033])
[CLAP PIPELINE] Sample 0 stats: min=-0.8413, max=0.8599, mean=-0.0028, std=0.1842
[CLAP PIPELINE] Sample 0: 0/3033 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1828, max=0.1303, mean=-0.0027, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2229, max=14.2229, mean=14.2229
[SIMILARITY] C_ref_clap: min=-0.9670, max=-0.9670, mean=-0.9670
[SIMILARITY] C_im_clap: min=-0.8767, max=-0.8767, mean=-0.8767
Epoch 0:   1%|▋                                                                                     | 79/10586 [00:15<35:20,  4.96it/s, v_num=tetn, train/loss=-0.00, lr=2.7e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9952, max=0.9767, mean=-0.0001, std=0.0806
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Removed 140475 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([19525])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9756, mean=-0.0008, std=0.2307
[CLAP PIPELINE] Sample 0: 1/19525 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1740, max=0.1323, mean=-0.0027, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=-0.7887, max=-0.7887, mean=-0.7887
[SIMILARITY] C_im_clap: min=-0.8929, max=-0.8929, mean=-0.8929
Epoch 0:   1%|▋                                                                                    | 80/10586 [00:16<35:37,  4.92it/s, v_num=tetn, train/loss=-0.00, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9555, max=0.9872, mean=-0.0000, std=0.2328
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9482, max=0.9893, mean=-0.0000, std=0.2327
[CLAP PIPELINE] Removed 80485 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([79515])
[CLAP PIPELINE] Sample 0 stats: min=-0.9482, max=0.9893, mean=-0.0001, std=0.3301
[CLAP PIPELINE] Sample 0: 1/79515 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1694, max=0.1131, mean=-0.0031, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.9931, max=-0.9931, mean=-0.9931
[SIMILARITY] C_im_clap: min=-0.9801, max=-0.9801, mean=-0.9801
Epoch 0:   1%|▋                                                                                    | 81/10586 [00:16<35:57,  4.87it/s, v_num=tetn, train/loss=-0.00, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9952, max=0.9767, mean=-0.0001, std=0.0806
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9756, mean=-0.0001, std=0.0806
[CLAP PIPELINE] Removed 140475 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([19525])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9756, mean=-0.0008, std=0.2307
[CLAP PIPELINE] Sample 0: 1/19525 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1740, max=0.1323, mean=-0.0027, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.8161, max=-0.8161, mean=-0.8161
[SIMILARITY] C_im_clap: min=-0.8366, max=-0.8366, mean=-0.8366
Epoch 0:   1%|▋                                                                                    | 82/10586 [00:16<36:11,  4.84it/s, v_num=tetn, train/loss=-0.00, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0640, max=0.6390, mean=0.0000, std=0.0066
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.5913, max=0.5054, mean=0.0000, std=0.0059
[CLAP PIPELINE] Removed 152279 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([7721])
[CLAP PIPELINE] Sample 0 stats: min=-0.5913, max=0.5054, mean=0.0003, std=0.0267
[CLAP PIPELINE] Sample 0: 5/7721 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 83/10586 [00:17<36:28,  4.80it/s, v_num=tetn, train/loss=nan.0, lr=2.71e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8165, max=0.7935, mean=-0.0002, std=0.1749
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6182, max=0.5903, mean=-0.0002, std=0.1547
[CLAP PIPELINE] Removed 47652 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([112348])
[CLAP PIPELINE] Sample 0 stats: min=-0.6182, max=0.5903, mean=-0.0002, std=0.1846
[CLAP PIPELINE] Sample 0: 1/112348 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1820, max=0.1265, mean=-0.0028, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.9766, max=-0.9766, mean=-0.9766
[SIMILARITY] C_im_clap: min=-0.8868, max=-0.8868, mean=-0.8868
Epoch 0:   1%|▋                                                                                    | 84/10586 [00:17<36:45,  4.76it/s, v_num=tetn, train/loss=-0.00, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0079, max=1.0084, mean=-0.0001, std=0.1935
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.1769
[CLAP PIPELINE] Removed 39111 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([120889])
[CLAP PIPELINE] Sample 0 stats: min=-0.9668, max=0.9990, mean=-0.0001, std=0.2036
[CLAP PIPELINE] Sample 0: 1/120889 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 85/10586 [00:17<35:52,  4.88it/s, v_num=tetn, train/loss=nan.0, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9893, max=1.0040, mean=0.0002, std=0.0502
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9502, max=0.9209, mean=0.0002, std=0.0501
[CLAP PIPELINE] Removed 157790 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([2210])
[CLAP PIPELINE] Sample 0 stats: min=-0.9502, max=0.9209, mean=0.0153, std=0.4260
[CLAP PIPELINE] Sample 0: 0/2210 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1903, max=0.1326, mean=-0.0019, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=-0.6138, max=-0.6138, mean=-0.6138
[SIMILARITY] C_im_clap: min=-0.6077, max=-0.6077, mean=-0.6077
Epoch 0:   1%|▋                                                                                    | 86/10586 [00:17<36:04,  4.85it/s, v_num=tetn, train/loss=-0.00, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9057, max=0.9394, mean=0.0000, std=0.0707
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0453
[CLAP PIPELINE] Removed 51575 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([108425])
[CLAP PIPELINE] Sample 0 stats: min=-0.4321, max=0.3740, mean=0.0000, std=0.0551
[CLAP PIPELINE] Sample 0: 47/108425 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 87/10586 [00:17<36:07,  4.84it/s, v_num=tetn, train/loss=nan.0, lr=2.72e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7803, max=0.9980, mean=-0.0001, std=0.1884
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7729, max=0.8696, mean=-0.0001, std=0.1881
[CLAP PIPELINE] Removed 0 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([160000])
[CLAP PIPELINE] Sample 0 stats: min=-0.7729, max=0.8696, mean=-0.0001, std=0.1881
[CLAP PIPELINE] Sample 0: 0/160000 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1710, max=0.1165, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.9617, max=-0.9617, mean=-0.9617
[SIMILARITY] C_im_clap: min=-0.9661, max=-0.9661, mean=-0.9661
Epoch 0:   1%|▋                                                                                    | 88/10586 [00:18<36:33,  4.79it/s, v_num=tetn, train/loss=-0.00, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9847, mean=-0.0013, std=0.1384
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9766, max=0.9678, mean=-0.0013, std=0.1385
[CLAP PIPELINE] Removed 145785 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([14215])
[CLAP PIPELINE] Sample 0 stats: min=-0.9766, max=0.9678, mean=-0.0143, std=0.4646
[CLAP PIPELINE] Sample 0: 0/14215 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1961, max=0.1282, mean=-0.0026, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.8929, max=-0.8929, mean=-0.8929
[SIMILARITY] C_im_clap: min=-0.8048, max=-0.8048, mean=-0.8048
Epoch 0:   1%|▋                                                                                    | 89/10586 [00:18<36:50,  4.75it/s, v_num=tetn, train/loss=-0.00, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9994, mean=-0.0028, std=0.1001
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9595, max=0.9692, mean=-0.0028, std=0.1001
[CLAP PIPELINE] Removed 126649 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([33351])
[CLAP PIPELINE] Sample 0 stats: min=-0.9595, max=0.9692, mean=-0.0135, std=0.2190
[CLAP PIPELINE] Sample 0: 478/33351 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1868, max=0.1344, mean=-0.0028, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.8854, max=-0.8854, mean=-0.8854
[SIMILARITY] C_im_clap: min=-0.8946, max=-0.8946, mean=-0.8946
Epoch 0:   1%|▋                                                                                    | 90/10586 [00:19<37:10,  4.71it/s, v_num=tetn, train/loss=-0.00, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=0.9759, mean=-0.0001, std=0.0569
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9688, max=0.9858, mean=-0.0001, std=0.0569
[CLAP PIPELINE] Removed 138074 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([21926])
[CLAP PIPELINE] Sample 0 stats: min=-0.9688, max=0.9858, mean=-0.0008, std=0.1537
[CLAP PIPELINE] Sample 0: 18/21926 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1804, max=0.1207, mean=-0.0030, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.9600, max=-0.9600, mean=-0.9600
[SIMILARITY] C_im_clap: min=-0.9949, max=-0.9949, mean=-0.9949
Epoch 0:   1%|▋                                                                                    | 91/10586 [00:19<37:30,  4.66it/s, v_num=tetn, train/loss=-0.00, lr=2.73e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9838, max=0.9872, mean=0.0001, std=0.5035
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0186, max=1.0156, mean=0.0001, std=0.5015
[CLAP PIPELINE] Removed 48079 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([111921])
[CLAP PIPELINE] Sample 0 stats: min=-1.0186, max=1.0156, mean=0.0002, std=0.5996
[CLAP PIPELINE] Sample 0: 3/111921 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1313, max=0.1193, mean=-0.0024, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.7472, max=-0.7472, mean=-0.7472
[SIMILARITY] C_im_clap: min=-0.7263, max=-0.7263, mean=-0.7263
Epoch 0:   1%|▋                                                                                    | 92/10586 [00:19<37:55,  4.61it/s, v_num=tetn, train/loss=-0.00, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9034, max=1.0000, mean=-0.0000, std=0.1119
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9868, mean=-0.0000, std=0.1119
[CLAP PIPELINE] Removed 63472 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([96528])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9868, mean=-0.0001, std=0.1442
[CLAP PIPELINE] Sample 0: 2/96528 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▋                                                                                    | 93/10586 [00:20<38:03,  4.59it/s, v_num=tetn, train/loss=nan.0, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9234, max=0.9909, mean=-0.0019, std=0.1012
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8613, max=0.9893, mean=-0.0019, std=0.1000
[CLAP PIPELINE] Removed 115693 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([44307])
[CLAP PIPELINE] Sample 0 stats: min=-0.8613, max=0.9893, mean=-0.0069, std=0.1899
[CLAP PIPELINE] Sample 0: 1/44307 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1676, max=0.1281, mean=-0.0026, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.8065, max=-0.8065, mean=-0.8065
[SIMILARITY] C_im_clap: min=-0.8445, max=-0.8445, mean=-0.8445
Epoch 0:   1%|▊                                                                                    | 94/10586 [00:20<38:28,  4.54it/s, v_num=tetn, train/loss=-0.00, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9149, max=0.9979, mean=-0.0002, std=0.1847
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9126, max=0.9985, mean=-0.0002, std=0.1847
[CLAP PIPELINE] Removed 126336 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([33664])
[CLAP PIPELINE] Sample 0 stats: min=-0.9126, max=0.9985, mean=-0.0008, std=0.4028
[CLAP PIPELINE] Sample 0: 3/33664 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 95/10586 [00:21<38:44,  4.51it/s, v_num=tetn, train/loss=nan.0, lr=2.74e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9921, max=0.6521, mean=-0.0003, std=0.1274
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0107, max=0.6460, mean=-0.0003, std=0.1272
[CLAP PIPELINE] Removed 70434 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([89566])
[CLAP PIPELINE] Sample 0 stats: min=-1.0107, max=0.6460, mean=-0.0005, std=0.1700
[CLAP PIPELINE] Sample 0: 2/89566 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1797, max=0.1255, mean=-0.0025, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.8323, max=-0.8323, mean=-0.8323
[SIMILARITY] C_im_clap: min=-0.8026, max=-0.8026, mean=-0.8026
Epoch 0:   1%|▊                                                                                    | 96/10586 [00:21<39:06,  4.47it/s, v_num=tetn, train/loss=-0.00, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0097, max=0.9844, mean=0.0002, std=0.0780
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0264, max=0.9795, mean=0.0002, std=0.0779
[CLAP PIPELINE] Removed 53205 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([106795])
[CLAP PIPELINE] Sample 0 stats: min=-1.0264, max=0.9795, mean=0.0003, std=0.0954
[CLAP PIPELINE] Sample 0: 22/106795 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1815, max=0.1322, mean=-0.0030, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2787, max=14.2787, mean=14.2787
[SIMILARITY] C_ref_clap: min=-0.9626, max=-0.9626, mean=-0.9626
[SIMILARITY] C_im_clap: min=-0.9617, max=-0.9617, mean=-0.9617
Epoch 0:   1%|▊                                                                                    | 97/10586 [00:21<39:19,  4.45it/s, v_num=tetn, train/loss=-0.00, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9991, mean=-0.0000, std=0.2045
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9912, max=0.9946, mean=-0.0000, std=0.2046
[CLAP PIPELINE] Removed 45495 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([114505])
[CLAP PIPELINE] Sample 0 stats: min=-0.9912, max=0.9946, mean=-0.0000, std=0.2419
[CLAP PIPELINE] Sample 0: 203/114505 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 98/10586 [00:22<39:29,  4.43it/s, v_num=tetn, train/loss=nan.0, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.9035, mean=-0.0000, std=0.1785
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.1786
[CLAP PIPELINE] Removed 78921 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([81079])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9038, mean=-0.0000, std=0.2510
[CLAP PIPELINE] Sample 0: 0/81079 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                    | 99/10586 [00:22<39:44,  4.40it/s, v_num=tetn, train/loss=nan.0, lr=2.75e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9433, max=0.9782, mean=-0.0005, std=0.0523
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8989, max=0.8970, mean=-0.0005, std=0.0514
[CLAP PIPELINE] Removed 129995 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([30005])
[CLAP PIPELINE] Sample 0 stats: min=-0.8989, max=0.8970, mean=-0.0026, std=0.1186
[CLAP PIPELINE] Sample 0: 3/30005 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1817, max=0.1287, mean=-0.0028, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=-0.9173, max=-0.9173, mean=-0.9173
[SIMILARITY] C_im_clap: min=-0.8689, max=-0.8689, mean=-0.8689
Epoch 0:   1%|▊                                                                                   | 100/10586 [00:22<39:55,  4.38it/s, v_num=tetn, train/loss=-0.00, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0004, max=0.8341, mean=-0.0000, std=0.0354
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9868, max=0.8340, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Removed 64757 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([95243])
[CLAP PIPELINE] Sample 0 stats: min=-0.9868, max=0.8340, mean=-0.0000, std=0.0458
[CLAP PIPELINE] Sample 0: 870/95243 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 101/10586 [00:22<39:14,  4.45it/s, v_num=tetn, train/loss=nan.0, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8739, max=0.9986, mean=-0.0000, std=0.0380
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0377
[CLAP PIPELINE] Removed 59395 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([100605])
[CLAP PIPELINE] Sample 0 stats: min=-0.8784, max=0.9668, mean=-0.0000, std=0.0475
[CLAP PIPELINE] Sample 0: 2090/100605 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 102/10586 [00:23<39:25,  4.43it/s, v_num=tetn, train/loss=nan.0, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.6688, max=0.7300, mean=-0.0000, std=0.0359
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.6216, max=0.6060, mean=-0.0000, std=0.0346
[CLAP PIPELINE] Removed 127667 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([32333])
[CLAP PIPELINE] Sample 0 stats: min=-0.6216, max=0.6060, mean=-0.0002, std=0.0770
[CLAP PIPELINE] Sample 0: 19/32333 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 103/10586 [00:23<39:42,  4.40it/s, v_num=tetn, train/loss=nan.0, lr=2.76e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0005, max=0.9991, mean=-0.0000, std=0.0704
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0010, max=0.9990, mean=-0.0000, std=0.0704
[CLAP PIPELINE] Removed 95843 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([64157])
[CLAP PIPELINE] Sample 0 stats: min=-1.0010, max=0.9990, mean=-0.0000, std=0.1111
[CLAP PIPELINE] Sample 0: 1/64157 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 104/10586 [00:23<39:58,  4.37it/s, v_num=tetn, train/loss=nan.0, lr=2.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9232, max=1.0015, mean=-0.0001, std=0.1578
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9033, max=0.9556, mean=-0.0001, std=0.1578
[CLAP PIPELINE] Removed 40568 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([119432])
[CLAP PIPELINE] Sample 0 stats: min=-0.9033, max=0.9556, mean=-0.0001, std=0.1826
[CLAP PIPELINE] Sample 0: 0/119432 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 105/10586 [00:24<40:01,  4.36it/s, v_num=tetn, train/loss=nan.0, lr=2.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9997, max=0.9991, mean=-0.0015, std=0.0866
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.9985, mean=-0.0015, std=0.0866
[CLAP PIPELINE] Removed 84761 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([75239])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.9985, mean=-0.0033, std=0.1262
[CLAP PIPELINE] Sample 0: 536/75239 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1806, max=0.1332, mean=-0.0025, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.7329, max=-0.7329, mean=-0.7329
[SIMILARITY] C_im_clap: min=-0.8279, max=-0.8279, mean=-0.8279
Epoch 0:   1%|▊                                                                                   | 106/10586 [00:24<40:08,  4.35it/s, v_num=tetn, train/loss=-0.00, lr=2.77e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9216, max=0.9771, mean=-0.0000, std=0.0947
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8647, max=0.9453, mean=-0.0000, std=0.0916
[CLAP PIPELINE] Removed 77920 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([82080])
[CLAP PIPELINE] Sample 0 stats: min=-0.8647, max=0.9453, mean=-0.0000, std=0.1278
[CLAP PIPELINE] Sample 0: 10/82080 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 107/10586 [00:24<40:16,  4.34it/s, v_num=tetn, train/loss=nan.0, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0137, max=1.0162, mean=-0.0000, std=0.1196
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9604, max=0.9634, mean=-0.0000, std=0.1135
[CLAP PIPELINE] Removed 151832 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([8168])
[CLAP PIPELINE] Sample 0 stats: min=-0.9604, max=0.9634, mean=-0.0004, std=0.5024
[CLAP PIPELINE] Sample 0: 0/8168 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1626, max=0.1327, mean=-0.0033, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=-1.0393, max=-1.0393, mean=-1.0393
[SIMILARITY] C_im_clap: min=-1.0725, max=-1.0725, mean=-1.0725
Epoch 0:   1%|▊                                                                                   | 108/10586 [00:24<40:15,  4.34it/s, v_num=tetn, train/loss=-0.00, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Removed 47228 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([112772])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0273
[CLAP PIPELINE] Sample 0: 10/112772 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 109/10586 [00:25<40:26,  4.32it/s, v_num=tetn, train/loss=nan.0, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8604, max=0.7066, mean=-0.0001, std=0.0305
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0229
[CLAP PIPELINE] Removed 47228 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([112772])
[CLAP PIPELINE] Sample 0 stats: min=-0.4136, max=0.4590, mean=-0.0001, std=0.0273
[CLAP PIPELINE] Sample 0: 10/112772 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▊                                                                                   | 110/10586 [00:25<40:38,  4.30it/s, v_num=tetn, train/loss=nan.0, lr=2.78e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.7465, max=0.9657, mean=-0.0000, std=0.0711
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7476, max=0.9722, mean=-0.0000, std=0.0708
[CLAP PIPELINE] Removed 65190 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([94810])
[CLAP PIPELINE] Sample 0 stats: min=-0.7476, max=0.9722, mean=-0.0001, std=0.0920
[CLAP PIPELINE] Sample 0: 2/94810 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1848, max=0.1265, mean=-0.0028, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.9068, max=-0.9068, mean=-0.9068
[SIMILARITY] C_im_clap: min=-0.8780, max=-0.8780, mean=-0.8780
Epoch 0:   1%|▉                                                                                   | 111/10586 [00:25<40:41,  4.29it/s, v_num=tetn, train/loss=-0.00, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.8718, max=0.9936, mean=-0.0003, std=0.0920
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8423, max=0.9917, mean=-0.0003, std=0.0919
[CLAP PIPELINE] Removed 57594 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([102406])
[CLAP PIPELINE] Sample 0 stats: min=-0.8423, max=0.9917, mean=-0.0005, std=0.1149
[CLAP PIPELINE] Sample 0: 29/102406 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 112/10586 [00:26<40:42,  4.29it/s, v_num=tetn, train/loss=nan.0, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0002, max=0.8367, mean=-0.0003, std=0.0839
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9971, max=0.8145, mean=-0.0003, std=0.0839
[CLAP PIPELINE] Removed 120295 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([39705])
[CLAP PIPELINE] Sample 0 stats: min=-0.9971, max=0.8145, mean=-0.0012, std=0.1683
[CLAP PIPELINE] Sample 0: 1/39705 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1883, max=0.1451, mean=-0.0021, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=-0.6483, max=-0.6483, mean=-0.6483
[SIMILARITY] C_im_clap: min=-0.6766, max=-0.6766, mean=-0.6766
Epoch 0:   1%|▉                                                                                   | 113/10586 [00:26<40:48,  4.28it/s, v_num=tetn, train/loss=-0.00, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9967, max=0.8892, mean=-0.0005, std=0.1574
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9980, max=0.8481, mean=-0.0005, std=0.1570
[CLAP PIPELINE] Removed 12271 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([147729])
[CLAP PIPELINE] Sample 0 stats: min=-0.9980, max=0.8481, mean=-0.0006, std=0.1635
[CLAP PIPELINE] Sample 0: 0/147729 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 114/10586 [00:26<40:40,  4.29it/s, v_num=tetn, train/loss=nan.0, lr=2.79e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9042, max=0.9983, mean=-0.0000, std=0.1139
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9048, max=0.9966, mean=-0.0000, std=0.1140
[CLAP PIPELINE] Removed 124315 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([35685])
[CLAP PIPELINE] Sample 0 stats: min=-0.9048, max=0.9966, mean=-0.0001, std=0.2413
[CLAP PIPELINE] Sample 0: 0/35685 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                    | 115/10586 [00:26<40:47,  4.28it/s, v_num=tetn, train/loss=nan.0, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9555, max=0.9872, mean=-0.0000, std=0.2328
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9482, max=0.9893, mean=-0.0000, std=0.2327
[CLAP PIPELINE] Removed 80485 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([79515])
[CLAP PIPELINE] Sample 0 stats: min=-0.9482, max=0.9893, mean=-0.0001, std=0.3301
[CLAP PIPELINE] Sample 0: 1/79515 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1694, max=0.1131, mean=-0.0031, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=-0.9705, max=-0.9705, mean=-0.9705
[SIMILARITY] C_im_clap: min=-0.9896, max=-0.9896, mean=-0.9896
Epoch 0:   1%|▉                                                                                    | 116/10586 [00:27<40:46,  4.28it/s, v_num=tetn, train/loss=-0.00, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0006, max=0.7591, mean=-0.0002, std=0.1469
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9961, max=0.7500, mean=-0.0002, std=0.1467
[CLAP PIPELINE] Removed 8289 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([151711])
[CLAP PIPELINE] Sample 0 stats: min=-0.9961, max=0.7500, mean=-0.0002, std=0.1506
[CLAP PIPELINE] Sample 0: 1/151711 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1771, max=0.1325, mean=-0.0026, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.8296, max=-0.8296, mean=-0.8296
[SIMILARITY] C_im_clap: min=-0.8850, max=-0.8850, mean=-0.8850
Epoch 0:   1%|▉                                                                                    | 117/10586 [00:27<40:47,  4.28it/s, v_num=tetn, train/loss=-0.00, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9953, max=0.9269, mean=-0.0007, std=0.0796
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9307, max=0.9238, mean=-0.0007, std=0.0796
[CLAP PIPELINE] Removed 119925 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([40075])
[CLAP PIPELINE] Sample 0 stats: min=-0.9307, max=0.9238, mean=-0.0027, std=0.1591
[CLAP PIPELINE] Sample 0: 1/40075 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1888, max=0.1265, mean=-0.0025, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.8218, max=-0.8218, mean=-0.8218
[SIMILARITY] C_im_clap: min=-0.8201, max=-0.8201, mean=-0.8201
Epoch 0:   1%|▉                                                                                    | 118/10586 [00:27<40:54,  4.26it/s, v_num=tetn, train/loss=-0.00, lr=2.8e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9713, max=0.9290, mean=-0.0000, std=0.1432
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.7432, max=0.7358, mean=-0.0000, std=0.1395
[CLAP PIPELINE] Removed 67813 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([92187])
[CLAP PIPELINE] Sample 0 stats: min=-0.7432, max=0.7358, mean=-0.0000, std=0.1838
[CLAP PIPELINE] Sample 0: 12/92187 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 119/10586 [00:27<40:11,  4.34it/s, v_num=tetn, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9976, max=0.9832, mean=-0.0000, std=0.0619
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0039, max=1.0049, mean=-0.0000, std=0.0618
[CLAP PIPELINE] Removed 139329 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([20671])
[CLAP PIPELINE] Sample 0 stats: min=-1.0039, max=1.0049, mean=-0.0001, std=0.1720
[CLAP PIPELINE] Sample 0: 115/20671 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1724, max=0.1303, mean=-0.0023, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=-0.7298, max=-0.7298, mean=-0.7298
[SIMILARITY] C_im_clap: min=-0.7089, max=-0.7089, mean=-0.7089
Epoch 0:   1%|▉                                                                                   | 120/10586 [00:27<40:17,  4.33it/s, v_num=tetn, train/loss=-0.00, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9559, max=0.6156, mean=0.0001, std=0.0153
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.8501, max=0.6035, mean=0.0001, std=0.0152
[CLAP PIPELINE] Removed 150049 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([9951])
[CLAP PIPELINE] Sample 0 stats: min=-0.8501, max=0.6035, mean=0.0008, std=0.0610
[CLAP PIPELINE] Sample 0: 17/9951 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 121/10586 [00:27<40:15,  4.33it/s, v_num=tetn, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0003, max=0.8273, mean=-0.0025, std=0.1678
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9990, max=0.8276, mean=-0.0025, std=0.1678
[CLAP PIPELINE] Removed 24965 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([135035])
[CLAP PIPELINE] Sample 0 stats: min=-0.9990, max=0.8276, mean=-0.0029, std=0.1827
[CLAP PIPELINE] Sample 0: 4/135035 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 122/10586 [00:28<40:22,  4.32it/s, v_num=tetn, train/loss=nan.0, lr=2.81e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9729, max=0.8797, mean=-0.0026, std=0.1368
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9287, max=0.8765, mean=-0.0026, std=0.1364
[CLAP PIPELINE] Removed 100083 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([59917])
[CLAP PIPELINE] Sample 0 stats: min=-0.9287, max=0.8765, mean=-0.0070, std=0.2228
[CLAP PIPELINE] Sample 0: 490/59917 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1749, max=0.1170, mean=-0.0027, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2299, max=14.2299, mean=14.2299
[SIMILARITY] C_ref_clap: min=-0.8388, max=-0.8388, mean=-0.8388
[SIMILARITY] C_im_clap: min=-0.8436, max=-0.8436, mean=-0.8436
Epoch 0:   1%|▉                                                                                   | 123/10586 [00:28<40:25,  4.31it/s, v_num=tetn, train/loss=-0.00, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0009, max=0.8496, mean=-0.0000, std=0.0451
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0450
[CLAP PIPELINE] Removed 101885 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([58115])
[CLAP PIPELINE] Sample 0 stats: min=-1.0176, max=0.8301, mean=-0.0000, std=0.0746
[CLAP PIPELINE] Sample 0: 7/58115 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 124/10586 [00:28<40:26,  4.31it/s, v_num=tetn, train/loss=nan.0, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9995, max=0.9657, mean=0.0000, std=0.2225
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9663, mean=0.0000, std=0.2227
[CLAP PIPELINE] Removed 41608 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([118392])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9663, mean=0.0000, std=0.2588
[CLAP PIPELINE] Sample 0: 1/118392 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2439, max=14.2439, mean=14.2439
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|▉                                                                                   | 125/10586 [00:28<40:23,  4.32it/s, v_num=tetn, train/loss=nan.0, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9996, max=0.6950, mean=-0.0000, std=0.0353
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9976, max=0.6948, mean=-0.0000, std=0.0353
[CLAP PIPELINE] Removed 125324 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([34676])
[CLAP PIPELINE] Sample 0 stats: min=-0.9976, max=0.6948, mean=-0.0000, std=0.0759
[CLAP PIPELINE] Sample 0: 4/34676 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1882, max=0.1245, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.9112, max=-0.9112, mean=-0.9112
[SIMILARITY] C_im_clap: min=-0.9365, max=-0.9365, mean=-0.9365
Epoch 0:   1%|▉                                                                                   | 126/10586 [00:29<40:27,  4.31it/s, v_num=tetn, train/loss=-0.00, lr=2.82e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9178, max=0.9936, mean=0.1238, std=0.4039
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9248, max=0.9927, mean=0.1239, std=0.4033
[CLAP PIPELINE] Removed 22881 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([137119])
[CLAP PIPELINE] Sample 0 stats: min=-0.9248, max=0.9927, mean=0.1445, std=0.4324
[CLAP PIPELINE] Sample 0: 1/137119 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2718, max=14.2718, mean=14.2718
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 127/10586 [00:29<40:38,  4.29it/s, v_num=tetn, train/loss=nan.0, lr=2.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9941, max=0.9983, mean=0.0005, std=0.1294
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9941, max=0.9985, mean=0.0005, std=0.1294
[CLAP PIPELINE] Removed 134275 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([25725])
[CLAP PIPELINE] Sample 0 stats: min=-0.9941, max=0.9985, mean=0.0033, std=0.3228
[CLAP PIPELINE] Sample 0: 6346/25725 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2648, max=14.2648, mean=14.2648
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 128/10586 [00:29<40:47,  4.27it/s, v_num=tetn, train/loss=nan.0, lr=2.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9908, max=1.0015, mean=-0.0000, std=0.0681
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9399, max=0.9409, mean=-0.0000, std=0.0680
[CLAP PIPELINE] Removed 109222 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([50778])
[CLAP PIPELINE] Sample 0 stats: min=-0.9399, max=0.9409, mean=-0.0001, std=0.1207
[CLAP PIPELINE] Sample 0: 0/50778 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1831, max=0.1315, mean=-0.0029, std=0.0441
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2578, max=14.2578, mean=14.2578
[SIMILARITY] C_ref_clap: min=-0.8929, max=-0.8929, mean=-0.8929
[SIMILARITY] C_im_clap: min=-0.9426, max=-0.9426, mean=-0.9426
Epoch 0:   1%|█                                                                                   | 129/10586 [00:30<40:52,  4.26it/s, v_num=tetn, train/loss=-0.00, lr=2.83e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-0.9488, max=0.8610, mean=0.0000, std=0.0401
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.0393
[CLAP PIPELINE] Removed 148710 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([11290])
[CLAP PIPELINE] Sample 0 stats: min=-0.9160, max=0.8511, mean=0.0000, std=0.1481
[CLAP PIPELINE] Sample 0: 0/11290 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] CLAP embedding seems valid (no NaN/Inf)
[CLAP PIPELINE] CLAP embedding stats: min=-0.1774, max=0.1362, mean=-0.0027, std=0.0442
[CLAP PIPELINE] CLAP embedding norms: min=1.0000, max=1.0000, mean=1.0000
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap 1.0000
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2508, max=14.2508, mean=14.2508
[SIMILARITY] C_ref_clap: min=-0.8929, max=-0.8929, mean=-0.8929
[SIMILARITY] C_im_clap: min=-0.9024, max=-0.9024, mean=-0.9024
Epoch 0:   1%|█                                                                                   | 130/10586 [00:30<40:58,  4.25it/s, v_num=tetn, train/loss=-0.00, lr=2.84e-5][CLAP PIPELINE] Input audio shape: torch.Size([1, 320000])
[CLAP PIPELINE] Input audio stats: min=-1.0007, max=0.9994, mean=0.0000, std=0.2048
[CLAP PIPELINE] After resampling: shape=torch.Size([1, 160000])
[CLAP PIPELINE] After resampling stats: min=-1.0000, max=0.9995, mean=0.0000, std=0.2048
[CLAP PIPELINE] Removed 88054 trailing zeros from sample 0
[CLAP PIPELINE] Sample 0 waveform: shape=torch.Size([71946])
[CLAP PIPELINE] Sample 0 stats: min=-1.0000, max=0.9995, mean=0.0000, std=0.3054
[CLAP PIPELINE] Sample 0: 3/71946 near-zero values
[CLAP PIPELINE] About to call get_audio_embedding with 1 waveform dicts
[CLAP PIPELINE] CLAP embedding shape: torch.Size([1, 512])
[CLAP PIPELINE] WARNING: CLAP embedding has 512/512 NaN values and 0/512 Inf values
[CLAP PIPELINE] CLAP embedding stats: min=nan, max=nan, mean=nan, std=nan
[CLAP PIPELINE] CLAP embedding norms: min=nan, max=nan, mean=nan
[SIMILARITY] Shapes: y_im torch.Size([1, 512]), y_ref torch.Size([1, 512]), y_clap torch.Size([1, 512])
[SIMILARITY] Norms: y_im 1.0000, y_ref 1.0000, y_clap nan
[SIMILARITY] Temps: tau 0.070000, cross_temp 0.070000
[SIMILARITY] WARNING: y_clap has 512/512 NaN and 0/512 Inf
[SIMILARITY] Similarity matrices: C_qvim torch.Size([1, 1]), C_ref_clap torch.Size([1, 1]), C_im_clap torch.Size([1, 1])
[SIMILARITY] C_qvim: min=14.2369, max=14.2369, mean=14.2369
[SIMILARITY] C_ref_clap: min=nan, max=nan, mean=nan
[SIMILARITY] C_im_clap: min=nan, max=nan, mean=nan
[SIMILARITY] WARNING: C_ref_clap_log has 1/1 NaN values
[SIMILARITY] WARNING: C_im_clap_log has 1/1 NaN values
Epoch 0:   1%|█                                                                                   | 131/10586 [00:30<41:07,  4.24it/s, v_num=tetn, train/loss=nan.0, lr=2.84e-5]

Detected KeyboardInterrupt, attempting graceful shutdown ...
