=== AudioLDM Parameters ===
logvar, Shape: torch.Size([1000]), Params: 1.0K, Requires grad: False
model.diffusion_model.time_embed.0.weight, Shape: torch.Size([768, 192]), Params: 147.5K, Requires grad: False
model.diffusion_model.time_embed.0.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
model.diffusion_model.time_embed.2.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
model.diffusion_model.time_embed.2.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
model.diffusion_model.film_emb.weight, Shape: torch.Size([768, 512]), Params: 393.2K, Requires grad: False
model.diffusion_model.film_emb.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
model.diffusion_model.input_blocks.0.0.weight, Shape: torch.Size([192, 8, 3, 3]), Params: 13.8K, Requires grad: False
model.diffusion_model.input_blocks.0.0.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.1.0.in_layers.0.weight, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.1.0.in_layers.0.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.1.0.in_layers.2.weight, Shape: torch.Size([192, 192, 3, 3]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.1.0.in_layers.2.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.1.0.emb_layers.1.weight, Shape: torch.Size([192, 1536]), Params: 294.9K, Requires grad: False
model.diffusion_model.input_blocks.1.0.emb_layers.1.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.1.0.out_layers.0.weight, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.1.0.out_layers.0.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.1.0.out_layers.3.weight, Shape: torch.Size([192, 192, 3, 3]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.1.0.out_layers.3.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.2.0.in_layers.0.weight, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.2.0.in_layers.0.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.2.0.in_layers.2.weight, Shape: torch.Size([192, 192, 3, 3]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.2.0.in_layers.2.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.2.0.emb_layers.1.weight, Shape: torch.Size([192, 1536]), Params: 294.9K, Requires grad: False
model.diffusion_model.input_blocks.2.0.emb_layers.1.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.2.0.out_layers.0.weight, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.2.0.out_layers.0.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.2.0.out_layers.3.weight, Shape: torch.Size([192, 192, 3, 3]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.2.0.out_layers.3.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.3.0.op.weight, Shape: torch.Size([192, 192, 3, 3]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.3.0.op.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.4.0.in_layers.0.weight, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.4.0.in_layers.0.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.input_blocks.4.0.in_layers.2.weight, Shape: torch.Size([384, 192, 3, 3]), Params: 663.6K, Requires grad: False
model.diffusion_model.input_blocks.4.0.in_layers.2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.0.emb_layers.1.weight, Shape: torch.Size([384, 1536]), Params: 589.8K, Requires grad: False
model.diffusion_model.input_blocks.4.0.emb_layers.1.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.0.out_layers.0.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.0.out_layers.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.0.out_layers.3.weight, Shape: torch.Size([384, 384, 3, 3]), Params: 1327.1K, Requires grad: False
model.diffusion_model.input_blocks.4.0.out_layers.3.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.0.skip_connection.weight, Shape: torch.Size([384, 192, 1, 1]), Params: 73.7K, Requires grad: False
model.diffusion_model.input_blocks.4.0.skip_connection.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.norm.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.norm.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.proj_in.weight, Shape: torch.Size([384, 384, 1, 1]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.4.1.proj_in.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([3072, 384]), Params: 1179.6K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([384, 1536]), Params: 589.8K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.4.1.proj_out.weight, Shape: torch.Size([384, 384, 1, 1]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.4.1.proj_out.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.0.in_layers.0.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.0.in_layers.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.0.in_layers.2.weight, Shape: torch.Size([384, 384, 3, 3]), Params: 1327.1K, Requires grad: False
model.diffusion_model.input_blocks.5.0.in_layers.2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.0.emb_layers.1.weight, Shape: torch.Size([384, 1536]), Params: 589.8K, Requires grad: False
model.diffusion_model.input_blocks.5.0.emb_layers.1.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.0.out_layers.0.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.0.out_layers.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.0.out_layers.3.weight, Shape: torch.Size([384, 384, 3, 3]), Params: 1327.1K, Requires grad: False
model.diffusion_model.input_blocks.5.0.out_layers.3.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.norm.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.norm.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.proj_in.weight, Shape: torch.Size([384, 384, 1, 1]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.5.1.proj_in.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([3072, 384]), Params: 1179.6K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([384, 1536]), Params: 589.8K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.5.1.proj_out.weight, Shape: torch.Size([384, 384, 1, 1]), Params: 147.5K, Requires grad: False
model.diffusion_model.input_blocks.5.1.proj_out.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.6.0.op.weight, Shape: torch.Size([384, 384, 3, 3]), Params: 1327.1K, Requires grad: False
model.diffusion_model.input_blocks.6.0.op.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.7.0.in_layers.0.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.7.0.in_layers.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.input_blocks.7.0.in_layers.2.weight, Shape: torch.Size([576, 384, 3, 3]), Params: 1990.7K, Requires grad: False
model.diffusion_model.input_blocks.7.0.in_layers.2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.0.emb_layers.1.weight, Shape: torch.Size([576, 1536]), Params: 884.7K, Requires grad: False
model.diffusion_model.input_blocks.7.0.emb_layers.1.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.0.out_layers.0.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.0.out_layers.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.0.out_layers.3.weight, Shape: torch.Size([576, 576, 3, 3]), Params: 2986.0K, Requires grad: False
model.diffusion_model.input_blocks.7.0.out_layers.3.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.0.skip_connection.weight, Shape: torch.Size([576, 384, 1, 1]), Params: 221.2K, Requires grad: False
model.diffusion_model.input_blocks.7.0.skip_connection.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.norm.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.norm.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.proj_in.weight, Shape: torch.Size([576, 576, 1, 1]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.7.1.proj_in.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([4608, 576]), Params: 2654.2K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([4608]), Params: 4.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([576, 2304]), Params: 1327.1K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.7.1.proj_out.weight, Shape: torch.Size([576, 576, 1, 1]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.7.1.proj_out.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.0.in_layers.0.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.0.in_layers.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.0.in_layers.2.weight, Shape: torch.Size([576, 576, 3, 3]), Params: 2986.0K, Requires grad: False
model.diffusion_model.input_blocks.8.0.in_layers.2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.0.emb_layers.1.weight, Shape: torch.Size([576, 1536]), Params: 884.7K, Requires grad: False
model.diffusion_model.input_blocks.8.0.emb_layers.1.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.0.out_layers.0.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.0.out_layers.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.0.out_layers.3.weight, Shape: torch.Size([576, 576, 3, 3]), Params: 2986.0K, Requires grad: False
model.diffusion_model.input_blocks.8.0.out_layers.3.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.norm.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.norm.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.proj_in.weight, Shape: torch.Size([576, 576, 1, 1]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.8.1.proj_in.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([4608, 576]), Params: 2654.2K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([4608]), Params: 4.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([576, 2304]), Params: 1327.1K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.8.1.proj_out.weight, Shape: torch.Size([576, 576, 1, 1]), Params: 331.8K, Requires grad: False
model.diffusion_model.input_blocks.8.1.proj_out.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.9.0.op.weight, Shape: torch.Size([576, 576, 3, 3]), Params: 2986.0K, Requires grad: False
model.diffusion_model.input_blocks.9.0.op.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.10.0.in_layers.0.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.10.0.in_layers.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.input_blocks.10.0.in_layers.2.weight, Shape: torch.Size([960, 576, 3, 3]), Params: 4976.6K, Requires grad: False
model.diffusion_model.input_blocks.10.0.in_layers.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.0.emb_layers.1.weight, Shape: torch.Size([960, 1536]), Params: 1474.6K, Requires grad: False
model.diffusion_model.input_blocks.10.0.emb_layers.1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.0.out_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.0.out_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.0.out_layers.3.weight, Shape: torch.Size([960, 960, 3, 3]), Params: 8294.4K, Requires grad: False
model.diffusion_model.input_blocks.10.0.out_layers.3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.0.skip_connection.weight, Shape: torch.Size([960, 576, 1, 1]), Params: 553.0K, Requires grad: False
model.diffusion_model.input_blocks.10.0.skip_connection.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.norm.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.norm.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.proj_in.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.10.1.proj_in.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([7680, 960]), Params: 7372.8K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([7680]), Params: 7.7K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([960, 3840]), Params: 3686.4K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.10.1.proj_out.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.10.1.proj_out.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.0.in_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.0.in_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.0.in_layers.2.weight, Shape: torch.Size([960, 960, 3, 3]), Params: 8294.4K, Requires grad: False
model.diffusion_model.input_blocks.11.0.in_layers.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.0.emb_layers.1.weight, Shape: torch.Size([960, 1536]), Params: 1474.6K, Requires grad: False
model.diffusion_model.input_blocks.11.0.emb_layers.1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.0.out_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.0.out_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.0.out_layers.3.weight, Shape: torch.Size([960, 960, 3, 3]), Params: 8294.4K, Requires grad: False
model.diffusion_model.input_blocks.11.0.out_layers.3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.norm.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.norm.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.proj_in.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.11.1.proj_in.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([7680, 960]), Params: 7372.8K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([7680]), Params: 7.7K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([960, 3840]), Params: 3686.4K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.input_blocks.11.1.proj_out.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.input_blocks.11.1.proj_out.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.0.in_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.0.in_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.0.in_layers.2.weight, Shape: torch.Size([960, 960, 3, 3]), Params: 8294.4K, Requires grad: False
model.diffusion_model.middle_block.0.in_layers.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.0.emb_layers.1.weight, Shape: torch.Size([960, 1536]), Params: 1474.6K, Requires grad: False
model.diffusion_model.middle_block.0.emb_layers.1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.0.out_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.0.out_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.0.out_layers.3.weight, Shape: torch.Size([960, 960, 3, 3]), Params: 8294.4K, Requires grad: False
model.diffusion_model.middle_block.0.out_layers.3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.norm.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.norm.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.proj_in.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.middle_block.1.proj_in.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([7680, 960]), Params: 7372.8K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([7680]), Params: 7.7K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([960, 3840]), Params: 3686.4K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.1.proj_out.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.middle_block.1.proj_out.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.2.in_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.2.in_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.2.in_layers.2.weight, Shape: torch.Size([960, 960, 3, 3]), Params: 8294.4K, Requires grad: False
model.diffusion_model.middle_block.2.in_layers.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.2.emb_layers.1.weight, Shape: torch.Size([960, 1536]), Params: 1474.6K, Requires grad: False
model.diffusion_model.middle_block.2.emb_layers.1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.2.out_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.2.out_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.middle_block.2.out_layers.3.weight, Shape: torch.Size([960, 960, 3, 3]), Params: 8294.4K, Requires grad: False
model.diffusion_model.middle_block.2.out_layers.3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.0.in_layers.0.weight, Shape: torch.Size([1920]), Params: 1.9K, Requires grad: False
model.diffusion_model.output_blocks.0.0.in_layers.0.bias, Shape: torch.Size([1920]), Params: 1.9K, Requires grad: False
model.diffusion_model.output_blocks.0.0.in_layers.2.weight, Shape: torch.Size([960, 1920, 3, 3]), Params: 16588.8K, Requires grad: False
model.diffusion_model.output_blocks.0.0.in_layers.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.0.emb_layers.1.weight, Shape: torch.Size([960, 1536]), Params: 1474.6K, Requires grad: False
model.diffusion_model.output_blocks.0.0.emb_layers.1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.0.out_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.0.out_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.0.out_layers.3.weight, Shape: torch.Size([960, 960, 3, 3]), Params: 8294.4K, Requires grad: False
model.diffusion_model.output_blocks.0.0.out_layers.3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.0.skip_connection.weight, Shape: torch.Size([960, 1920, 1, 1]), Params: 1843.2K, Requires grad: False
model.diffusion_model.output_blocks.0.0.skip_connection.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.norm.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.norm.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.proj_in.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.0.1.proj_in.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([7680, 960]), Params: 7372.8K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([7680]), Params: 7.7K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([960, 3840]), Params: 3686.4K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.0.1.proj_out.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.0.1.proj_out.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.0.in_layers.0.weight, Shape: torch.Size([1920]), Params: 1.9K, Requires grad: False
model.diffusion_model.output_blocks.1.0.in_layers.0.bias, Shape: torch.Size([1920]), Params: 1.9K, Requires grad: False
model.diffusion_model.output_blocks.1.0.in_layers.2.weight, Shape: torch.Size([960, 1920, 3, 3]), Params: 16588.8K, Requires grad: False
model.diffusion_model.output_blocks.1.0.in_layers.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.0.emb_layers.1.weight, Shape: torch.Size([960, 1536]), Params: 1474.6K, Requires grad: False
model.diffusion_model.output_blocks.1.0.emb_layers.1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.0.out_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.0.out_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.0.out_layers.3.weight, Shape: torch.Size([960, 960, 3, 3]), Params: 8294.4K, Requires grad: False
model.diffusion_model.output_blocks.1.0.out_layers.3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.0.skip_connection.weight, Shape: torch.Size([960, 1920, 1, 1]), Params: 1843.2K, Requires grad: False
model.diffusion_model.output_blocks.1.0.skip_connection.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.norm.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.norm.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.proj_in.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.1.1.proj_in.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([7680, 960]), Params: 7372.8K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([7680]), Params: 7.7K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([960, 3840]), Params: 3686.4K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.1.1.proj_out.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.1.1.proj_out.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.0.in_layers.0.weight, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
model.diffusion_model.output_blocks.2.0.in_layers.0.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
model.diffusion_model.output_blocks.2.0.in_layers.2.weight, Shape: torch.Size([960, 1536, 3, 3]), Params: 13271.0K, Requires grad: False
model.diffusion_model.output_blocks.2.0.in_layers.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.0.emb_layers.1.weight, Shape: torch.Size([960, 1536]), Params: 1474.6K, Requires grad: False
model.diffusion_model.output_blocks.2.0.emb_layers.1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.0.out_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.0.out_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.0.out_layers.3.weight, Shape: torch.Size([960, 960, 3, 3]), Params: 8294.4K, Requires grad: False
model.diffusion_model.output_blocks.2.0.out_layers.3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.0.skip_connection.weight, Shape: torch.Size([960, 1536, 1, 1]), Params: 1474.6K, Requires grad: False
model.diffusion_model.output_blocks.2.0.skip_connection.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.norm.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.norm.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.proj_in.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.2.1.proj_in.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([7680, 960]), Params: 7372.8K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([7680]), Params: 7.7K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([960, 3840]), Params: 3686.4K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([960, 960]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.1.proj_out.weight, Shape: torch.Size([960, 960, 1, 1]), Params: 921.6K, Requires grad: False
model.diffusion_model.output_blocks.2.1.proj_out.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.2.2.conv.weight, Shape: torch.Size([960, 960, 3, 3]), Params: 8294.4K, Requires grad: False
model.diffusion_model.output_blocks.2.2.conv.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.3.0.in_layers.0.weight, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
model.diffusion_model.output_blocks.3.0.in_layers.0.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
model.diffusion_model.output_blocks.3.0.in_layers.2.weight, Shape: torch.Size([576, 1536, 3, 3]), Params: 7962.6K, Requires grad: False
model.diffusion_model.output_blocks.3.0.in_layers.2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.0.emb_layers.1.weight, Shape: torch.Size([576, 1536]), Params: 884.7K, Requires grad: False
model.diffusion_model.output_blocks.3.0.emb_layers.1.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.0.out_layers.0.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.0.out_layers.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.0.out_layers.3.weight, Shape: torch.Size([576, 576, 3, 3]), Params: 2986.0K, Requires grad: False
model.diffusion_model.output_blocks.3.0.out_layers.3.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.0.skip_connection.weight, Shape: torch.Size([576, 1536, 1, 1]), Params: 884.7K, Requires grad: False
model.diffusion_model.output_blocks.3.0.skip_connection.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.norm.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.norm.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.proj_in.weight, Shape: torch.Size([576, 576, 1, 1]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.3.1.proj_in.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([4608, 576]), Params: 2654.2K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([4608]), Params: 4.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([576, 2304]), Params: 1327.1K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.3.1.proj_out.weight, Shape: torch.Size([576, 576, 1, 1]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.3.1.proj_out.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.0.in_layers.0.weight, Shape: torch.Size([1152]), Params: 1.2K, Requires grad: False
model.diffusion_model.output_blocks.4.0.in_layers.0.bias, Shape: torch.Size([1152]), Params: 1.2K, Requires grad: False
model.diffusion_model.output_blocks.4.0.in_layers.2.weight, Shape: torch.Size([576, 1152, 3, 3]), Params: 5972.0K, Requires grad: False
model.diffusion_model.output_blocks.4.0.in_layers.2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.0.emb_layers.1.weight, Shape: torch.Size([576, 1536]), Params: 884.7K, Requires grad: False
model.diffusion_model.output_blocks.4.0.emb_layers.1.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.0.out_layers.0.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.0.out_layers.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.0.out_layers.3.weight, Shape: torch.Size([576, 576, 3, 3]), Params: 2986.0K, Requires grad: False
model.diffusion_model.output_blocks.4.0.out_layers.3.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.0.skip_connection.weight, Shape: torch.Size([576, 1152, 1, 1]), Params: 663.6K, Requires grad: False
model.diffusion_model.output_blocks.4.0.skip_connection.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.norm.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.norm.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.proj_in.weight, Shape: torch.Size([576, 576, 1, 1]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.4.1.proj_in.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([4608, 576]), Params: 2654.2K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([4608]), Params: 4.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([576, 2304]), Params: 1327.1K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.4.1.proj_out.weight, Shape: torch.Size([576, 576, 1, 1]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.4.1.proj_out.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.0.in_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.5.0.in_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.5.0.in_layers.2.weight, Shape: torch.Size([576, 960, 3, 3]), Params: 4976.6K, Requires grad: False
model.diffusion_model.output_blocks.5.0.in_layers.2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.0.emb_layers.1.weight, Shape: torch.Size([576, 1536]), Params: 884.7K, Requires grad: False
model.diffusion_model.output_blocks.5.0.emb_layers.1.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.0.out_layers.0.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.0.out_layers.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.0.out_layers.3.weight, Shape: torch.Size([576, 576, 3, 3]), Params: 2986.0K, Requires grad: False
model.diffusion_model.output_blocks.5.0.out_layers.3.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.0.skip_connection.weight, Shape: torch.Size([576, 960, 1, 1]), Params: 553.0K, Requires grad: False
model.diffusion_model.output_blocks.5.0.skip_connection.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.norm.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.norm.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.proj_in.weight, Shape: torch.Size([576, 576, 1, 1]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.5.1.proj_in.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([4608, 576]), Params: 2654.2K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([4608]), Params: 4.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([576, 2304]), Params: 1327.1K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([576, 576]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.1.proj_out.weight, Shape: torch.Size([576, 576, 1, 1]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.5.1.proj_out.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.5.2.conv.weight, Shape: torch.Size([576, 576, 3, 3]), Params: 2986.0K, Requires grad: False
model.diffusion_model.output_blocks.5.2.conv.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.6.0.in_layers.0.weight, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.6.0.in_layers.0.bias, Shape: torch.Size([960]), Params: 1.0K, Requires grad: False
model.diffusion_model.output_blocks.6.0.in_layers.2.weight, Shape: torch.Size([384, 960, 3, 3]), Params: 3317.8K, Requires grad: False
model.diffusion_model.output_blocks.6.0.in_layers.2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.0.emb_layers.1.weight, Shape: torch.Size([384, 1536]), Params: 589.8K, Requires grad: False
model.diffusion_model.output_blocks.6.0.emb_layers.1.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.0.out_layers.0.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.0.out_layers.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.0.out_layers.3.weight, Shape: torch.Size([384, 384, 3, 3]), Params: 1327.1K, Requires grad: False
model.diffusion_model.output_blocks.6.0.out_layers.3.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.0.skip_connection.weight, Shape: torch.Size([384, 960, 1, 1]), Params: 368.6K, Requires grad: False
model.diffusion_model.output_blocks.6.0.skip_connection.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.norm.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.norm.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.proj_in.weight, Shape: torch.Size([384, 384, 1, 1]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.6.1.proj_in.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([3072, 384]), Params: 1179.6K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([384, 1536]), Params: 589.8K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.6.1.proj_out.weight, Shape: torch.Size([384, 384, 1, 1]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.6.1.proj_out.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.0.in_layers.0.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
model.diffusion_model.output_blocks.7.0.in_layers.0.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
model.diffusion_model.output_blocks.7.0.in_layers.2.weight, Shape: torch.Size([384, 768, 3, 3]), Params: 2654.2K, Requires grad: False
model.diffusion_model.output_blocks.7.0.in_layers.2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.0.emb_layers.1.weight, Shape: torch.Size([384, 1536]), Params: 589.8K, Requires grad: False
model.diffusion_model.output_blocks.7.0.emb_layers.1.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.0.out_layers.0.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.0.out_layers.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.0.out_layers.3.weight, Shape: torch.Size([384, 384, 3, 3]), Params: 1327.1K, Requires grad: False
model.diffusion_model.output_blocks.7.0.out_layers.3.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.0.skip_connection.weight, Shape: torch.Size([384, 768, 1, 1]), Params: 294.9K, Requires grad: False
model.diffusion_model.output_blocks.7.0.skip_connection.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.norm.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.norm.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.proj_in.weight, Shape: torch.Size([384, 384, 1, 1]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.7.1.proj_in.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([3072, 384]), Params: 1179.6K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([384, 1536]), Params: 589.8K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.7.1.proj_out.weight, Shape: torch.Size([384, 384, 1, 1]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.7.1.proj_out.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.0.in_layers.0.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.8.0.in_layers.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.8.0.in_layers.2.weight, Shape: torch.Size([384, 576, 3, 3]), Params: 1990.7K, Requires grad: False
model.diffusion_model.output_blocks.8.0.in_layers.2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.0.emb_layers.1.weight, Shape: torch.Size([384, 1536]), Params: 589.8K, Requires grad: False
model.diffusion_model.output_blocks.8.0.emb_layers.1.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.0.out_layers.0.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.0.out_layers.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.0.out_layers.3.weight, Shape: torch.Size([384, 384, 3, 3]), Params: 1327.1K, Requires grad: False
model.diffusion_model.output_blocks.8.0.out_layers.3.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.0.skip_connection.weight, Shape: torch.Size([384, 576, 1, 1]), Params: 221.2K, Requires grad: False
model.diffusion_model.output_blocks.8.0.skip_connection.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.norm.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.norm.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.proj_in.weight, Shape: torch.Size([384, 384, 1, 1]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.8.1.proj_in.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight, Shape: torch.Size([3072, 384]), Params: 1179.6K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight, Shape: torch.Size([384, 1536]), Params: 589.8K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight, Shape: torch.Size([384, 384]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.1.proj_out.weight, Shape: torch.Size([384, 384, 1, 1]), Params: 147.5K, Requires grad: False
model.diffusion_model.output_blocks.8.1.proj_out.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.8.2.conv.weight, Shape: torch.Size([384, 384, 3, 3]), Params: 1327.1K, Requires grad: False
model.diffusion_model.output_blocks.8.2.conv.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.9.0.in_layers.0.weight, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.9.0.in_layers.0.bias, Shape: torch.Size([576]), Params: 0.6K, Requires grad: False
model.diffusion_model.output_blocks.9.0.in_layers.2.weight, Shape: torch.Size([192, 576, 3, 3]), Params: 995.3K, Requires grad: False
model.diffusion_model.output_blocks.9.0.in_layers.2.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.9.0.emb_layers.1.weight, Shape: torch.Size([192, 1536]), Params: 294.9K, Requires grad: False
model.diffusion_model.output_blocks.9.0.emb_layers.1.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.9.0.out_layers.0.weight, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.9.0.out_layers.0.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.9.0.out_layers.3.weight, Shape: torch.Size([192, 192, 3, 3]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.9.0.out_layers.3.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.9.0.skip_connection.weight, Shape: torch.Size([192, 576, 1, 1]), Params: 110.6K, Requires grad: False
model.diffusion_model.output_blocks.9.0.skip_connection.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.10.0.in_layers.0.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.10.0.in_layers.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.10.0.in_layers.2.weight, Shape: torch.Size([192, 384, 3, 3]), Params: 663.6K, Requires grad: False
model.diffusion_model.output_blocks.10.0.in_layers.2.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.10.0.emb_layers.1.weight, Shape: torch.Size([192, 1536]), Params: 294.9K, Requires grad: False
model.diffusion_model.output_blocks.10.0.emb_layers.1.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.10.0.out_layers.0.weight, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.10.0.out_layers.0.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.10.0.out_layers.3.weight, Shape: torch.Size([192, 192, 3, 3]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.10.0.out_layers.3.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.10.0.skip_connection.weight, Shape: torch.Size([192, 384, 1, 1]), Params: 73.7K, Requires grad: False
model.diffusion_model.output_blocks.10.0.skip_connection.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.11.0.in_layers.0.weight, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.11.0.in_layers.0.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
model.diffusion_model.output_blocks.11.0.in_layers.2.weight, Shape: torch.Size([192, 384, 3, 3]), Params: 663.6K, Requires grad: False
model.diffusion_model.output_blocks.11.0.in_layers.2.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.11.0.emb_layers.1.weight, Shape: torch.Size([192, 1536]), Params: 294.9K, Requires grad: False
model.diffusion_model.output_blocks.11.0.emb_layers.1.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.11.0.out_layers.0.weight, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.11.0.out_layers.0.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.11.0.out_layers.3.weight, Shape: torch.Size([192, 192, 3, 3]), Params: 331.8K, Requires grad: False
model.diffusion_model.output_blocks.11.0.out_layers.3.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.output_blocks.11.0.skip_connection.weight, Shape: torch.Size([192, 384, 1, 1]), Params: 73.7K, Requires grad: False
model.diffusion_model.output_blocks.11.0.skip_connection.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.out.0.weight, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.out.0.bias, Shape: torch.Size([192]), Params: 0.2K, Requires grad: False
model.diffusion_model.out.2.weight, Shape: torch.Size([8, 192, 3, 3]), Params: 13.8K, Requires grad: False
model.diffusion_model.out.2.bias, Shape: torch.Size([8]), Params: 0.0K, Requires grad: False
first_stage_model.encoder.conv_in.weight, Shape: torch.Size([128, 1, 3, 3]), Params: 1.2K, Requires grad: False
first_stage_model.encoder.conv_in.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.0.norm1.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.0.norm1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.0.conv1.weight, Shape: torch.Size([128, 128, 3, 3]), Params: 147.5K, Requires grad: False
first_stage_model.encoder.down.0.block.0.conv1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.0.norm2.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.0.norm2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.0.conv2.weight, Shape: torch.Size([128, 128, 3, 3]), Params: 147.5K, Requires grad: False
first_stage_model.encoder.down.0.block.0.conv2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.1.norm1.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.1.norm1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.1.conv1.weight, Shape: torch.Size([128, 128, 3, 3]), Params: 147.5K, Requires grad: False
first_stage_model.encoder.down.0.block.1.conv1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.1.norm2.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.1.norm2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.block.1.conv2.weight, Shape: torch.Size([128, 128, 3, 3]), Params: 147.5K, Requires grad: False
first_stage_model.encoder.down.0.block.1.conv2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.0.downsample.conv.weight, Shape: torch.Size([128, 128, 3, 3]), Params: 147.5K, Requires grad: False
first_stage_model.encoder.down.0.downsample.conv.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.1.block.0.norm1.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.1.block.0.norm1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.encoder.down.1.block.0.conv1.weight, Shape: torch.Size([256, 128, 3, 3]), Params: 294.9K, Requires grad: False
first_stage_model.encoder.down.1.block.0.conv1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.1.block.0.norm2.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.1.block.0.norm2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.1.block.0.conv2.weight, Shape: torch.Size([256, 256, 3, 3]), Params: 589.8K, Requires grad: False
first_stage_model.encoder.down.1.block.0.conv2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.1.block.0.nin_shortcut.weight, Shape: torch.Size([256, 128, 1, 1]), Params: 32.8K, Requires grad: False
first_stage_model.encoder.down.1.block.0.nin_shortcut.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.1.block.1.norm1.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.1.block.1.norm1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.1.block.1.conv1.weight, Shape: torch.Size([256, 256, 3, 3]), Params: 589.8K, Requires grad: False
first_stage_model.encoder.down.1.block.1.conv1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.1.block.1.norm2.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.1.block.1.norm2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.1.block.1.conv2.weight, Shape: torch.Size([256, 256, 3, 3]), Params: 589.8K, Requires grad: False
first_stage_model.encoder.down.1.block.1.conv2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.1.downsample.conv.weight, Shape: torch.Size([256, 256, 3, 3]), Params: 589.8K, Requires grad: False
first_stage_model.encoder.down.1.downsample.conv.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.2.block.0.norm1.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.2.block.0.norm1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.encoder.down.2.block.0.conv1.weight, Shape: torch.Size([512, 256, 3, 3]), Params: 1179.6K, Requires grad: False
first_stage_model.encoder.down.2.block.0.conv1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.down.2.block.0.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.down.2.block.0.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.down.2.block.0.conv2.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.encoder.down.2.block.0.conv2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.down.2.block.0.nin_shortcut.weight, Shape: torch.Size([512, 256, 1, 1]), Params: 131.1K, Requires grad: False
first_stage_model.encoder.down.2.block.0.nin_shortcut.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.down.2.block.1.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.down.2.block.1.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.down.2.block.1.conv1.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.encoder.down.2.block.1.conv1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.down.2.block.1.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.down.2.block.1.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.down.2.block.1.conv2.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.encoder.down.2.block.1.conv2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_1.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_1.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_1.conv1.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.encoder.mid.block_1.conv1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_1.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_1.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_1.conv2.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.encoder.mid.block_1.conv2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.attn_1.norm.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.attn_1.norm.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.attn_1.q.weight, Shape: torch.Size([512, 512, 1, 1]), Params: 262.1K, Requires grad: False
first_stage_model.encoder.mid.attn_1.q.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.attn_1.k.weight, Shape: torch.Size([512, 512, 1, 1]), Params: 262.1K, Requires grad: False
first_stage_model.encoder.mid.attn_1.k.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.attn_1.v.weight, Shape: torch.Size([512, 512, 1, 1]), Params: 262.1K, Requires grad: False
first_stage_model.encoder.mid.attn_1.v.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.attn_1.proj_out.weight, Shape: torch.Size([512, 512, 1, 1]), Params: 262.1K, Requires grad: False
first_stage_model.encoder.mid.attn_1.proj_out.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_2.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_2.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_2.conv1.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.encoder.mid.block_2.conv1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_2.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_2.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.mid.block_2.conv2.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.encoder.mid.block_2.conv2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.norm_out.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.norm_out.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.encoder.conv_out.weight, Shape: torch.Size([16, 512, 3, 3]), Params: 73.7K, Requires grad: False
first_stage_model.encoder.conv_out.bias, Shape: torch.Size([16]), Params: 0.0K, Requires grad: False
first_stage_model.decoder.conv_in.weight, Shape: torch.Size([512, 8, 3, 3]), Params: 36.9K, Requires grad: False
first_stage_model.decoder.conv_in.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_1.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_1.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_1.conv1.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.decoder.mid.block_1.conv1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_1.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_1.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_1.conv2.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.decoder.mid.block_1.conv2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.attn_1.norm.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.attn_1.norm.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.attn_1.q.weight, Shape: torch.Size([512, 512, 1, 1]), Params: 262.1K, Requires grad: False
first_stage_model.decoder.mid.attn_1.q.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.attn_1.k.weight, Shape: torch.Size([512, 512, 1, 1]), Params: 262.1K, Requires grad: False
first_stage_model.decoder.mid.attn_1.k.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.attn_1.v.weight, Shape: torch.Size([512, 512, 1, 1]), Params: 262.1K, Requires grad: False
first_stage_model.decoder.mid.attn_1.v.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.attn_1.proj_out.weight, Shape: torch.Size([512, 512, 1, 1]), Params: 262.1K, Requires grad: False
first_stage_model.decoder.mid.attn_1.proj_out.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_2.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_2.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_2.conv1.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.decoder.mid.block_2.conv1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_2.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_2.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.mid.block_2.conv2.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.decoder.mid.block_2.conv2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.0.block.0.norm1.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.0.block.0.norm1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.0.block.0.conv1.weight, Shape: torch.Size([128, 256, 3, 3]), Params: 294.9K, Requires grad: False
first_stage_model.decoder.up.0.block.0.conv1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.0.norm2.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.0.norm2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.0.conv2.weight, Shape: torch.Size([128, 128, 3, 3]), Params: 147.5K, Requires grad: False
first_stage_model.decoder.up.0.block.0.conv2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.0.nin_shortcut.weight, Shape: torch.Size([128, 256, 1, 1]), Params: 32.8K, Requires grad: False
first_stage_model.decoder.up.0.block.0.nin_shortcut.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.1.norm1.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.1.norm1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.1.conv1.weight, Shape: torch.Size([128, 128, 3, 3]), Params: 147.5K, Requires grad: False
first_stage_model.decoder.up.0.block.1.conv1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.1.norm2.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.1.norm2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.1.conv2.weight, Shape: torch.Size([128, 128, 3, 3]), Params: 147.5K, Requires grad: False
first_stage_model.decoder.up.0.block.1.conv2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.2.norm1.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.2.norm1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.2.conv1.weight, Shape: torch.Size([128, 128, 3, 3]), Params: 147.5K, Requires grad: False
first_stage_model.decoder.up.0.block.2.conv1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.2.norm2.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.2.norm2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.0.block.2.conv2.weight, Shape: torch.Size([128, 128, 3, 3]), Params: 147.5K, Requires grad: False
first_stage_model.decoder.up.0.block.2.conv2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.up.1.block.0.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.1.block.0.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.1.block.0.conv1.weight, Shape: torch.Size([256, 512, 3, 3]), Params: 1179.6K, Requires grad: False
first_stage_model.decoder.up.1.block.0.conv1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.0.norm2.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.0.norm2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.0.conv2.weight, Shape: torch.Size([256, 256, 3, 3]), Params: 589.8K, Requires grad: False
first_stage_model.decoder.up.1.block.0.conv2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.0.nin_shortcut.weight, Shape: torch.Size([256, 512, 1, 1]), Params: 131.1K, Requires grad: False
first_stage_model.decoder.up.1.block.0.nin_shortcut.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.1.norm1.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.1.norm1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.1.conv1.weight, Shape: torch.Size([256, 256, 3, 3]), Params: 589.8K, Requires grad: False
first_stage_model.decoder.up.1.block.1.conv1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.1.norm2.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.1.norm2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.1.conv2.weight, Shape: torch.Size([256, 256, 3, 3]), Params: 589.8K, Requires grad: False
first_stage_model.decoder.up.1.block.1.conv2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.2.norm1.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.2.norm1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.2.conv1.weight, Shape: torch.Size([256, 256, 3, 3]), Params: 589.8K, Requires grad: False
first_stage_model.decoder.up.1.block.2.conv1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.2.norm2.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.2.norm2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.block.2.conv2.weight, Shape: torch.Size([256, 256, 3, 3]), Params: 589.8K, Requires grad: False
first_stage_model.decoder.up.1.block.2.conv2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.1.upsample.conv.weight, Shape: torch.Size([256, 256, 3, 3]), Params: 589.8K, Requires grad: False
first_stage_model.decoder.up.1.upsample.conv.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.decoder.up.2.block.0.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.0.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.0.conv1.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.decoder.up.2.block.0.conv1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.0.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.0.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.0.conv2.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.decoder.up.2.block.0.conv2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.1.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.1.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.1.conv1.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.decoder.up.2.block.1.conv1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.1.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.1.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.1.conv2.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.decoder.up.2.block.1.conv2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.2.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.2.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.2.conv1.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.decoder.up.2.block.2.conv1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.2.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.2.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.block.2.conv2.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.decoder.up.2.block.2.conv2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.up.2.upsample.conv.weight, Shape: torch.Size([512, 512, 3, 3]), Params: 2359.3K, Requires grad: False
first_stage_model.decoder.up.2.upsample.conv.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.decoder.norm_out.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.norm_out.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.decoder.conv_out.weight, Shape: torch.Size([1, 128, 3, 3]), Params: 1.2K, Requires grad: False
first_stage_model.decoder.conv_out.bias, Shape: torch.Size([1]), Params: 0.0K, Requires grad: False
first_stage_model.quant_conv.weight, Shape: torch.Size([16, 16, 1, 1]), Params: 0.3K, Requires grad: False
first_stage_model.quant_conv.bias, Shape: torch.Size([16]), Params: 0.0K, Requires grad: False
first_stage_model.post_quant_conv.weight, Shape: torch.Size([8, 8, 1, 1]), Params: 0.1K, Requires grad: False
first_stage_model.post_quant_conv.bias, Shape: torch.Size([8]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.conv_pre.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
first_stage_model.vocoder.conv_pre.weight, Shape: torch.Size([1024, 64, 7]), Params: 458.8K, Requires grad: False
first_stage_model.vocoder.ups.0.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.ups.0.weight, Shape: torch.Size([1024, 512, 16]), Params: 8388.6K, Requires grad: False
first_stage_model.vocoder.ups.1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.ups.1.weight, Shape: torch.Size([512, 256, 16]), Params: 2097.2K, Requires grad: False
first_stage_model.vocoder.ups.2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.ups.2.weight, Shape: torch.Size([256, 128, 8]), Params: 262.1K, Requires grad: False
first_stage_model.vocoder.ups.3.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.ups.3.weight, Shape: torch.Size([128, 64, 4]), Params: 32.8K, Requires grad: False
first_stage_model.vocoder.ups.4.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.ups.4.weight, Shape: torch.Size([64, 32, 4]), Params: 8.2K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs1.0.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs1.0.weight, Shape: torch.Size([512, 512, 3]), Params: 786.4K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs1.1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs1.1.weight, Shape: torch.Size([512, 512, 3]), Params: 786.4K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs1.2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs1.2.weight, Shape: torch.Size([512, 512, 3]), Params: 786.4K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs2.0.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs2.0.weight, Shape: torch.Size([512, 512, 3]), Params: 786.4K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs2.1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs2.1.weight, Shape: torch.Size([512, 512, 3]), Params: 786.4K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs2.2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.0.convs2.2.weight, Shape: torch.Size([512, 512, 3]), Params: 786.4K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs1.0.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs1.0.weight, Shape: torch.Size([512, 512, 7]), Params: 1835.0K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs1.1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs1.1.weight, Shape: torch.Size([512, 512, 7]), Params: 1835.0K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs1.2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs1.2.weight, Shape: torch.Size([512, 512, 7]), Params: 1835.0K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs2.0.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs2.0.weight, Shape: torch.Size([512, 512, 7]), Params: 1835.0K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs2.1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs2.1.weight, Shape: torch.Size([512, 512, 7]), Params: 1835.0K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs2.2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.1.convs2.2.weight, Shape: torch.Size([512, 512, 7]), Params: 1835.0K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs1.0.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs1.0.weight, Shape: torch.Size([512, 512, 11]), Params: 2883.6K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs1.1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs1.1.weight, Shape: torch.Size([512, 512, 11]), Params: 2883.6K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs1.2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs1.2.weight, Shape: torch.Size([512, 512, 11]), Params: 2883.6K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs2.0.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs2.0.weight, Shape: torch.Size([512, 512, 11]), Params: 2883.6K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs2.1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs2.1.weight, Shape: torch.Size([512, 512, 11]), Params: 2883.6K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs2.2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
first_stage_model.vocoder.resblocks.2.convs2.2.weight, Shape: torch.Size([512, 512, 11]), Params: 2883.6K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs1.0.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs1.0.weight, Shape: torch.Size([256, 256, 3]), Params: 196.6K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs1.1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs1.1.weight, Shape: torch.Size([256, 256, 3]), Params: 196.6K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs1.2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs1.2.weight, Shape: torch.Size([256, 256, 3]), Params: 196.6K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs2.0.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs2.0.weight, Shape: torch.Size([256, 256, 3]), Params: 196.6K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs2.1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs2.1.weight, Shape: torch.Size([256, 256, 3]), Params: 196.6K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs2.2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.3.convs2.2.weight, Shape: torch.Size([256, 256, 3]), Params: 196.6K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs1.0.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs1.0.weight, Shape: torch.Size([256, 256, 7]), Params: 458.8K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs1.1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs1.1.weight, Shape: torch.Size([256, 256, 7]), Params: 458.8K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs1.2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs1.2.weight, Shape: torch.Size([256, 256, 7]), Params: 458.8K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs2.0.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs2.0.weight, Shape: torch.Size([256, 256, 7]), Params: 458.8K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs2.1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs2.1.weight, Shape: torch.Size([256, 256, 7]), Params: 458.8K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs2.2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.4.convs2.2.weight, Shape: torch.Size([256, 256, 7]), Params: 458.8K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs1.0.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs1.0.weight, Shape: torch.Size([256, 256, 11]), Params: 720.9K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs1.1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs1.1.weight, Shape: torch.Size([256, 256, 11]), Params: 720.9K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs1.2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs1.2.weight, Shape: torch.Size([256, 256, 11]), Params: 720.9K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs2.0.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs2.0.weight, Shape: torch.Size([256, 256, 11]), Params: 720.9K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs2.1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs2.1.weight, Shape: torch.Size([256, 256, 11]), Params: 720.9K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs2.2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
first_stage_model.vocoder.resblocks.5.convs2.2.weight, Shape: torch.Size([256, 256, 11]), Params: 720.9K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs1.0.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs1.0.weight, Shape: torch.Size([128, 128, 3]), Params: 49.2K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs1.1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs1.1.weight, Shape: torch.Size([128, 128, 3]), Params: 49.2K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs1.2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs1.2.weight, Shape: torch.Size([128, 128, 3]), Params: 49.2K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs2.0.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs2.0.weight, Shape: torch.Size([128, 128, 3]), Params: 49.2K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs2.1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs2.1.weight, Shape: torch.Size([128, 128, 3]), Params: 49.2K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs2.2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.6.convs2.2.weight, Shape: torch.Size([128, 128, 3]), Params: 49.2K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs1.0.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs1.0.weight, Shape: torch.Size([128, 128, 7]), Params: 114.7K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs1.1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs1.1.weight, Shape: torch.Size([128, 128, 7]), Params: 114.7K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs1.2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs1.2.weight, Shape: torch.Size([128, 128, 7]), Params: 114.7K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs2.0.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs2.0.weight, Shape: torch.Size([128, 128, 7]), Params: 114.7K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs2.1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs2.1.weight, Shape: torch.Size([128, 128, 7]), Params: 114.7K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs2.2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.7.convs2.2.weight, Shape: torch.Size([128, 128, 7]), Params: 114.7K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs1.0.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs1.0.weight, Shape: torch.Size([128, 128, 11]), Params: 180.2K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs1.1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs1.1.weight, Shape: torch.Size([128, 128, 11]), Params: 180.2K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs1.2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs1.2.weight, Shape: torch.Size([128, 128, 11]), Params: 180.2K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs2.0.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs2.0.weight, Shape: torch.Size([128, 128, 11]), Params: 180.2K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs2.1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs2.1.weight, Shape: torch.Size([128, 128, 11]), Params: 180.2K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs2.2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.8.convs2.2.weight, Shape: torch.Size([128, 128, 11]), Params: 180.2K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs1.0.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs1.0.weight, Shape: torch.Size([64, 64, 3]), Params: 12.3K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs1.1.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs1.1.weight, Shape: torch.Size([64, 64, 3]), Params: 12.3K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs1.2.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs1.2.weight, Shape: torch.Size([64, 64, 3]), Params: 12.3K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs2.0.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs2.0.weight, Shape: torch.Size([64, 64, 3]), Params: 12.3K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs2.1.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs2.1.weight, Shape: torch.Size([64, 64, 3]), Params: 12.3K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs2.2.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.9.convs2.2.weight, Shape: torch.Size([64, 64, 3]), Params: 12.3K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs1.0.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs1.0.weight, Shape: torch.Size([64, 64, 7]), Params: 28.7K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs1.1.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs1.1.weight, Shape: torch.Size([64, 64, 7]), Params: 28.7K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs1.2.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs1.2.weight, Shape: torch.Size([64, 64, 7]), Params: 28.7K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs2.0.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs2.0.weight, Shape: torch.Size([64, 64, 7]), Params: 28.7K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs2.1.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs2.1.weight, Shape: torch.Size([64, 64, 7]), Params: 28.7K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs2.2.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.10.convs2.2.weight, Shape: torch.Size([64, 64, 7]), Params: 28.7K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs1.0.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs1.0.weight, Shape: torch.Size([64, 64, 11]), Params: 45.1K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs1.1.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs1.1.weight, Shape: torch.Size([64, 64, 11]), Params: 45.1K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs1.2.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs1.2.weight, Shape: torch.Size([64, 64, 11]), Params: 45.1K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs2.0.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs2.0.weight, Shape: torch.Size([64, 64, 11]), Params: 45.1K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs2.1.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs2.1.weight, Shape: torch.Size([64, 64, 11]), Params: 45.1K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs2.2.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
first_stage_model.vocoder.resblocks.11.convs2.2.weight, Shape: torch.Size([64, 64, 11]), Params: 45.1K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs1.0.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs1.0.weight, Shape: torch.Size([32, 32, 3]), Params: 3.1K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs1.1.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs1.1.weight, Shape: torch.Size([32, 32, 3]), Params: 3.1K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs1.2.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs1.2.weight, Shape: torch.Size([32, 32, 3]), Params: 3.1K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs2.0.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs2.0.weight, Shape: torch.Size([32, 32, 3]), Params: 3.1K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs2.1.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs2.1.weight, Shape: torch.Size([32, 32, 3]), Params: 3.1K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs2.2.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.12.convs2.2.weight, Shape: torch.Size([32, 32, 3]), Params: 3.1K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs1.0.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs1.0.weight, Shape: torch.Size([32, 32, 7]), Params: 7.2K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs1.1.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs1.1.weight, Shape: torch.Size([32, 32, 7]), Params: 7.2K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs1.2.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs1.2.weight, Shape: torch.Size([32, 32, 7]), Params: 7.2K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs2.0.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs2.0.weight, Shape: torch.Size([32, 32, 7]), Params: 7.2K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs2.1.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs2.1.weight, Shape: torch.Size([32, 32, 7]), Params: 7.2K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs2.2.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.13.convs2.2.weight, Shape: torch.Size([32, 32, 7]), Params: 7.2K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs1.0.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs1.0.weight, Shape: torch.Size([32, 32, 11]), Params: 11.3K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs1.1.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs1.1.weight, Shape: torch.Size([32, 32, 11]), Params: 11.3K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs1.2.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs1.2.weight, Shape: torch.Size([32, 32, 11]), Params: 11.3K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs2.0.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs2.0.weight, Shape: torch.Size([32, 32, 11]), Params: 11.3K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs2.1.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs2.1.weight, Shape: torch.Size([32, 32, 11]), Params: 11.3K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs2.2.bias, Shape: torch.Size([32]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.resblocks.14.convs2.2.weight, Shape: torch.Size([32, 32, 11]), Params: 11.3K, Requires grad: False
first_stage_model.vocoder.conv_post.bias, Shape: torch.Size([1]), Params: 0.0K, Requires grad: False
first_stage_model.vocoder.conv_post.weight, Shape: torch.Size([1, 32, 7]), Params: 0.2K, Requires grad: False
cond_stage_model.model.logit_scale_a, Shape: torch.Size([]), Params: 0.0K, Requires grad: False
cond_stage_model.model.logit_scale_t, Shape: torch.Size([]), Params: 0.0K, Requires grad: False
cond_stage_model.model.audio_branch.spectrogram_extractor.stft.conv_real.weight, Shape: torch.Size([513, 1, 1024]), Params: 525.3K, Requires grad: False
cond_stage_model.model.audio_branch.spectrogram_extractor.stft.conv_imag.weight, Shape: torch.Size([513, 1, 1024]), Params: 525.3K, Requires grad: False
cond_stage_model.model.audio_branch.logmel_extractor.melW, Shape: torch.Size([513, 64]), Params: 32.8K, Requires grad: False
cond_stage_model.model.audio_branch.bn0.weight, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.bn0.bias, Shape: torch.Size([64]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.patch_embed.proj.weight, Shape: torch.Size([128, 1, 4, 4]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.patch_embed.proj.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.patch_embed.norm.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.patch_embed.norm.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.norm1.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.norm1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.attn.relative_position_bias_table, Shape: torch.Size([225, 4]), Params: 0.9K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.attn.qkv.weight, Shape: torch.Size([384, 128]), Params: 49.2K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.attn.qkv.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.attn.proj.weight, Shape: torch.Size([128, 128]), Params: 16.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.attn.proj.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.norm2.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.norm2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.mlp.fc1.weight, Shape: torch.Size([512, 128]), Params: 65.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.mlp.fc1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.mlp.fc2.weight, Shape: torch.Size([128, 512]), Params: 65.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.0.mlp.fc2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.norm1.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.norm1.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.attn.relative_position_bias_table, Shape: torch.Size([225, 4]), Params: 0.9K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.attn.qkv.weight, Shape: torch.Size([384, 128]), Params: 49.2K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.attn.qkv.bias, Shape: torch.Size([384]), Params: 0.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.attn.proj.weight, Shape: torch.Size([128, 128]), Params: 16.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.attn.proj.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.norm2.weight, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.norm2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.mlp.fc1.weight, Shape: torch.Size([512, 128]), Params: 65.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.mlp.fc1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.mlp.fc2.weight, Shape: torch.Size([128, 512]), Params: 65.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.blocks.1.mlp.fc2.bias, Shape: torch.Size([128]), Params: 0.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.downsample.reduction.weight, Shape: torch.Size([256, 512]), Params: 131.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.downsample.norm.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.0.downsample.norm.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.norm1.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.norm1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.attn.relative_position_bias_table, Shape: torch.Size([225, 8]), Params: 1.8K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.attn.qkv.weight, Shape: torch.Size([768, 256]), Params: 196.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.attn.qkv.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.attn.proj.weight, Shape: torch.Size([256, 256]), Params: 65.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.attn.proj.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.norm2.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.norm2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.mlp.fc1.weight, Shape: torch.Size([1024, 256]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.mlp.fc1.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.mlp.fc2.weight, Shape: torch.Size([256, 1024]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.0.mlp.fc2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.norm1.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.norm1.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.attn.relative_position_bias_table, Shape: torch.Size([225, 8]), Params: 1.8K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.attn.qkv.weight, Shape: torch.Size([768, 256]), Params: 196.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.attn.qkv.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.attn.proj.weight, Shape: torch.Size([256, 256]), Params: 65.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.attn.proj.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.norm2.weight, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.norm2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.mlp.fc1.weight, Shape: torch.Size([1024, 256]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.mlp.fc1.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.mlp.fc2.weight, Shape: torch.Size([256, 1024]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.blocks.1.mlp.fc2.bias, Shape: torch.Size([256]), Params: 0.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.downsample.reduction.weight, Shape: torch.Size([512, 1024]), Params: 524.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.downsample.norm.weight, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.1.downsample.norm.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.0.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.1.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.2.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.3.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.4.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.5.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.6.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.7.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.8.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.9.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.10.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.norm1.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.norm1.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.attn.relative_position_bias_table, Shape: torch.Size([225, 16]), Params: 3.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.attn.qkv.weight, Shape: torch.Size([1536, 512]), Params: 786.4K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.attn.qkv.bias, Shape: torch.Size([1536]), Params: 1.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.attn.proj.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.attn.proj.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.norm2.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.norm2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.mlp.fc1.weight, Shape: torch.Size([2048, 512]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.mlp.fc1.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.mlp.fc2.weight, Shape: torch.Size([512, 2048]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.blocks.11.mlp.fc2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.downsample.reduction.weight, Shape: torch.Size([1024, 2048]), Params: 2097.2K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.downsample.norm.weight, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.2.downsample.norm.bias, Shape: torch.Size([2048]), Params: 2.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.norm1.weight, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.norm1.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.attn.relative_position_bias_table, Shape: torch.Size([225, 32]), Params: 7.2K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.attn.qkv.weight, Shape: torch.Size([3072, 1024]), Params: 3145.7K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.attn.qkv.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.attn.proj.weight, Shape: torch.Size([1024, 1024]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.attn.proj.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.norm2.weight, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.norm2.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.mlp.fc1.weight, Shape: torch.Size([4096, 1024]), Params: 4194.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.mlp.fc1.bias, Shape: torch.Size([4096]), Params: 4.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.mlp.fc2.weight, Shape: torch.Size([1024, 4096]), Params: 4194.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.0.mlp.fc2.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.norm1.weight, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.norm1.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.attn.relative_position_bias_table, Shape: torch.Size([225, 32]), Params: 7.2K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.attn.qkv.weight, Shape: torch.Size([3072, 1024]), Params: 3145.7K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.attn.qkv.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.attn.proj.weight, Shape: torch.Size([1024, 1024]), Params: 1048.6K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.attn.proj.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.norm2.weight, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.norm2.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.mlp.fc1.weight, Shape: torch.Size([4096, 1024]), Params: 4194.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.mlp.fc1.bias, Shape: torch.Size([4096]), Params: 4.1K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.mlp.fc2.weight, Shape: torch.Size([1024, 4096]), Params: 4194.3K, Requires grad: False
cond_stage_model.model.audio_branch.layers.3.blocks.1.mlp.fc2.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.norm.weight, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.norm.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: False
cond_stage_model.model.audio_branch.tscam_conv.weight, Shape: torch.Size([527, 1024, 2, 3]), Params: 3237.9K, Requires grad: False
cond_stage_model.model.audio_branch.tscam_conv.bias, Shape: torch.Size([527]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_branch.head.weight, Shape: torch.Size([527, 527]), Params: 277.7K, Requires grad: False
cond_stage_model.model.audio_branch.head.bias, Shape: torch.Size([527]), Params: 0.5K, Requires grad: False
cond_stage_model.model.text_branch.embeddings.word_embeddings.weight, Shape: torch.Size([50265, 768]), Params: 38603.5K, Requires grad: False
cond_stage_model.model.text_branch.embeddings.position_embeddings.weight, Shape: torch.Size([514, 768]), Params: 394.8K, Requires grad: False
cond_stage_model.model.text_branch.embeddings.token_type_embeddings.weight, Shape: torch.Size([1, 768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.embeddings.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.embeddings.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.0.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.1.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.2.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.3.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.4.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.5.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.6.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.7.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.8.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.9.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.10.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.attention.self.query.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.attention.self.query.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.attention.self.key.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.attention.self.key.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.attention.self.value.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.attention.self.value.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.attention.output.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.attention.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.attention.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.attention.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.intermediate.dense.weight, Shape: torch.Size([3072, 768]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.intermediate.dense.bias, Shape: torch.Size([3072]), Params: 3.1K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.output.dense.weight, Shape: torch.Size([768, 3072]), Params: 2359.3K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.output.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.output.LayerNorm.weight, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.encoder.layer.11.output.LayerNorm.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_branch.pooler.dense.weight, Shape: torch.Size([768, 768]), Params: 589.8K, Requires grad: False
cond_stage_model.model.text_branch.pooler.dense.bias, Shape: torch.Size([768]), Params: 0.8K, Requires grad: False
cond_stage_model.model.text_transform.sequential.0.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.text_transform.sequential.0.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.text_transform.sequential.3.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.text_transform.sequential.3.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.text_projection.0.weight, Shape: torch.Size([512, 768]), Params: 393.2K, Requires grad: False
cond_stage_model.model.text_projection.0.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.text_projection.2.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.text_projection.2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_transform.sequential.0.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_transform.sequential.0.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_transform.sequential.3.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_transform.sequential.3.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_projection.0.weight, Shape: torch.Size([512, 1024]), Params: 524.3K, Requires grad: False
cond_stage_model.model.audio_projection.0.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False
cond_stage_model.model.audio_projection.2.weight, Shape: torch.Size([512, 512]), Params: 262.1K, Requires grad: False
cond_stage_model.model.audio_projection.2.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: False

=== Adapter Parameters ===
adapter.0.weight, Shape: torch.Size([1024, 960]), Params: 983.0K, Requires grad: True
adapter.0.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: True
adapter.1.weight, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: True
adapter.1.bias, Shape: torch.Size([1024]), Params: 1.0K, Requires grad: True
adapter.3.weight, Shape: torch.Size([512, 1024]), Params: 524.3K, Requires grad: True
adapter.3.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: True
adapter.4.weight, Shape: torch.Size([512]), Params: 0.5K, Requires grad: True
adapter.4.bias, Shape: torch.Size([512]), Params: 0.5K, Requires grad: True

Total Parameters: 737,427,570 (737427.6K)
Trainable Parameters: 1,511,936 (1511.9K)
