/root/miniconda3/envs/qvim-baseline/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /mnt/c/Users/Davy/Documents/Code/QM/CC/AudioLDM/ckpt/vocaldm exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type            | Params | Mode
-------------------------------------------------------
0 | qvim_model | QVIMModule      | 9.8 M  | eval
1 | adapter    | QVIMAdapter     | 1.5 M  | train
2 | audioldm   | LatentDiffusion | 726 M  | eval
-------------------------------------------------------
2.6 M     Trainable params
734 M     Non-trainable params
737 M     Total params
2,949.710 Total estimated model params size (MB)
12        Modules in train mode
2245      Modules in eval mode
Sanity Checking DataLoader 0:   0%|                                                                                                                                           | 0/2 [00:00<?, ?it/s]Skipping audio logging to avoid format issues
DDIM Sampler: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:22<00:00,  2.25it/s]
/root/miniconda3/envs/qvim-baseline/lib/python3.10/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:25<00:00,  0.08it/s]
Saved improved adapter to ckpt/vocaldm/qvim_adapter_val_loss_0.0154.pt
Epoch 0:   0%|                                                                                                                                                               | 0/32 [00:00<?, ?it/s]z_noisy requires_grad: True
t requires_grad: False
adapted_embedding requires_grad: True
Trainable parameters: ['adapter.adapter.0.weight', 'adapter.adapter.0.bias', 'adapter.adapter.1.weight', 'adapter.adapter.1.bias', 'adapter.adapter.3.weight', 'adapter.adapter.3.bias', 'adapter.adapter.4.weight', 'adapter.adapter.4.bias', 'audioldm.model.diffusion_model.time_embed.0.weight', 'audioldm.model.diffusion_model.time_embed.0.bias', 'audioldm.model.diffusion_model.time_embed.2.weight', 'audioldm.model.diffusion_model.time_embed.2.bias', 'audioldm.model.diffusion_model.film_emb.weight', 'audioldm.model.diffusion_model.film_emb.bias']
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)
model_output shape: torch.Size([1, 8, 64, 64]), requires_grad: True
target shape: torch.Size([1, 8, 64, 64]), requires_grad: False
Conditioning structure (cond): <class 'dict'>
  Key: c_film, Value type: List with 1 items
    Item 0: Tensor shape=torch.Size([1, 512]), requires_grad=True
Model conditioning key: film
Model parameterization: eps

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([1024, 960])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 1 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter adapter.adapter.0.weight is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([1024])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 1 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter adapter.adapter.0.bias is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([1024])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 1 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter adapter.adapter.1.weight is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([1024])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 1 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter adapter.adapter.1.bias is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([512, 1024])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 1 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter adapter.adapter.3.weight is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([512])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 1 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter adapter.adapter.3.bias is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([512])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 1 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter adapter.adapter.4.weight is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([512])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 1 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter adapter.adapter.4.bias is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([768, 192])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 1 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter audioldm.model.diffusion_model.time_embed.0.weight is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([768])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 1 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter audioldm.model.diffusion_model.time_embed.0.bias is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([768, 768])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter audioldm.model.diffusion_model.time_embed.2.weight is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([768])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter audioldm.model.diffusion_model.time_embed.2.bias is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([768, 512])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter audioldm.model.diffusion_model.film_emb.weight is properly connected to computation graph

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([])
Single input tensor: requires_grad=True, shape=torch.Size([768])
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
-------- Exiting patched grad function --------

Parameter audioldm.model.diffusion_model.film_emb.bias is properly connected to computation graph
diffusion_loss requires_grad: True
Checking gradient paths:
Component requires_grad check: {'squared_diff': True, 'model_output': True, 'target': False}
Visualizing autograd computation graph...
Autograd graph saved to debug/autograd_graph.png
FiLM conditioning graph saved to debug/film_conditioning_graph.png
Training loss: 0.005934
Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:   3%|███▏                                                                                                  | 1/32 [00:57<29:52,  0.02it/s, v_num=vi6a, train/loss=0.00593, train/lr=0.0001]Filtered inputs from 1 to 1 tensors
/root/miniconda3/envs/qvim-baseline/lib/python3.10/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:   6%|██████▌                                                                                                 | 2/32 [00:58<14:40,  0.03it/s, v_num=vi6a, train/loss=0.662, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:   9%|█████████▊                                                                                              | 3/32 [00:59<09:34,  0.05it/s, v_num=vi6a, train/loss=0.056, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  12%|█████████████                                                                                           | 4/32 [01:00<07:01,  0.07it/s, v_num=vi6a, train/loss=0.186, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  16%|████████████████▎                                                                                       | 5/32 [01:01<05:30,  0.08it/s, v_num=vi6a, train/loss=0.287, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  19%|███████████████████▌                                                                                    | 6/32 [01:01<04:28,  0.10it/s, v_num=vi6a, train/loss=0.223, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  22%|██████████████████████▎                                                                               | 7/32 [01:02<03:43,  0.11it/s, v_num=vi6a, train/loss=0.00035, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  25%|█████████████████████████▌                                                                            | 8/32 [01:03<03:10,  0.13it/s, v_num=vi6a, train/loss=0.00191, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  28%|████████████████████████████▉                                                                          | 9/32 [01:04<02:44,  0.14it/s, v_num=vi6a, train/loss=0.0619, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  31%|████████████████████████████████▏                                                                      | 10/32 [01:12<02:40,  0.14it/s, v_num=vi6a, train/loss=0.389, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  34%|██████████████████████████████████▋                                                                  | 11/32 [01:13<02:20,  0.15it/s, v_num=vi6a, train/loss=0.00817, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  38%|██████████████████████████████████████▎                                                               | 12/32 [01:14<02:03,  0.16it/s, v_num=vi6a, train/loss=0.0014, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  41%|█████████████████████████████████████████▍                                                            | 13/32 [01:15<01:50,  0.17it/s, v_num=vi6a, train/loss=0.0338, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  44%|███████████████████████████████████████████▊                                                        | 14/32 [01:16<01:37,  0.18it/s, v_num=vi6a, train/loss=0.000562, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  47%|███████████████████████████████████████████████▊                                                      | 15/32 [01:17<01:27,  0.19it/s, v_num=vi6a, train/loss=0.0749, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  50%|███████████████████████████████████████████████████                                                   | 16/32 [01:17<01:17,  0.21it/s, v_num=vi6a, train/loss=0.0678, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  53%|██████████████████████████████████████████████████████▋                                                | 17/32 [01:18<01:09,  0.22it/s, v_num=vi6a, train/loss=0.138, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  56%|████████████████████████████████████████████████████████▊                                            | 18/32 [01:19<01:01,  0.23it/s, v_num=vi6a, train/loss=0.00044, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  59%|████████████████████████████████████████████████████████████▌                                         | 19/32 [01:19<00:54,  0.24it/s, v_num=vi6a, train/loss=0.0388, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  62%|████████████████████████████████████████████████████████████████▍                                      | 20/32 [01:20<00:48,  0.25it/s, v_num=vi6a, train/loss=0.154, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  66%|███████████████████████████████████████████████████████████████████▌                                   | 21/32 [01:21<00:42,  0.26it/s, v_num=vi6a, train/loss=0.163, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  69%|██████████████████████████████████████████████████████████████████████▏                               | 22/32 [01:21<00:37,  0.27it/s, v_num=vi6a, train/loss=0.0436, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  72%|█████████████████████████████████████████████████████████████████████████▎                            | 23/32 [01:22<00:32,  0.28it/s, v_num=vi6a, train/loss=0.0152, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  75%|████████████████████████████████████████████████████████████████████████████▌                         | 24/32 [01:23<00:27,  0.29it/s, v_num=vi6a, train/loss=0.0195, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  78%|██████████████████████████████████████████████████████████████████████████████▉                      | 25/32 [01:23<00:23,  0.30it/s, v_num=vi6a, train/loss=0.00382, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  81%|██████████████████████████████████████████████████████████████████████████████████▉                   | 26/32 [01:24<00:19,  0.31it/s, v_num=vi6a, train/loss=0.0495, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  84%|██████████████████████████████████████████████████████████████████████████████████████▉                | 27/32 [01:25<00:15,  0.32it/s, v_num=vi6a, train/loss=0.435, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  88%|█████████████████████████████████████████████████████████████████████████████████████████▎            | 28/32 [01:25<00:12,  0.33it/s, v_num=vi6a, train/loss=0.0305, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  91%|██████████████████████████████████████████████████████████████████████████████████████████▋         | 29/32 [01:26<00:08,  0.34it/s, v_num=vi6a, train/loss=0.000371, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  94%|██████████████████████████████████████████████████████████████████████████████████████████████▋      | 30/32 [01:27<00:05,  0.34it/s, v_num=vi6a, train/loss=0.00186, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0:  97%|████████████████████████████████████████████████████████████████████████████████████████████████▉   | 31/32 [01:19<00:02,  0.39it/s, v_num=vi6a, train/loss=0.000962, train/lr=0.0001]Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 64, 960])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 64, 960])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 256, 576])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 256, 576])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Filtered inputs from 1 to 1 tensors

-------- Entering patched torch.autograd.grad --------
Output tensor: requires_grad=True, shape=torch.Size([1, 1024, 384])
Number of input tensors: 1
Input tensor 0: requires_grad=True, shape=torch.Size([1, 1024, 384])
-------- Exiting patched grad function --------

Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 1 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Module Sequential received input 0 that doesn't require_grad
Module Sequential received input 1 that doesn't require_grad
Module Sequential received input 2 that doesn't require_grad
Module Sequential produced output 0 that doesn't require_grad
Module SiLU received input 0 that doesn't require_grad
Module SiLU produced output 0 that doesn't require_grad
Module Linear received input 0 that doesn't require_grad
Module Linear received input 2 that doesn't require_grad
Module Linear produced output 0 that doesn't require_grad
Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [01:20<00:00,  0.40it/s, v_num=vi6a, train/loss=0.00132, train/lr=0.0001]Skipping audio logging to avoid format issues
Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████| 32/32 [03:32<00:00,  0.15it/s, v_num=vi6a, train/loss=0.00132, train/lr=0.0001, val/loss=0.0461]
DDIM Sampler: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:16<00:00,  3.04it/s]
Metric val/loss improved. New best score: 0.046
Epoch 0, global step 32: 'val/loss' reached 0.04611 (best 0.04611), saving model to '/mnt/c/Users/Davy/Documents/Code/QM/CC/AudioLDM/ckpt/vocaldm/vocaldm-epoch=00-val_loss=0.0000.ckpt' as top 3/s]
Saved adapter to ckpt/vocaldm/qvim_adapter.pt                                                                                                                                                       
`Trainer.fit` stopped: `max_epochs=1` reached.
Training failed with error: Run.log_artifact() got an unexpected keyword argument 'description'
Traceback (most recent call last):
  File "/mnt/c/Users/Davy/Documents/Code/QM/CC/AudioLDM/train_vocaldm.py", line 1144, in train_vocaldm
    wandb_logger.experiment.log_artifact(
  File "/root/miniconda3/envs/qvim-baseline/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 387, in wrapper
    return func(self, *args, **kwargs)
  File "/root/miniconda3/envs/qvim-baseline/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 435, in wrapper_fn
    return func(self, *args, **kwargs)
  File "/root/miniconda3/envs/qvim-baseline/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 425, in wrapper
    return func(self, *args, **kwargs)
TypeError: Run.log_artifact() got an unexpected keyword argument 'description'
Resources cleaned up
